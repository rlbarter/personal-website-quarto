[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome",
    "section": "",
    "text": "Hi there, I’m Rebecca, welcome to my website! Below you will find my newest posts as well as quick links to some of my more popular posts.\nI’m also available for data science trainings and consulting! Click here to learn more."
  },
  {
    "objectID": "index.html#new-posts",
    "href": "index.html#new-posts",
    "title": "Welcome",
    "section": "New posts",
    "text": "New posts\n\n\nAI Tutorial: Using Text Embeddings to Label Synthetic Doctor’s Notes Generated with ChatGPT\n\n\n\n\n\nI’ve been playing around with OpenAI and ChatGPT in my research, and I thought I’d put together a short tutorial that demonstrates using ChatGPT API to generate synthetic doctor’s notes, and then using OpenAI’s text embedding models to label the notes according to whether they involve a chronic or acute condition. And yes, I’m fully aware that what I write here will probably be out of date in about 3 hours.\n\n\n\n\nAn introduction to Python for R Users\n\n\n\n\n\nI have a confession to make: I am now a Python user. Don’t judge me, join me! In this post, I introduce Python for data analysis from the perspective of an R (tidyverse) user. This post is a must-read if you are an R user hoping to dip your toes in the Python pool."
  },
  {
    "objectID": "index.html#popular-posts",
    "href": "index.html#popular-posts",
    "title": "Welcome",
    "section": "Popular posts",
    "text": "Popular posts\n\n\n\nR\n\n\n\nLearn to purrr\n\n\nPurrr is the tidyverse’s answer to apply functions for iteration. It’s one of those packages that you might have heard of, but seemed too complicated to sit down and learn. Starting with map functions, and taking you on a journey that will harness the power of the list, this post will have you purrring in no time.\n\n\n\n\n\n\nTidymodels: Tidymodels: tidy machine learning in R\n\n\nThe tidyverse’s take on machine learning is finally here. Tidymodels forms the basis of tidy machine learning, and this post provides a whirlwind tour to get you started.\n\n\n\n\n\n\nAcross: applying dplyr functions simultaneously across multiple columns\n\n\nWith the introduction of dplyr 1.0.0, there are a few new features: the biggest of which is across() which supersedes the scoped versions of dplyr functions.\n\n\n\n\n\nStatistics\n\n\n\nThe intuition behind inverse probability weighting in causal inference\n\n\nRemoving confounding can be done via a variety methods including IP-weighting. This post provides a summary of the intuition behind IP-weighting.\n\n\n\n\n\n\nConfounding in causal inference: what is it, and what to do about it?\n\n\nAn introduction to the field of causal inference and the issues surrounding confounding.\n\n\n\n\n\n\nUnderstanding Instrumental Variables\n\n\nInstrumental variables is one of the most mystical concepts in causal inference. For some reason, most of the existing explanations are overly complicated and focus on specific nuanced aspects of generating IV estimates without really providing the intuition for why it makes sense. In this post, you will not find too many technical details, but rather a narrative introducing instruments and why they are useful."
  },
  {
    "objectID": "blog/2017-02-05-superheat-cran.html",
    "href": "blog/2017-02-05-superheat-cran.html",
    "title": "superheat 0.1.0",
    "section": "",
    "text": "superheat 0.1.0 is now available on CRAN. Superheat makes it easy to create extendable, cutomizable, and most importantly, beautiful heatmaps. It has increased flexibility and user-friendliness when compared to alternatives such as heatmap() and pheatmap().\nFor usage options see the vignette and for examples see the accompanying paper by Barter and Yu (2017).\nYou can install the latest version with:\ninstall.packages(\"superheat\")\nStay tuned for new versions with added features and minor usability tweaks."
  },
  {
    "objectID": "blog/2017-02-05-superheat-cran.html#example-usage",
    "href": "blog/2017-02-05-superheat-cran.html#example-usage",
    "title": "superheat 0.1.0",
    "section": "Example usage",
    "text": "Example usage\nUsage is straightforward and intuitive:\n\nlibrary(superheat)\nset.seed(1347983)\nselected.rows &lt;- sample(1:nrow(mtcars), 10)\n# add a space after the column names for aesthetic purposes\ncolnames(mtcars) &lt;- paste0(colnames(mtcars), \" \")\n\n# plot the superheatmap\nsuperheat(mtcars[selected.rows,], \n          # scale columns\n          scale = T, \n          # label aesthetics\n          left.label.size = 0.5,\n          bottom.label.size = 0.15,\n          bottom.label.text.angle = 90,\n          bottom.label.text.alignment = \"right\",\n          bottom.label.col = \"white\",\n          # dendrogram\n          row.dendrogram = T,\n          # top plot\n          yt = sapply(mtcars, function(x) cor(x, mtcars$hp)),\n          yt.plot.type = \"bar\",\n          yt.axis.name = \"correlation\\nwith\\nhorsepower\",\n          # column order\n          order.cols = order(sapply(mtcars, function(x) cor(x, mtcars$hp))),\n          # grid lines\n          grid.vline.col = \"white\",\n          grid.hline.col = \"white\")"
  },
  {
    "objectID": "blog/2019-01-23_scoped-verbs.html",
    "href": "blog/2019-01-23_scoped-verbs.html",
    "title": "mutate_all(), select_if(), summarise_at()… what’s the deal with scoped verbs?!",
    "section": "",
    "text": "Note: Scoped verbs have now essentially been superseded by accross() (soon to be available in dplyr 1.0.0). See http://www.rebeccabarter.com/blog/2020-07-09-across/ for details.\nI often find myself wishing that I could apply the same mutate function to several columns in a data frame at once, such as convert all factors to characters, or do something to all columns that have missing values, or select all variables whose names end with _important. When I first googled these problems around a year ago, I started to see solutions that use weird extensions of the basic mutate(), select(), rename(), and summarise() dplyr functions that look like summarise_all(), filter_at(), mutate_if(), and so on. I have since learned that these functions are called “scoped verbs” (where “scoped” means that they operate only on a selection of variables).\nUnfortunately, despite my extensive googling, I never really found a satisfactory description of how to use these functions in general, I think primarily because the documentation for these functions is not particularly useful (try ?mutate_at()).\nFortunately, I recently attended a series of lightening talks hosted by the RLadies SF chapter where Sara Altman pointed us towards a summary document that Hadley Wickham wrote for the Data Science class he helped create at Stanford in 2017 (this class is now taught by Sara Altman herself).\nTo summarise what I will demonstrate below, there are three scoped variants of the standard mutate, summarise, rename and select (and transmute) dplyr functions that can be specified by the following suffixes:\nTo explain how these functions all work, I will use the dataset from a survey of 800 Pittsburgh residents on whether or not they approve of self-driving car companies testing their autonomous vehicles on the streets of Pittsburgh (there have several articles on this issue in recent times in case you missed them: 1, 2). The data can usually be downloaded from data.gov (but is currently unavailable due to the current Government Shutdown - I will update this with an actual link to the data one day). For now you can download the data from here.\nA random sample of 10 rows of this dataset is shown below. To make it easy to see what’s going on, I’ll restrict my analysis below to these 10 rows\n# load in the only library you ever really need\nlibrary(tidyverse)\nlibrary(lubridate)\n# load in survey data\nav_survey &lt;- read_csv(\"data/bikepghpublic.csv\")\nset.seed(45679)\nav_survey_sample &lt;- av_survey %&gt;% \n  # select jsut a few columns and give some more intuitive column names\n  select(id = `Response ID`,\n         start_date = `Start Date`, \n         end_date = `End Date`,\n         interacted_with_av_as_pedestrian = InteractPedestrian,\n         interacted_with_av_as_cyclist = InteractBicycle,\n         circumstanses_of_interaction = CircumstancesCoded, # lol @ typo in data\n         approve_av_testing_pgh = FeelingsProvingGround) %&gt;%\n  # take a random sample of 10 rows\n  sample_n(10) %&gt;%\n  # make data frame so that we view the whole thing\n  as.data.frame()\nav_survey_sample\n\n          id                 start_date                   end_date\n1  260381029  02/24/2017 3:14:19 AM PST  02/24/2017 3:18:05 AM PST\n2  260822947  03/03/2017 7:08:33 AM PST  03/03/2017 7:19:15 AM PST\n3  260907069  03/06/2017 5:57:07 PM PST  03/06/2017 5:59:08 PM PST\n4  261099035  03/08/2017 3:05:41 PM PST  03/09/2017 7:17:53 AM PST\n5  260332379  02/23/2017 9:09:11 AM PST  02/23/2017 9:11:07 AM PST\n6  260355021 02/23/2017 10:11:52 PM PST 02/23/2017 10:20:02 PM PST\n7  260350676  02/23/2017 6:10:42 PM PST  02/23/2017 6:13:59 PM PST\n8  261092370 03/08/2017 11:22:43 AM PST 03/08/2017 11:25:22 AM PST\n9  260332519  02/23/2017 9:16:14 AM PST  02/23/2017 9:21:40 AM PST\n10 260351560  02/23/2017 6:40:54 PM PST  02/23/2017 6:42:02 PM PST\n   interacted_with_av_as_pedestrian interacted_with_av_as_cyclist\n1                               Yes                           Yes\n2                                No                           Yes\n3                               Yes                           Yes\n4                                No                           Yes\n5                                No                           Yes\n6                               Yes                           Yes\n7                                No                            No\n8                          Not sure                      Not sure\n9                               Yes                            No\n10                               No                            No\n   circumstanses_of_interaction approve_av_testing_pgh\n1                             2                Approve\n2                             4             Disapprove\n3                            NA                Approve\n4                             3       Somewhat Approve\n5                            NA       Somewhat Approve\n6                             1                Approve\n7                            NA                Approve\n8                            NA    Somewhat Disapprove\n9                             2                Approve\n10                           NA    Somewhat Disapprove"
  },
  {
    "objectID": "blog/2019-01-23_scoped-verbs.html#select_if",
    "href": "blog/2019-01-23_scoped-verbs.html#select_if",
    "title": "mutate_all(), select_if(), summarise_at()… what’s the deal with scoped verbs?!",
    "section": "select_if()",
    "text": "select_if()\nFor instance, we can use select_if() to extract the numeric columns of the tibble only.\n\nav_survey_sample %&gt;% select_if(is.numeric)\n\n          id circumstanses_of_interaction\n1  260381029                            2\n2  260822947                            4\n3  260907069                           NA\n4  261099035                            3\n5  260332379                           NA\n6  260355021                            1\n7  260350676                           NA\n8  261092370                           NA\n9  260332519                            2\n10 260351560                           NA\n\n\nWe could also apply use more complex logical statements, for example by selecting columns that have at least one missing value.\n\nav_survey_sample %&gt;% \n  # select columns with at least one NA\n  # the expression evaluates to TRUE if there is one or more missing values\n  select_if(~sum(is.na(.x)) &gt; 0) \n\n   circumstanses_of_interaction\n1                             2\n2                             4\n3                            NA\n4                             3\n5                            NA\n6                             1\n7                            NA\n8                            NA\n9                             2\n10                           NA"
  },
  {
    "objectID": "blog/2019-01-23_scoped-verbs.html#rename_if",
    "href": "blog/2019-01-23_scoped-verbs.html#rename_if",
    "title": "mutate_all(), select_if(), summarise_at()… what’s the deal with scoped verbs?!",
    "section": "rename_if()",
    "text": "rename_if()\nWe could rename columns that satisfy a logical expression using rename_if(). For instance, we can add a num_ prefix to all numeric column names.\n\nav_survey_sample %&gt;%\n  # only rename numeric columns by adding a \"num_\" prefix\n  rename_if(is.numeric, ~paste0(\"num_\", .x))\n\n      num_id                 start_date                   end_date\n1  260381029  02/24/2017 3:14:19 AM PST  02/24/2017 3:18:05 AM PST\n2  260822947  03/03/2017 7:08:33 AM PST  03/03/2017 7:19:15 AM PST\n3  260907069  03/06/2017 5:57:07 PM PST  03/06/2017 5:59:08 PM PST\n4  261099035  03/08/2017 3:05:41 PM PST  03/09/2017 7:17:53 AM PST\n5  260332379  02/23/2017 9:09:11 AM PST  02/23/2017 9:11:07 AM PST\n6  260355021 02/23/2017 10:11:52 PM PST 02/23/2017 10:20:02 PM PST\n7  260350676  02/23/2017 6:10:42 PM PST  02/23/2017 6:13:59 PM PST\n8  261092370 03/08/2017 11:22:43 AM PST 03/08/2017 11:25:22 AM PST\n9  260332519  02/23/2017 9:16:14 AM PST  02/23/2017 9:21:40 AM PST\n10 260351560  02/23/2017 6:40:54 PM PST  02/23/2017 6:42:02 PM PST\n   interacted_with_av_as_pedestrian interacted_with_av_as_cyclist\n1                               Yes                           Yes\n2                                No                           Yes\n3                               Yes                           Yes\n4                                No                           Yes\n5                                No                           Yes\n6                               Yes                           Yes\n7                                No                            No\n8                          Not sure                      Not sure\n9                               Yes                            No\n10                               No                            No\n   num_circumstanses_of_interaction approve_av_testing_pgh\n1                                 2                Approve\n2                                 4             Disapprove\n3                                NA                Approve\n4                                 3       Somewhat Approve\n5                                NA       Somewhat Approve\n6                                 1                Approve\n7                                NA                Approve\n8                                NA    Somewhat Disapprove\n9                                 2                Approve\n10                               NA    Somewhat Disapprove"
  },
  {
    "objectID": "blog/2019-01-23_scoped-verbs.html#mutate_if",
    "href": "blog/2019-01-23_scoped-verbs.html#mutate_if",
    "title": "mutate_all(), select_if(), summarise_at()… what’s the deal with scoped verbs?!",
    "section": "mutate_if()",
    "text": "mutate_if()\nWe could similarly use mutate_if() to mutate columns that satisfy specified logical conditions. In the example below, we mutate all columns that have at least one missing value by replacing NA with \"missing\".\n\nav_survey_sample %&gt;% \n  # only mutate columns with at least one NA\n  # replace each NA value with the character \"missing\"\n  mutate_if(~sum(is.na(.x)) &gt; 0,\n            ~if_else(is.na(.x), \"missing\", as.character(.x)))\n\n          id                 start_date                   end_date\n1  260381029  02/24/2017 3:14:19 AM PST  02/24/2017 3:18:05 AM PST\n2  260822947  03/03/2017 7:08:33 AM PST  03/03/2017 7:19:15 AM PST\n3  260907069  03/06/2017 5:57:07 PM PST  03/06/2017 5:59:08 PM PST\n4  261099035  03/08/2017 3:05:41 PM PST  03/09/2017 7:17:53 AM PST\n5  260332379  02/23/2017 9:09:11 AM PST  02/23/2017 9:11:07 AM PST\n6  260355021 02/23/2017 10:11:52 PM PST 02/23/2017 10:20:02 PM PST\n7  260350676  02/23/2017 6:10:42 PM PST  02/23/2017 6:13:59 PM PST\n8  261092370 03/08/2017 11:22:43 AM PST 03/08/2017 11:25:22 AM PST\n9  260332519  02/23/2017 9:16:14 AM PST  02/23/2017 9:21:40 AM PST\n10 260351560  02/23/2017 6:40:54 PM PST  02/23/2017 6:42:02 PM PST\n   interacted_with_av_as_pedestrian interacted_with_av_as_cyclist\n1                               Yes                           Yes\n2                                No                           Yes\n3                               Yes                           Yes\n4                                No                           Yes\n5                                No                           Yes\n6                               Yes                           Yes\n7                                No                            No\n8                          Not sure                      Not sure\n9                               Yes                            No\n10                               No                            No\n   circumstanses_of_interaction approve_av_testing_pgh\n1                             2                Approve\n2                             4             Disapprove\n3                       missing                Approve\n4                             3       Somewhat Approve\n5                       missing       Somewhat Approve\n6                             1                Approve\n7                       missing                Approve\n8                       missing    Somewhat Disapprove\n9                             2                Approve\n10                      missing    Somewhat Disapprove"
  },
  {
    "objectID": "blog/2019-01-23_scoped-verbs.html#summarise_if",
    "href": "blog/2019-01-23_scoped-verbs.html#summarise_if",
    "title": "mutate_all(), select_if(), summarise_at()… what’s the deal with scoped verbs?!",
    "section": "summarise_if()",
    "text": "summarise_if()\nSimilarly, summarise_if() will summarise columns that satisfy the specified logical conditions. Below, we summarise each character column by reporting the most common value (but for some reason there is no mode() function in R, so we need to write our own).\n\n# function to calculate the mode (most common) observation\nmode &lt;- function(x) {\n  names(sort(table(x)))[1]\n}\n# summarise character\nav_survey_sample %&gt;% \n  summarise_if(is.character, mode)\n\n                  start_date                   end_date\n1 02/23/2017 10:11:52 PM PST 02/23/2017 10:20:02 PM PST\n  interacted_with_av_as_pedestrian interacted_with_av_as_cyclist\n1                         Not sure                      Not sure\n  approve_av_testing_pgh\n1             Disapprove"
  },
  {
    "objectID": "blog/2019-01-23_scoped-verbs.html#select-helpers",
    "href": "blog/2019-01-23_scoped-verbs.html#select-helpers",
    "title": "mutate_all(), select_if(), summarise_at()… what’s the deal with scoped verbs?!",
    "section": "Select helpers",
    "text": "Select helpers\nSelect helpers are functions that you can use within select() to help specify which variables you want to select. The options are\n\nstarts_with(): select all variables that start with a specified character string\nends_with(): select all variables that end with a specified character string\ncontains(): select all variables that contain a specified character string\nmatches(): select variables that match a specified character string\none_of(): selects variables that match any entries in the specified character vector\nnum_range(): selects variables that are numbered (e.g. columns named V1, V2, V3 would be selected by select(num_range(\"V\", 1:3)))\n\nThere are many ways that we could select the date variables using the ends_with() and contains() select helpers:\n\n# selecting the date columns by providing their names\nav_survey_sample %&gt;% select(start_date, end_date)\n\n                   start_date                   end_date\n1   02/24/2017 3:14:19 AM PST  02/24/2017 3:18:05 AM PST\n2   03/03/2017 7:08:33 AM PST  03/03/2017 7:19:15 AM PST\n3   03/06/2017 5:57:07 PM PST  03/06/2017 5:59:08 PM PST\n4   03/08/2017 3:05:41 PM PST  03/09/2017 7:17:53 AM PST\n5   02/23/2017 9:09:11 AM PST  02/23/2017 9:11:07 AM PST\n6  02/23/2017 10:11:52 PM PST 02/23/2017 10:20:02 PM PST\n7   02/23/2017 6:10:42 PM PST  02/23/2017 6:13:59 PM PST\n8  03/08/2017 11:22:43 AM PST 03/08/2017 11:25:22 AM PST\n9   02/23/2017 9:16:14 AM PST  02/23/2017 9:21:40 AM PST\n10  02/23/2017 6:40:54 PM PST  02/23/2017 6:42:02 PM PST\n\n# selecting the columns that end with \"_date\"\nav_survey_sample %&gt;% select(ends_with(\"_date\"))\n\n                   start_date                   end_date\n1   02/24/2017 3:14:19 AM PST  02/24/2017 3:18:05 AM PST\n2   03/03/2017 7:08:33 AM PST  03/03/2017 7:19:15 AM PST\n3   03/06/2017 5:57:07 PM PST  03/06/2017 5:59:08 PM PST\n4   03/08/2017 3:05:41 PM PST  03/09/2017 7:17:53 AM PST\n5   02/23/2017 9:09:11 AM PST  02/23/2017 9:11:07 AM PST\n6  02/23/2017 10:11:52 PM PST 02/23/2017 10:20:02 PM PST\n7   02/23/2017 6:10:42 PM PST  02/23/2017 6:13:59 PM PST\n8  03/08/2017 11:22:43 AM PST 03/08/2017 11:25:22 AM PST\n9   02/23/2017 9:16:14 AM PST  02/23/2017 9:21:40 AM PST\n10  02/23/2017 6:40:54 PM PST  02/23/2017 6:42:02 PM PST\n\n# selecting the columns that contain \"date\"\nav_survey_sample %&gt;% select(contains(\"date\"))\n\n                   start_date                   end_date\n1   02/24/2017 3:14:19 AM PST  02/24/2017 3:18:05 AM PST\n2   03/03/2017 7:08:33 AM PST  03/03/2017 7:19:15 AM PST\n3   03/06/2017 5:57:07 PM PST  03/06/2017 5:59:08 PM PST\n4   03/08/2017 3:05:41 PM PST  03/09/2017 7:17:53 AM PST\n5   02/23/2017 9:09:11 AM PST  02/23/2017 9:11:07 AM PST\n6  02/23/2017 10:11:52 PM PST 02/23/2017 10:20:02 PM PST\n7   02/23/2017 6:10:42 PM PST  02/23/2017 6:13:59 PM PST\n8  03/08/2017 11:22:43 AM PST 03/08/2017 11:25:22 AM PST\n9   02/23/2017 9:16:14 AM PST  02/23/2017 9:21:40 AM PST\n10  02/23/2017 6:40:54 PM PST  02/23/2017 6:42:02 PM PST\n\n\nIf you ever find yourself wanting to provide variable names as characters, the matches() and one_of() select helpers can help you do that.\n\n# provide matches with a single character variables\nvariable &lt;- \"start_date\"\nav_survey_sample %&gt;% select(matches(variable))\n\n                   start_date\n1   02/24/2017 3:14:19 AM PST\n2   03/03/2017 7:08:33 AM PST\n3   03/06/2017 5:57:07 PM PST\n4   03/08/2017 3:05:41 PM PST\n5   02/23/2017 9:09:11 AM PST\n6  02/23/2017 10:11:52 PM PST\n7   02/23/2017 6:10:42 PM PST\n8  03/08/2017 11:22:43 AM PST\n9   02/23/2017 9:16:14 AM PST\n10  02/23/2017 6:40:54 PM PST\n\n# provide one_of with a vector of character variables\nvariables &lt;- c(\"start_date\", \"end_date\")\nav_survey_sample %&gt;% select(one_of(variables))\n\n                   start_date                   end_date\n1   02/24/2017 3:14:19 AM PST  02/24/2017 3:18:05 AM PST\n2   03/03/2017 7:08:33 AM PST  03/03/2017 7:19:15 AM PST\n3   03/06/2017 5:57:07 PM PST  03/06/2017 5:59:08 PM PST\n4   03/08/2017 3:05:41 PM PST  03/09/2017 7:17:53 AM PST\n5   02/23/2017 9:09:11 AM PST  02/23/2017 9:11:07 AM PST\n6  02/23/2017 10:11:52 PM PST 02/23/2017 10:20:02 PM PST\n7   02/23/2017 6:10:42 PM PST  02/23/2017 6:13:59 PM PST\n8  03/08/2017 11:22:43 AM PST 03/08/2017 11:25:22 AM PST\n9   02/23/2017 9:16:14 AM PST  02/23/2017 9:21:40 AM PST\n10  02/23/2017 6:40:54 PM PST  02/23/2017 6:42:02 PM PST\n\n\nNote that technically there does exist a select_at() function that requires a vars() input, but I can’t really think of a good use of this function…\n\n# this is the same as av_survey_sample %&gt;% select(start_date, end_date)\nav_survey_sample %&gt;% \n  select_at(vars(start_date, end_date))\n\n                   start_date                   end_date\n1   02/24/2017 3:14:19 AM PST  02/24/2017 3:18:05 AM PST\n2   03/03/2017 7:08:33 AM PST  03/03/2017 7:19:15 AM PST\n3   03/06/2017 5:57:07 PM PST  03/06/2017 5:59:08 PM PST\n4   03/08/2017 3:05:41 PM PST  03/09/2017 7:17:53 AM PST\n5   02/23/2017 9:09:11 AM PST  02/23/2017 9:11:07 AM PST\n6  02/23/2017 10:11:52 PM PST 02/23/2017 10:20:02 PM PST\n7   02/23/2017 6:10:42 PM PST  02/23/2017 6:13:59 PM PST\n8  03/08/2017 11:22:43 AM PST 03/08/2017 11:25:22 AM PST\n9   02/23/2017 9:16:14 AM PST  02/23/2017 9:21:40 AM PST\n10  02/23/2017 6:40:54 PM PST  02/23/2017 6:42:02 PM PST\n\n\nThe syntax of this select_at() example though can be useful for understanding how the vars() function can be used in the other _at() functions)."
  },
  {
    "objectID": "blog/2019-01-23_scoped-verbs.html#rename_at",
    "href": "blog/2019-01-23_scoped-verbs.html#rename_at",
    "title": "mutate_all(), select_if(), summarise_at()… what’s the deal with scoped verbs?!",
    "section": "rename_at()",
    "text": "rename_at()\nYou can rename specified variables using the rename_at() function. For instance, we could replace all column names that contain the character string “av” with the same column name but an uppercase “AV” instead of the original lowercase “av”.\nTo do this, we use the select helper contains() within the vars() function.\n\n# use a select helper to only apply to columns whose name contains \"av\"\n# then rename these columns with \"AV\" in place of \"av\"\nav_survey_sample %&gt;% \n  rename_at(vars(contains(\"av\")), \n            ~gsub(\"av\", \"AV\", .x))\n\n          id                 start_date                   end_date\n1  260381029  02/24/2017 3:14:19 AM PST  02/24/2017 3:18:05 AM PST\n2  260822947  03/03/2017 7:08:33 AM PST  03/03/2017 7:19:15 AM PST\n3  260907069  03/06/2017 5:57:07 PM PST  03/06/2017 5:59:08 PM PST\n4  261099035  03/08/2017 3:05:41 PM PST  03/09/2017 7:17:53 AM PST\n5  260332379  02/23/2017 9:09:11 AM PST  02/23/2017 9:11:07 AM PST\n6  260355021 02/23/2017 10:11:52 PM PST 02/23/2017 10:20:02 PM PST\n7  260350676  02/23/2017 6:10:42 PM PST  02/23/2017 6:13:59 PM PST\n8  261092370 03/08/2017 11:22:43 AM PST 03/08/2017 11:25:22 AM PST\n9  260332519  02/23/2017 9:16:14 AM PST  02/23/2017 9:21:40 AM PST\n10 260351560  02/23/2017 6:40:54 PM PST  02/23/2017 6:42:02 PM PST\n   interacted_with_AV_as_pedestrian interacted_with_AV_as_cyclist\n1                               Yes                           Yes\n2                                No                           Yes\n3                               Yes                           Yes\n4                                No                           Yes\n5                                No                           Yes\n6                               Yes                           Yes\n7                                No                            No\n8                          Not sure                      Not sure\n9                               Yes                            No\n10                               No                            No\n   circumstanses_of_interaction approve_AV_testing_pgh\n1                             2                Approve\n2                             4             Disapprove\n3                            NA                Approve\n4                             3       Somewhat Approve\n5                            NA       Somewhat Approve\n6                             1                Approve\n7                            NA                Approve\n8                            NA    Somewhat Disapprove\n9                             2                Approve\n10                           NA    Somewhat Disapprove"
  },
  {
    "objectID": "blog/2019-01-23_scoped-verbs.html#mutate_at",
    "href": "blog/2019-01-23_scoped-verbs.html#mutate_at",
    "title": "mutate_all(), select_if(), summarise_at()… what’s the deal with scoped verbs?!",
    "section": "mutate_at()",
    "text": "mutate_at()\nTo mutate only the date variables, normally we would do the mdy_hms() transformation to each variable separately as follows:\n\n# use the standard (unscoped) approach\nav_survey_sample %&gt;% \n  mutate(start_date = mdy_hms(start_date),\n         end_date = mdy_hms(end_date))\n\n          id          start_date            end_date\n1  260381029 2017-02-24 03:14:19 2017-02-24 03:18:05\n2  260822947 2017-03-03 07:08:33 2017-03-03 07:19:15\n3  260907069 2017-03-06 17:57:07 2017-03-06 17:59:08\n4  261099035 2017-03-08 15:05:41 2017-03-09 07:17:53\n5  260332379 2017-02-23 09:09:11 2017-02-23 09:11:07\n6  260355021 2017-02-23 22:11:52 2017-02-23 22:20:02\n7  260350676 2017-02-23 18:10:42 2017-02-23 18:13:59\n8  261092370 2017-03-08 11:22:43 2017-03-08 11:25:22\n9  260332519 2017-02-23 09:16:14 2017-02-23 09:21:40\n10 260351560 2017-02-23 18:40:54 2017-02-23 18:42:02\n   interacted_with_av_as_pedestrian interacted_with_av_as_cyclist\n1                               Yes                           Yes\n2                                No                           Yes\n3                               Yes                           Yes\n4                                No                           Yes\n5                                No                           Yes\n6                               Yes                           Yes\n7                                No                            No\n8                          Not sure                      Not sure\n9                               Yes                            No\n10                               No                            No\n   circumstanses_of_interaction approve_av_testing_pgh\n1                             2                Approve\n2                             4             Disapprove\n3                            NA                Approve\n4                             3       Somewhat Approve\n5                            NA       Somewhat Approve\n6                             1                Approve\n7                            NA                Approve\n8                            NA    Somewhat Disapprove\n9                             2                Approve\n10                           NA    Somewhat Disapprove\n\n\nHowever, using mutate_at() and supplying these column names as arguments to the vars() function, we could specify the function only once.\n\n# specifying specific variables to apply the same function to\nav_survey_sample %&gt;% \n  mutate_at(vars(start_date, end_date), mdy_hms)\n\n          id          start_date            end_date\n1  260381029 2017-02-24 03:14:19 2017-02-24 03:18:05\n2  260822947 2017-03-03 07:08:33 2017-03-03 07:19:15\n3  260907069 2017-03-06 17:57:07 2017-03-06 17:59:08\n4  261099035 2017-03-08 15:05:41 2017-03-09 07:17:53\n5  260332379 2017-02-23 09:09:11 2017-02-23 09:11:07\n6  260355021 2017-02-23 22:11:52 2017-02-23 22:20:02\n7  260350676 2017-02-23 18:10:42 2017-02-23 18:13:59\n8  261092370 2017-03-08 11:22:43 2017-03-08 11:25:22\n9  260332519 2017-02-23 09:16:14 2017-02-23 09:21:40\n10 260351560 2017-02-23 18:40:54 2017-02-23 18:42:02\n   interacted_with_av_as_pedestrian interacted_with_av_as_cyclist\n1                               Yes                           Yes\n2                                No                           Yes\n3                               Yes                           Yes\n4                                No                           Yes\n5                                No                           Yes\n6                               Yes                           Yes\n7                                No                            No\n8                          Not sure                      Not sure\n9                               Yes                            No\n10                               No                            No\n   circumstanses_of_interaction approve_av_testing_pgh\n1                             2                Approve\n2                             4             Disapprove\n3                            NA                Approve\n4                             3       Somewhat Approve\n5                            NA       Somewhat Approve\n6                             1                Approve\n7                            NA                Approve\n8                            NA    Somewhat Disapprove\n9                             2                Approve\n10                           NA    Somewhat Disapprove\n\n\nMoreover, we can use the select helpers to specify which columns we want to mutate, without having to write out the entire column names.\n\n# use a \"select helper\" to specify the variables that end with \"_date\"\nav_survey_sample %&gt;% \n  mutate_at(vars(ends_with(\"_date\")), mdy_hms)\n\n          id          start_date            end_date\n1  260381029 2017-02-24 03:14:19 2017-02-24 03:18:05\n2  260822947 2017-03-03 07:08:33 2017-03-03 07:19:15\n3  260907069 2017-03-06 17:57:07 2017-03-06 17:59:08\n4  261099035 2017-03-08 15:05:41 2017-03-09 07:17:53\n5  260332379 2017-02-23 09:09:11 2017-02-23 09:11:07\n6  260355021 2017-02-23 22:11:52 2017-02-23 22:20:02\n7  260350676 2017-02-23 18:10:42 2017-02-23 18:13:59\n8  261092370 2017-03-08 11:22:43 2017-03-08 11:25:22\n9  260332519 2017-02-23 09:16:14 2017-02-23 09:21:40\n10 260351560 2017-02-23 18:40:54 2017-02-23 18:42:02\n   interacted_with_av_as_pedestrian interacted_with_av_as_cyclist\n1                               Yes                           Yes\n2                                No                           Yes\n3                               Yes                           Yes\n4                                No                           Yes\n5                                No                           Yes\n6                               Yes                           Yes\n7                                No                            No\n8                          Not sure                      Not sure\n9                               Yes                            No\n10                               No                            No\n   circumstanses_of_interaction approve_av_testing_pgh\n1                             2                Approve\n2                             4             Disapprove\n3                            NA                Approve\n4                             3       Somewhat Approve\n5                            NA       Somewhat Approve\n6                             1                Approve\n7                            NA                Approve\n8                            NA    Somewhat Disapprove\n9                             2                Approve\n10                           NA    Somewhat Disapprove"
  },
  {
    "objectID": "blog/2019-01-23_scoped-verbs.html#summarise_at",
    "href": "blog/2019-01-23_scoped-verbs.html#summarise_at",
    "title": "mutate_all(), select_if(), summarise_at()… what’s the deal with scoped verbs?!",
    "section": "summarise_at()",
    "text": "summarise_at()\nThe summarise_at() scoped verb behaves very similarly to the mutate_at() scoped verb, in that we can easily specify which variables we want to apply the same summary function to.\nFor instance, the following example summarises all variables that contain the word “interacted” by counting the number of “Yes” entries.\n\nav_survey_sample %&gt;% \n  summarise_at(vars(contains(\"interacted\")), ~sum(.x == \"Yes\"))\n\n  interacted_with_av_as_pedestrian interacted_with_av_as_cyclist\n1                                4                             6"
  },
  {
    "objectID": "blog/2019-01-23_scoped-verbs.html#rename_all",
    "href": "blog/2019-01-23_scoped-verbs.html#rename_all",
    "title": "mutate_all(), select_if(), summarise_at()… what’s the deal with scoped verbs?!",
    "section": "rename_all()",
    "text": "rename_all()\nThe select_all() would is quite redundant (it would simply return all columns). Its friend rename_all(), however can be very useful.\nFor instance, we could rename all variables by replacing underscores _ with dots . (although I would advise against this: underscores are way better than dots!).\n\nav_survey_sample %&gt;% \n  rename_all(~gsub(\"_\", \".\", .x))\n\n          id                 start.date                   end.date\n1  260381029  02/24/2017 3:14:19 AM PST  02/24/2017 3:18:05 AM PST\n2  260822947  03/03/2017 7:08:33 AM PST  03/03/2017 7:19:15 AM PST\n3  260907069  03/06/2017 5:57:07 PM PST  03/06/2017 5:59:08 PM PST\n4  261099035  03/08/2017 3:05:41 PM PST  03/09/2017 7:17:53 AM PST\n5  260332379  02/23/2017 9:09:11 AM PST  02/23/2017 9:11:07 AM PST\n6  260355021 02/23/2017 10:11:52 PM PST 02/23/2017 10:20:02 PM PST\n7  260350676  02/23/2017 6:10:42 PM PST  02/23/2017 6:13:59 PM PST\n8  261092370 03/08/2017 11:22:43 AM PST 03/08/2017 11:25:22 AM PST\n9  260332519  02/23/2017 9:16:14 AM PST  02/23/2017 9:21:40 AM PST\n10 260351560  02/23/2017 6:40:54 PM PST  02/23/2017 6:42:02 PM PST\n   interacted.with.av.as.pedestrian interacted.with.av.as.cyclist\n1                               Yes                           Yes\n2                                No                           Yes\n3                               Yes                           Yes\n4                                No                           Yes\n5                                No                           Yes\n6                               Yes                           Yes\n7                                No                            No\n8                          Not sure                      Not sure\n9                               Yes                            No\n10                               No                            No\n   circumstanses.of.interaction approve.av.testing.pgh\n1                             2                Approve\n2                             4             Disapprove\n3                            NA                Approve\n4                             3       Somewhat Approve\n5                            NA       Somewhat Approve\n6                             1                Approve\n7                            NA                Approve\n8                            NA    Somewhat Disapprove\n9                             2                Approve\n10                           NA    Somewhat Disapprove"
  },
  {
    "objectID": "blog/2019-01-23_scoped-verbs.html#mutate_all",
    "href": "blog/2019-01-23_scoped-verbs.html#mutate_all",
    "title": "mutate_all(), select_if(), summarise_at()… what’s the deal with scoped verbs?!",
    "section": "mutate_all()",
    "text": "mutate_all()\nWe could apply the same mutate function to every column at once using mutate_all(). For instance, the code below converts every column to a numeric (although this results in mostly missing values for the character variables)\n\nav_survey_sample %&gt;%\n  mutate_all(as.numeric)\n\n          id start_date end_date interacted_with_av_as_pedestrian\n1  260381029         NA       NA                               NA\n2  260822947         NA       NA                               NA\n3  260907069         NA       NA                               NA\n4  261099035         NA       NA                               NA\n5  260332379         NA       NA                               NA\n6  260355021         NA       NA                               NA\n7  260350676         NA       NA                               NA\n8  261092370         NA       NA                               NA\n9  260332519         NA       NA                               NA\n10 260351560         NA       NA                               NA\n   interacted_with_av_as_cyclist circumstanses_of_interaction\n1                             NA                            2\n2                             NA                            4\n3                             NA                           NA\n4                             NA                            3\n5                             NA                           NA\n6                             NA                            1\n7                             NA                           NA\n8                             NA                           NA\n9                             NA                            2\n10                            NA                           NA\n   approve_av_testing_pgh\n1                      NA\n2                      NA\n3                      NA\n4                      NA\n5                      NA\n6                      NA\n7                      NA\n8                      NA\n9                      NA\n10                     NA"
  },
  {
    "objectID": "blog/2019-01-23_scoped-verbs.html#summarise_all",
    "href": "blog/2019-01-23_scoped-verbs.html#summarise_all",
    "title": "mutate_all(), select_if(), summarise_at()… what’s the deal with scoped verbs?!",
    "section": "summarise_all()",
    "text": "summarise_all()\nWe could also apply the same summary function to every column at once using summarise_all(). For instance, the example below calculates the number of distinct entries in each column.\n\nav_survey_sample %&gt;%\n  summarise_all(n_distinct)\n\n  id start_date end_date interacted_with_av_as_pedestrian\n1 10         10       10                                3\n  interacted_with_av_as_cyclist circumstanses_of_interaction\n1                             3                            5\n  approve_av_testing_pgh\n1                      4"
  },
  {
    "objectID": "blog/2020-07-30-applied-statistics-universities.html",
    "href": "blog/2020-07-30-applied-statistics-universities.html",
    "title": "It’s time for statistics departments to start supporting their applied students",
    "section": "",
    "text": "I graduated with a PhD from UC Berkeley’s statistics department in December. My PhD dissertation consisted of three 100% applied projects (one of which was a piece of open-source software). This is, unfortunately, incredibly rare.\nOver the past few years, I’ve had a number of current and prospective statistics PhD students both at Berkeley and outside Berkeley get in touch with me to ask me how I made my way through a statistics PhD by working only on applied projects. My answer is always as follows: I got lucky. I ended up with an incredibly supportive and advisor, Bin Yu, who went out of her way to find applied projects for me, and was comfortable letting me spend time doing non-conventional things (like learn D3.js, work on my blog, and she even recruited me to be the co-author on a book on Data Science that she wanted to write based on her data science philosophy that she has developed throughout her career). I got a lot out of my PhD, and I’m incredibly grateful to Bin for her guidance. If you’re interested in Bin’s work, I recommend reading her paper, Veridical Data Science, which reflects the philosophy that underlies our book.\nUnfortunately, my experience is definitely not the norm, even in my own department. I’ve had so many upsetting conversations with fellow applied stats grad students about how they feel lost and unsupported, about how they’re struggling to find applied projects to work on, and when they do finally find a project, they end up having to fight to justify that their project is “statistical enough” to count towards their dissertation. I’ve had these conversations with students from a wide range of statistics departments, not just my own. I actually think that the Berkeley statistics department is a little better than a many other statistics departments on this front - at least at Berkeley it’s possible to graduate with purely applied projects, even if it’s not easy.\nWhile pretty much every statistics department is starting to realize that they need to embrace “Data Science” in order to remain up-to-date and relevant, they are often doing so in a way that is more performative than practical. They’re starting data science master’s programs, but these programs are often just theoretical statistics programs with a “data science” label slapped on it. They’re also trying to recruit more applied students, but they aren’t giving them any of the support they need to be successful. How can you expect your applied students to succeed if you don’t have any truly applied faculty to guide them? And when you do have applied faculty, you don’t give them tenure (because apparently they’re not “furthering the field of statistics”)? Who are these students going to work with? Who is going to show them that it’s possible to succeed as an applied statistician in a statistics department? Unfortunately, the vast majority of statistics departments are sending a strong message that there is no such thing as success in academia for truly applied statisticians. The end result is that applied students are being brought into statistics departments, chewed up a little, and then spat out, often with a fair amount of imposter syndrome, anxiety and depression, and sadly, often without their PhDs.\nMaybe at this point you’re getting aggravated and saying: “but most statistics departments do have applied statisticians on their faculty!”. Well, yes, technically that’s true, if your definition of “applied statistics” involves developing a method (or even developing the underlying theory for a method) and then applying it to a nice, clean dataset to show that the method works. What about answering scientific questions and actually solving real-world problems? What about working with real, messy data? What about communication, exploration, and visualization? Are none of these things statistics?\nIn my view, this failure to embrace truly applied projects as applied statistics is the whole reason Data Science exists. Data Science is what “Applied Statistics” is supposed to be. Didn’t statistics come about in the first place because governments started collecting and analyzing real data to understand their citizens? Sure, they developed methods, but these methods were developed specifically because they were needed to solve real problems! Why is it that today, when grad students want to solve real problems, they’re told that they aren’t doing statistics?\nI’m not saying that theory and methods aren’t an important part of statistics, or that having a theoretical background doesn’t help if you want to be an accomplished data scientist. What I’m saying is that theory shouldn’t always be the focus. If you want to support your applied statistics students (or data science students), then you need to have a track in your program that teaches students how to use real world (and messy!) data to ask scientific questions, and communicate about data and the subsequent scientific findings (I could do a whole post on how most statisticians can’t communicate, but I’ll leave that for another day). For that to happen, you need to hire truly applied faculty, who are working on real world problems with real, messy data.\nUnfortunately, it’s a bit of a Catch-22. How can you hire more applied faculty if by the time us applied students get out of your programs, we’re so fed up with feeling like the under-class in statistics departments that we have no desire to go down the academic route whatsoever? Statistics, I’ll leave you with two pieces of advice:\n\nSupport your applied students. Let them do applied work, and help them find projects in collaboration with faculty in other departments. Let them graduate, and don’t make them fight for it. Give them a positive grad school experience, rather than leave them with nightmares.\nSeek out applied faculty candidates. And when you hire them, actually give them f**king tenure."
  },
  {
    "objectID": "blog/2019-08-19_purrr.html",
    "href": "blog/2019-08-19_purrr.html",
    "title": "Learn to purrr",
    "section": "",
    "text": "“It was on the corner of the street that he noticed the first sign of something peculiar - a cat reading a map” - J.K. Rowling\nPurrr is one of those tidyverse packages that you keep hearing about, and you know you should probably learn it, but you just never seem to get around to it.\nAt it’s core, purrr is all about iteration. Purrr introduces map functions (the tidyverse’s answer to base R’s apply functions, but more in line with functional programming practices) as well as some new functions for manipulating lists. To get a quick snapshot of any tidyverse package, a nice place to go is the cheatsheet. I find these particularly useful after I’ve already got the basics of a package down, because I inevitably realise that there are a bunch of functionalities I knew nothing about.\nAnother useful resource for learning about purrr is Jenny Bryan’s tutorial. Jenny’s tutorial is fantastic, but is a lot longer than mine. This post is a lot shorter and my goal is to get you up and running with purrr very quickly.\nWhile the workhorse of dplyr is the data frame, the workhorse of purrr is the list. If you aren’t familiar with lists, hopefully this will help you understand what they are:\nHere is an example of a list that has three elements: a single number, a vector and a data frame\nmy_first_list &lt;- list(my_number = 5,\n                      my_vector = c(\"a\", \"b\", \"c\"),\n                      my_dataframe = data.frame(a = 1:3, b = c(\"q\", \"b\", \"z\"), c = c(\"bananas\", \"are\", \"so very great\")))\nmy_first_list\n\n$my_number\n[1] 5\n\n$my_vector\n[1] \"a\" \"b\" \"c\"\n\n$my_dataframe\n  a b             c\n1 1 q       bananas\n2 2 b           are\n3 3 z so very great\nNote that a data frame is actually a special case of a list where each element of the list is a vector of the same length."
  },
  {
    "objectID": "blog/2019-08-19_purrr.html#simplest-usage-repeated-looping-with-map",
    "href": "blog/2019-08-19_purrr.html#simplest-usage-repeated-looping-with-map",
    "title": "Learn to purrr",
    "section": "Simplest usage: repeated looping with map",
    "text": "Simplest usage: repeated looping with map\nFundamentally, maps are for iteration. In the example below I will iterate through the vector c(1, 4, 7) by adding 10 to each entry. This function applied to a single number, which we will call .x, can be defined as\n\naddTen &lt;- function(.x) {\n  return(.x + 10)\n}\n\nThe map() function below iterates addTen() across all entries of the vector, .x = c(1, 4, 7), and returns the output as a list\n\nlibrary(tidyverse)\nmap(.x = c(1, 4, 7), \n    .f = addTen)\n\n[[1]]\n[1] 11\n\n[[2]]\n[1] 14\n\n[[3]]\n[1] 17\n\n\nFortunately, you don’t actually need to specify the argument names\n\nmap(c(1, 4, 7), addTen)\n\n[[1]]\n[1] 11\n\n[[2]]\n[1] 14\n\n[[3]]\n[1] 17\n\n\nNote that\n\nthe first element of the output is the result of applying the function to the first element of the input (1),\nthe second element of the output is the result of applying the function to the second element of the input (4),\nand the third element of the output is the result of applying the function to the third element of the input (7).\n\nThe following code chunks show that no matter if the input object is a vector, a list, or a data frame, map() always returns a list.\n\nmap(list(1, 4, 7), addTen)\n\n[[1]]\n[1] 11\n\n[[2]]\n[1] 14\n\n[[3]]\n[1] 17\n\n\n\nmap(data.frame(a = 1, b = 4, c = 7), addTen)\n\n$a\n[1] 11\n\n$b\n[1] 14\n\n$c\n[1] 17\n\n\nIf we wanted the output of map to be some other object type, we need to use a different function. For instance to map the input to a numeric (double) vector, you can use the map_dbl() (“map to a double”) function.\n\nmap_dbl(c(1, 4, 7), addTen)\n\n[1] 11 14 17\n\n\nTo map to a character vector, you can use the map_chr() (“map to a character”) function.\n\nmap_chr(c(1, 4, 7), addTen)\n\nWarning: Automatic coercion from double to character was deprecated in purrr 1.0.0.\nℹ Please use an explicit call to `as.character()` within `map_chr()` instead.\n\n\n[1] \"11.000000\" \"14.000000\" \"17.000000\"\n\n\nIf you want to return a data frame, then you would use the map_df() function. However, you need to make sure that in each iteration you’re returning a data frame which has consistent column names. map_df will automatically bind the rows of each iteration.\nFor this example, I want to return a data frame whose columns correspond to the original number and the number plus ten.\n\nmap_df(c(1, 4, 7), function(.x) {\n  return(data.frame(old_number = .x, \n                    new_number = addTen(.x)))\n})\n\n  old_number new_number\n1          1         11\n2          4         14\n3          7         17\n\n\nNote that in this case, I defined an “anonymous” function as our output for each iteration. An anonymous function is a temporary function (that you define as the function argument to the map). Here I used the argument name .x, but I could have used anything.\nAnother function to be aware of is modify(), which is just like the map functions, but always returns an object the same type as the input object.\n\nlibrary(tidyverse)\nmodify(c(1, 4, 7), addTen)\n\n[1] 11 14 17\n\n\n\nmodify(list(1, 4, 7), addTen)\n\n[[1]]\n[1] 11\n\n[[2]]\n[1] 14\n\n[[3]]\n[1] 17\n\n\n\nmodify(data.frame(1, 4, 7), addTen)\n\n  X1 X4 X7\n1 11 14 17\n\n\nModify also has a pretty useful sibling, modify_if(), that only applies the function to elements that satisfy a specific criteria (specified by a “predicate function”, the second argument called .p). For instance, the following example only modifies the third entry since it is greater than 5.\n\nmodify_if(.x = list(1, 4, 7), \n          .p = function(x) x &gt; 5,\n          .f = addTen)\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] 4\n\n[[3]]\n[1] 17"
  },
  {
    "objectID": "blog/2019-08-19_purrr.html#the-tilde-dot-shorthand-for-functions",
    "href": "blog/2019-08-19_purrr.html#the-tilde-dot-shorthand-for-functions",
    "title": "Learn to purrr",
    "section": "The tilde-dot shorthand for functions",
    "text": "The tilde-dot shorthand for functions\nTo make the code more concise you can use the tilde-dot shorthand for anonymous functions (the functions that you create as arguments of other functions).\nThe notation works by replacing\n\nfunction(x) {\n  x + 10\n}\n\nwith\n\n~{.x + 10}\n\n~ indicates that you have started an anonymous function, and the argument of the anonymous function can be referred to using .x (or simply .). Unlike normal function arguments that can be anything that you like, the tilde-dot function argument is always .x.\nThus, instead of defining the addTen() function separately, we could use the tilde-dot shorthand\n\nmap_dbl(c(1, 4, 7), ~{.x + 10})\n\n[1] 11 14 17"
  },
  {
    "objectID": "blog/2019-08-19_purrr.html#applying-map-functions-in-a-slightly-more-interesting-context",
    "href": "blog/2019-08-19_purrr.html#applying-map-functions-in-a-slightly-more-interesting-context",
    "title": "Learn to purrr",
    "section": "Applying map functions in a slightly more interesting context",
    "text": "Applying map functions in a slightly more interesting context\nThroughout this tutorial, we will use the gapminder dataset that can be loaded directly if you’re connected to the internet. Each function will first be demonstrated using a simple numeric example, and then will be demonstrated using a more complex practical example based on the gapminder dataset.\nMy general workflow involves loading the original data and saving it as an object with a meaningful name and an _orig suffix. I then define a copy of the original dataset without the _orig suffix. Having an original copy of my data in my environment means that it is easy to check that my manipulations do what I expected. I will make direct data cleaning modifications to the gapminder data frame, but will never edit the gapminder_orig data frame.\n\n# to download the data directly:\ngapminder_orig &lt;- read.csv(\"https://raw.githubusercontent.com/rlbarter/personal-website-quarto/main/blog/data/gapminder.csv\")\n# define a copy of the original dataset that we will clean and play with \ngapminder &lt;- gapminder_orig\n\nThe gapminder dataset has 1704 rows containing information on population, life expectancy and GDP per capita by year and country.\nA “tidy” data frame is one where every row is a single observational unit (in this case, indexed by country and year), and every column corresponds to a variable that is measured for each observational unit (in this case, for each country and year, a measurement is made for population, continent, life expectancy and GDP). If you’d like to learn more about “tidy data”, I highly recommend reading Hadley Wickham’s tidy data article.\n\ndim(gapminder)\n\n[1] 1704    6\n\nhead(gapminder)\n\n      country continent year lifeExp      pop gdpPercap\n1 Afghanistan      Asia 1952  28.801  8425333  779.4453\n2 Afghanistan      Asia 1957  30.332  9240934  820.8530\n3 Afghanistan      Asia 1962  31.997 10267083  853.1007\n4 Afghanistan      Asia 1967  34.020 11537966  836.1971\n5 Afghanistan      Asia 1972  36.088 13079460  739.9811\n6 Afghanistan      Asia 1977  38.438 14880372  786.1134\n\n\nSince gapminder is a data frame, the map_ functions will iterate over each column. An example of simple usage of the map_ functions is to summarize each column. For instance, you can identify the type of each column by applying the class() function to each column. Since the output of the class() function is a character, we will use the map_chr() function:\n\n# apply the class() function to each column\ngapminder %&gt;% map_chr(class)\n\n    country   continent        year     lifeExp         pop   gdpPercap \n\"character\" \"character\"   \"integer\"   \"numeric\"   \"integer\"   \"numeric\" \n\n\nI frequently do this to get a quick snapshot of each column type of a new dataset directly in the console. As a habit, I usually pipe in the data using %&gt;%, rather than provide it as an argument. Remember that the pipe places the object to the left of the pipe in the first argument of the function to the right.\nSimilarly, if you wanted to identify the number of distinct values in each column, you could apply the n_distinct() function from the dplyr package to each column. Since the output of n_distinct() is a numeric (a double), you might want to use the map_dbl() function so that the results of each iteration (the application of n_distinct() to each column) are concatenated into a numeric vector:\n\n# apply the n_distinct() function to each column\ngapminder %&gt;% map_dbl(n_distinct)\n\n  country continent      year   lifeExp       pop gdpPercap \n      142         5        12      1626      1704      1704 \n\n\nIf you want to do something a little more complicated, such return a few different summaries of each column in a data frame, you can use map_df(). When things are getting a little bit more complicated, you typically need to define an anonymous function that you want to apply to each column. Using the tilde-dot notation, the anonymous function below calculates the number of distinct entries and the type of the current column (which is accessible as .x), and then combines them into a two-column data frame. Once it has iterated through each of the columns, the map_df function combines the data frames row-wise into a single data frame.\n\ngapminder %&gt;% map_df(~(data.frame(n_distinct = n_distinct(.x),\n                                  class = class(.x))))\n\n  n_distinct     class\n1        142 character\n2          5 character\n3         12   integer\n4       1626   numeric\n5       1704   integer\n6       1704   numeric\n\n\nNote that we’ve lost the variable names! The variable names correspond to the names of the objects over which we are iterating (in this case, the column names), and these are not automatically included as a column in the output data frame. You can tell map_df() to include them using the .id argument of map_df(). This will automatically take the name of the element being iterated over and include it in the column corresponding to whatever you set .id to.\n\ngapminder %&gt;% map_df(~(data.frame(n_distinct = n_distinct(.x),\n                                  class = class(.x))),\n                     .id = \"variable\")\n\n   variable n_distinct     class\n1   country        142 character\n2 continent          5 character\n3      year         12   integer\n4   lifeExp       1626   numeric\n5       pop       1704   integer\n6 gdpPercap       1704   numeric\n\n\nIf you’re having trouble thinking through these map actions, I recommend that you first figure out what the code would be to do what you want for a single element, and then paste it into the map_df() function (a nice trick I saw Hadley Wickham used a few years ago when he presented on purrr at RLadies SF).\nFor instance, since the first element of the gapminder data frame is the first column, let’s define .x in our environment to be this first column.\n\n# take the first element of the gapminder data\n.x &lt;- gapminder %&gt;% pluck(1)\n# look at the first 6 rows\nhead(.x)\n\n[1] \"Afghanistan\" \"Afghanistan\" \"Afghanistan\" \"Afghanistan\" \"Afghanistan\"\n[6] \"Afghanistan\"\n\n\nThen, you can create a data frame for this column that contains the number of distinct entries, and the class of the column.\n\ndata.frame(n_distinct = n_distinct(.x),\n           class = class(.x))\n\n  n_distinct     class\n1        142 character\n\n\nSince this has done what was expected want for the first column, you can paste this code into the map function using the tilde-dot shorthand.\n\ngapminder %&gt;% map_df(~(data.frame(n_distinct = n_distinct(.x),\n                                  class = class(.x))),\n                     .id = \"variable\")\n\n   variable n_distinct     class\n1   country        142 character\n2 continent          5 character\n3      year         12   integer\n4   lifeExp       1626   numeric\n5       pop       1704   integer\n6 gdpPercap       1704   numeric\n\n\nmap_df() is definitely one of the most powerful functions of purrr in my opinion, and is probably the one that I use most."
  },
  {
    "objectID": "blog/2019-08-19_purrr.html#maps-with-multiple-input-objects",
    "href": "blog/2019-08-19_purrr.html#maps-with-multiple-input-objects",
    "title": "Learn to purrr",
    "section": "Maps with multiple input objects",
    "text": "Maps with multiple input objects\nAfter gaining a basic understanding of purrr’s map functions, you can start to do some fancier stuff. For instance, what if you want to perform a map that iterates through two objects. The code below uses map functions to create a list of plots that compare life expectancy and GDP per capita for each continent/year combination.\nThe map function that maps over two objects instead of 1 is called map2(). The first two arguments are the two objects you want to iterate over, and the third is the function (with two arguments, one for each object).\n\nmap2(.x = object1, # the first object to iterate over\n     .y = object2, # the second object to iterate over\n     .f = plotFunction(.x, .y))\n\nFirst, you need to define a vector (or list) of continents and a paired vector (or list) of years that you want to iterate through. Note that in our continent/year example\n\nthe first iteration will correspond to the first continent in the continent vector and the first year in the year vector,\nthe second iteration will correspond to the second continent in the continent vector and the second year in the year vector.\n\nThis might seem obvious, but it is a natural instinct to incorrectly assume that map2() will automatically perform the action on all combinations that can be made from the two vectors. For instance if you have a continent vector .x = c(\"Americas\", \"Asia\") and a year vector .y = c(1952, 2007), then you might assume that map2 will iterate over the Americas for 1952 and for 2007, and then Asia for 1952 and 2007. It won’t though. The iteration will actually be first the Americas for 1952 only, and then Asia for 2007 only.\nFirst, let’s get our vectors of continents and years, starting by obtaining all distinct combinations of continents and years that appear in the data.\n\ncontinent_year &lt;- gapminder %&gt;% distinct(continent, year)\ncontinent_year\n\n   continent year\n1       Asia 1952\n2       Asia 1957\n3       Asia 1962\n4       Asia 1967\n5       Asia 1972\n6       Asia 1977\n7       Asia 1982\n8       Asia 1987\n9       Asia 1992\n10      Asia 1997\n11      Asia 2002\n12      Asia 2007\n13    Europe 1952\n14    Europe 1957\n15    Europe 1962\n16    Europe 1967\n17    Europe 1972\n18    Europe 1977\n19    Europe 1982\n20    Europe 1987\n21    Europe 1992\n22    Europe 1997\n23    Europe 2002\n24    Europe 2007\n25    Africa 1952\n26    Africa 1957\n27    Africa 1962\n28    Africa 1967\n29    Africa 1972\n30    Africa 1977\n31    Africa 1982\n32    Africa 1987\n33    Africa 1992\n34    Africa 1997\n35    Africa 2002\n36    Africa 2007\n37  Americas 1952\n38  Americas 1957\n39  Americas 1962\n40  Americas 1967\n41  Americas 1972\n42  Americas 1977\n43  Americas 1982\n44  Americas 1987\n45  Americas 1992\n46  Americas 1997\n47  Americas 2002\n48  Americas 2007\n49   Oceania 1952\n50   Oceania 1957\n51   Oceania 1962\n52   Oceania 1967\n53   Oceania 1972\n54   Oceania 1977\n55   Oceania 1982\n56   Oceania 1987\n57   Oceania 1992\n58   Oceania 1997\n59   Oceania 2002\n60   Oceania 2007\n\n\nThen extracting the continent and year pairs as separate vectors\n\n# extract the continent and year pairs as separate vectors\ncontinents &lt;- continent_year %&gt;% pull(continent) %&gt;% as.character\nyears &lt;- continent_year %&gt;% pull(year)\n\nIf you want to use tilde-dot short-hand, the anonymous arguments will be .x for the first object being iterated over, and .y for the second object being iterated over.\nBefore jumping straight into the map function, it’s a good idea to first figure out what the code will be for just first iteration (the first continent and the first year, which happen to be Asia in 1952).\n\n# try to figure out the code for the first example\n.x &lt;- continents[1]\n.y &lt;- years[1]\n# make a scatterplot of GDP vs life expectancy in all Asian countries for 1952\ngapminder %&gt;% \n  filter(continent == .x,\n         year == .y) %&gt;%\n  ggplot() +\n  geom_point(aes(x = gdpPercap, y = lifeExp)) +\n  ggtitle(glue::glue(.x, \" \", .y))\n\n\n\n\n\n\n\n\nThis seems to have worked. So you can then copy-and-paste the code into the map2 function\n\nplot_list &lt;- map2(.x = continents, \n                  .y = years, \n                  .f = ~{\n                    gapminder %&gt;% \n                      filter(continent == .x,\n                             year == .y) %&gt;%\n                      ggplot() +\n                      geom_point(aes(x = gdpPercap, y = lifeExp)) +\n                      ggtitle(glue::glue(.x, \" \", .y))\n                  })\n\nAnd you can look at a few of the entries of the list to see that they make sense\n\nplot_list[[1]]\n\n\n\n\n\n\n\nplot_list[[22]]\n\n\n\n\n\n\n\n\npmap() allows you to iterate over an arbitrary number of objects (i.e. more than two)."
  },
  {
    "objectID": "blog/2019-08-19_purrr.html#list-columns-and-nested-data-frames",
    "href": "blog/2019-08-19_purrr.html#list-columns-and-nested-data-frames",
    "title": "Learn to purrr",
    "section": "List columns and Nested data frames",
    "text": "List columns and Nested data frames\nTibbles are tidyverse data frames. Some crazy stuff starts happening when you learn that tibble columns can be lists (as opposed to vectors, which is what they usually are). This is where the difference between tibbles and data frames becomes real.\nFor instance, a tibble can be “nested” where the tibble is essentially split into separate data frames based on a grouping variable, and these separate data frames are stored as entries of a list (that is then stored in the data column of the data frame).\nBelow I nest the gapminder data by continent.\n\ngapminder_nested &lt;- gapminder %&gt;% \n  group_by(continent) %&gt;% \n  nest()\ngapminder_nested\n\n# A tibble: 5 × 2\n# Groups:   continent [5]\n  continent data              \n  &lt;chr&gt;     &lt;list&gt;            \n1 Asia      &lt;tibble [396 × 5]&gt;\n2 Europe    &lt;tibble [360 × 5]&gt;\n3 Africa    &lt;tibble [624 × 5]&gt;\n4 Americas  &lt;tibble [300 × 5]&gt;\n5 Oceania   &lt;tibble [24 × 5]&gt; \n\n\nThe first column is the variable that we grouped by, continent, and the second column is the rest of the data frame corresponding to that group (as if you had filtered the data frame to the specific continent). To see this, the code below shows that the first entry in the data column corresponds to the entire gapminder dataset for Asia.\n\ngapminder_nested$data[[1]]\n\n# A tibble: 396 × 5\n   country      year lifeExp      pop gdpPercap\n   &lt;chr&gt;       &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n 1 Afghanistan  1952    28.8  8425333      779.\n 2 Afghanistan  1957    30.3  9240934      821.\n 3 Afghanistan  1962    32.0 10267083      853.\n 4 Afghanistan  1967    34.0 11537966      836.\n 5 Afghanistan  1972    36.1 13079460      740.\n 6 Afghanistan  1977    38.4 14880372      786.\n 7 Afghanistan  1982    39.9 12881816      978.\n 8 Afghanistan  1987    40.8 13867957      852.\n 9 Afghanistan  1992    41.7 16317921      649.\n10 Afghanistan  1997    41.8 22227415      635.\n# ℹ 386 more rows\n\n\nUsing dplyr pluck() function, this can be written as\n\ngapminder_nested %&gt;% \n  # extract the first entry from the data column\n  pluck(\"data\", 1)\n\n# A tibble: 396 × 5\n   country      year lifeExp      pop gdpPercap\n   &lt;chr&gt;       &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n 1 Afghanistan  1952    28.8  8425333      779.\n 2 Afghanistan  1957    30.3  9240934      821.\n 3 Afghanistan  1962    32.0 10267083      853.\n 4 Afghanistan  1967    34.0 11537966      836.\n 5 Afghanistan  1972    36.1 13079460      740.\n 6 Afghanistan  1977    38.4 14880372      786.\n 7 Afghanistan  1982    39.9 12881816      978.\n 8 Afghanistan  1987    40.8 13867957      852.\n 9 Afghanistan  1992    41.7 16317921      649.\n10 Afghanistan  1997    41.8 22227415      635.\n# ℹ 386 more rows\n\n\nSimilarly, the 5th entry in the data column corresponds to the entire gapminder dataset for Oceania.\n\ngapminder_nested %&gt;% pluck(\"data\", 5)\n\n# A tibble: 24 × 5\n   country    year lifeExp      pop gdpPercap\n   &lt;chr&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n 1 Australia  1952    69.1  8691212    10040.\n 2 Australia  1957    70.3  9712569    10950.\n 3 Australia  1962    70.9 10794968    12217.\n 4 Australia  1967    71.1 11872264    14526.\n 5 Australia  1972    71.9 13177000    16789.\n 6 Australia  1977    73.5 14074100    18334.\n 7 Australia  1982    74.7 15184200    19477.\n 8 Australia  1987    76.3 16257249    21889.\n 9 Australia  1992    77.6 17481977    23425.\n10 Australia  1997    78.8 18565243    26998.\n# ℹ 14 more rows\n\n\nYou might be asking at this point why you would ever want to nest your data frame? It just doesn’t seem like that useful a thing to do… until you realise that you now have the power to use dplyr manipulations on more complex objects that can be stored in a list.\nHowever, since actions such as mutate() are applied directly to the entire column (which is usually a vector, so is fine), we run into issues when we try to mutate a list. For instance, since columns are usually vectors, normal vectorized functions work just fine on them\n\ntibble(vec_col = 1:10) %&gt;%\n  mutate(vec_sum = sum(vec_col))\n\n# A tibble: 10 × 2\n   vec_col vec_sum\n     &lt;int&gt;   &lt;int&gt;\n 1       1      55\n 2       2      55\n 3       3      55\n 4       4      55\n 5       5      55\n 6       6      55\n 7       7      55\n 8       8      55\n 9       9      55\n10      10      55\n\n\nbut when the column is a list, vectorized functions don’t know what to do with them, and we get an error that says Error in sum(x) : invalid 'type' (list) of argument. Try\n\ntibble(list_col = list(c(1, 5, 7), \n                       5, \n                       c(10, 10, 11))) %&gt;%\n  mutate(list_sum = sum(list_col))\n\nTo apply mutate functions to a list-column, you need to wrap the function you want to apply in a map function.\n\ntibble(list_col = list(c(1, 5, 7), \n                       5, \n                       c(10, 10, 11))) %&gt;%\n  mutate(list_sum = map(list_col, sum))\n\n# A tibble: 3 × 2\n  list_col  list_sum \n  &lt;list&gt;    &lt;list&gt;   \n1 &lt;dbl [3]&gt; &lt;dbl [1]&gt;\n2 &lt;dbl [1]&gt; &lt;dbl [1]&gt;\n3 &lt;dbl [3]&gt; &lt;dbl [1]&gt;\n\n\nSince map() returns a list itself, the list_sum column is thus itself a list\n\ntibble(list_col = list(c(1, 5, 7), \n                       5, \n                       c(10, 10, 11))) %&gt;%\n  mutate(list_sum = map(list_col, sum)) %&gt;% \n  pull(list_sum)\n\n[[1]]\n[1] 13\n\n[[2]]\n[1] 5\n\n[[3]]\n[1] 31\n\n\nWhat could we do if we wanted it to be a vector? We could use the map_dbl() function instead!\n\ntibble(list_col = list(c(1, 5, 7), \n                       5, \n                       c(10, 10, 11))) %&gt;%\n  mutate(list_sum = map_dbl(list_col, sum))\n\n# A tibble: 3 × 2\n  list_col  list_sum\n  &lt;list&gt;       &lt;dbl&gt;\n1 &lt;dbl [3]&gt;       13\n2 &lt;dbl [1]&gt;        5\n3 &lt;dbl [3]&gt;       31\n\n\n\nNesting the gapminder data\nLet’s return to the nested gapminder dataset. I want to calculate the average life expectancy within each continent and add it as a new column using mutate(). Based on the example above, can you explain why the following code doesn’t work?\n\ngapminder_nested %&gt;% \n  mutate(avg_lifeExp = mean(data$lifeExp))\n\nI was hoping that this code would extract the lifeExp column from each data frame. But I’m applying the mutate to the data column, which itself doesn’t have an entry called lifeExp since it’s a list of data frames. How could I get access to the lifeExp column of the data frames stored in the data list? Using a map function of course!\nThink of an individual data frame as .x. Again, I will first figure out the code for calculating the mean life expectancy for the first entry of the column. The following code defines .x to be the first entry of the data column (this is the data frame for Asia).\n\n# the first entry of the \"data\" column\n.x &lt;- gapminder_nested %&gt;% pluck(\"data\", 1)\n.x\n\n# A tibble: 396 × 5\n   country      year lifeExp      pop gdpPercap\n   &lt;chr&gt;       &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n 1 Afghanistan  1952    28.8  8425333      779.\n 2 Afghanistan  1957    30.3  9240934      821.\n 3 Afghanistan  1962    32.0 10267083      853.\n 4 Afghanistan  1967    34.0 11537966      836.\n 5 Afghanistan  1972    36.1 13079460      740.\n 6 Afghanistan  1977    38.4 14880372      786.\n 7 Afghanistan  1982    39.9 12881816      978.\n 8 Afghanistan  1987    40.8 13867957      852.\n 9 Afghanistan  1992    41.7 16317921      649.\n10 Afghanistan  1997    41.8 22227415      635.\n# ℹ 386 more rows\n\n\nThen to calculate the average life expectancy for Asia, I could write\n\nmean(.x$lifeExp)\n\n[1] 60.0649\n\n\nSo copy-pasting this into the tilde-dot anonymous function argument of the map_dbl() function within mutate(), I get what I wanted!\n\ngapminder_nested %&gt;% \n  mutate(avg_lifeExp = map_dbl(data, ~{mean(.x$lifeExp)}))\n\n# A tibble: 5 × 3\n# Groups:   continent [5]\n  continent data               avg_lifeExp\n  &lt;chr&gt;     &lt;list&gt;                   &lt;dbl&gt;\n1 Asia      &lt;tibble [396 × 5]&gt;        60.1\n2 Europe    &lt;tibble [360 × 5]&gt;        71.9\n3 Africa    &lt;tibble [624 × 5]&gt;        48.9\n4 Americas  &lt;tibble [300 × 5]&gt;        64.7\n5 Oceania   &lt;tibble [24 × 5]&gt;         74.3\n\n\nThis code iterates through the data frames stored in the data column, returns the average life expectancy for each data frame, and concatonates the results into a numeric vector (which is then stored as a column called avg_lifeExp).\nI hear what you’re saying… this is something that we could have done a lot more easily using standard dplyr commands (such as summarise()). True, but hopefully it helped you understand why you need to wrap mutate functions inside map functions when applying them to list columns.\nEven if this example was less than inspiring, I promise the next example will knock your socks off!\nThe next exampe will demonstrate how to fit a model separately for each continent, and evaluate it, all within a single tibble. First, I will fit a linear model for each continent and store it as a list-column. If the data frame for a single continent is .x, then the model I want to fit is lm(lifeExp ~ pop + gdpPercap + year, data = .x) (check for yourself that this does what you expect). So I can copy-past this command into the map() function within the mutate()\n\n# fit a model separately for each continent\ngapminder_nested &lt;- gapminder_nested %&gt;% \n  mutate(lm_obj = map(data, ~lm(lifeExp ~ pop + gdpPercap + year, data = .x)))\ngapminder_nested\n\n# A tibble: 5 × 3\n# Groups:   continent [5]\n  continent data               lm_obj\n  &lt;chr&gt;     &lt;list&gt;             &lt;list&gt;\n1 Asia      &lt;tibble [396 × 5]&gt; &lt;lm&gt;  \n2 Europe    &lt;tibble [360 × 5]&gt; &lt;lm&gt;  \n3 Africa    &lt;tibble [624 × 5]&gt; &lt;lm&gt;  \n4 Americas  &lt;tibble [300 × 5]&gt; &lt;lm&gt;  \n5 Oceania   &lt;tibble [24 × 5]&gt;  &lt;lm&gt;  \n\n\nWhere the first linear model (for Asia) is\n\ngapminder_nested %&gt;% pluck(\"lm_obj\", 1)\n\n\nCall:\nlm(formula = lifeExp ~ pop + gdpPercap + year, data = .x)\n\nCoefficients:\n(Intercept)          pop    gdpPercap         year  \n -7.833e+02    4.228e-11    2.510e-04    4.251e-01  \n\n\nI can then predict the response for the data stored in the data column using the corresponding linear model. So I have two objects I want to iterate over: the data and the linear model object. This means I want to use map2(). When things get a little more complicated I like to have multiple function arguments, so I’m going to use a full anonymous function rather than the tilde-dot shorthand.\n\n# predict the response for each continent\ngapminder_nested &lt;- gapminder_nested %&gt;% \n  mutate(pred = map2(lm_obj, data, function(.lm, .data) predict(.lm, .data)))\ngapminder_nested\n\n# A tibble: 5 × 4\n# Groups:   continent [5]\n  continent data               lm_obj pred       \n  &lt;chr&gt;     &lt;list&gt;             &lt;list&gt; &lt;list&gt;     \n1 Asia      &lt;tibble [396 × 5]&gt; &lt;lm&gt;   &lt;dbl [396]&gt;\n2 Europe    &lt;tibble [360 × 5]&gt; &lt;lm&gt;   &lt;dbl [360]&gt;\n3 Africa    &lt;tibble [624 × 5]&gt; &lt;lm&gt;   &lt;dbl [624]&gt;\n4 Americas  &lt;tibble [300 × 5]&gt; &lt;lm&gt;   &lt;dbl [300]&gt;\n5 Oceania   &lt;tibble [24 × 5]&gt;  &lt;lm&gt;   &lt;dbl [24]&gt; \n\n\nAnd I can then calculate the correlation between the predicted response and the true response, this time using the map2()_dbl function since I want the output the be a numeric vector rather than a list of single elements.\n\n# calculate the correlation between observed and predicted response for each continent\ngapminder_nested &lt;- gapminder_nested %&gt;% \n  mutate(cor = map2_dbl(pred, data, function(.pred, .data) cor(.pred, .data$lifeExp)))\ngapminder_nested\n\n# A tibble: 5 × 5\n# Groups:   continent [5]\n  continent data               lm_obj pred          cor\n  &lt;chr&gt;     &lt;list&gt;             &lt;list&gt; &lt;list&gt;      &lt;dbl&gt;\n1 Asia      &lt;tibble [396 × 5]&gt; &lt;lm&gt;   &lt;dbl [396]&gt; 0.723\n2 Europe    &lt;tibble [360 × 5]&gt; &lt;lm&gt;   &lt;dbl [360]&gt; 0.834\n3 Africa    &lt;tibble [624 × 5]&gt; &lt;lm&gt;   &lt;dbl [624]&gt; 0.645\n4 Americas  &lt;tibble [300 × 5]&gt; &lt;lm&gt;   &lt;dbl [300]&gt; 0.779\n5 Oceania   &lt;tibble [24 × 5]&gt;  &lt;lm&gt;   &lt;dbl [24]&gt;  0.987\n\n\nHoly guacamole, that is so awesome!"
  },
  {
    "objectID": "blog/2019-08-19_purrr.html#advanced-exercise",
    "href": "blog/2019-08-19_purrr.html#advanced-exercise",
    "title": "Learn to purrr",
    "section": "Advanced exercise",
    "text": "Advanced exercise\nThe goal of this exercise is to fit a separate linear model for each continent without splitting up the data. Create the following data frame that has the continent, each term in the model for the continent, its linear model coefficient estimate, and standard error.\n\n\n# A tibble: 20 × 6\n   continent term         estimate std.error statistic  p.value\n   &lt;chr&gt;     &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1 Asia      (Intercept) -7.83e+ 2   4.83e+1  -16.2    1.22e-45\n 2 Asia      pop          4.23e-11   2.04e-9    0.0207 9.83e- 1\n 3 Asia      year         4.25e- 1   2.44e-2   17.4    1.13e-50\n 4 Asia      gdpPercap    2.51e- 4   3.01e-5    8.34   1.31e-15\n 5 Europe    (Intercept) -1.61e+ 2   2.28e+1   -7.09   7.44e-12\n 6 Europe    pop         -8.18e- 9   7.80e-9   -1.05   2.95e- 1\n 7 Europe    year         1.16e- 1   1.16e-2    9.96   8.88e-21\n 8 Europe    gdpPercap    3.25e- 4   2.15e-5   15.2    2.21e-40\n 9 Africa    (Intercept) -4.70e+ 2   3.39e+1  -13.9    2.17e-38\n10 Africa    pop         -3.68e- 9   1.89e-8   -0.195  8.45e- 1\n11 Africa    year         2.61e- 1   1.71e-2   15.2    1.07e-44\n12 Africa    gdpPercap    1.12e- 3   1.01e-4   11.1    2.46e-26\n13 Americas  (Intercept) -5.33e+ 2   4.10e+1  -13.0    6.40e-31\n14 Americas  pop         -2.15e- 8   8.62e-9   -2.49   1.32e- 2\n15 Americas  year         3.00e- 1   2.08e-2   14.4    3.79e-36\n16 Americas  gdpPercap    6.75e- 4   7.15e-5    9.44   1.13e-18\n17 Oceania   (Intercept) -2.10e+ 2   5.12e+1   -4.10   5.61e- 4\n18 Oceania   pop          8.37e- 9   3.34e-8    0.251  8.05e- 1\n19 Oceania   year         1.42e- 1   2.65e-2    5.34   3.19e- 5\n20 Oceania   gdpPercap    2.03e- 4   8.47e-5    2.39   2.66e- 2\n\n\nHint: starting from the gapminder dataset, use group_by() and nest() to nest by continent, use a mutate together with map to fit a linear model for each continent, use another mutate with broom::tidy() to get a data frame of model coefficients for each model, and a transmute to get just the columns you want, followed by an unnest() to re-expand the nested tibble.\nThe solution code is at the end of this post.\nIf you want to stop here, you will already know more than most purrr users. The remainder of this blog post involves little-used features of purrr for manipulating lists."
  },
  {
    "objectID": "blog/2019-08-19_purrr.html#keepdiscard-select_if-for-lists",
    "href": "blog/2019-08-19_purrr.html#keepdiscard-select_if-for-lists",
    "title": "Learn to purrr",
    "section": "Keep/Discard: select_if for lists",
    "text": "Keep/Discard: select_if for lists\nkeep() only keeps elements of a list that satisfy a given condition, much like select_if() selects columns of a data frame that satisfy a given condition.\nThe following code only keeps the gapminder continent data frames (the elements of the list) that have an average (among the sample of 5 rows) life expectancy of at least 70.\n\ngapminder_list %&gt;%\n  keep(~{mean(.x$lifeExp) &gt; 70})\n\n$Americas\n             country continent year lifeExp     pop gdpPercap\n1 Dominican Republic  Americas 1997  69.957 7992357  3614.101\n2        Puerto Rico  Americas 1987  74.630 3444468 12281.342\n3           Honduras  Americas 1992  66.399 5077347  3081.695\n4            Uruguay  Americas 2007  76.384 3447496 10611.463\n5         Costa Rica  Americas 1962  62.842 1345187  3460.937\n\n$Europe\n         country continent year lifeExp      pop gdpPercap\n1 United Kingdom    Europe 2002  78.471 59912431  29479.00\n2         Greece    Europe 1997  77.869 10502372  18747.70\n3        Belgium    Europe 2002  78.320 10311970  30485.88\n4        Croatia    Europe 2002  74.876  4481020  11628.39\n5    Netherlands    Europe 1967  73.820 12596822  15363.25\n\n$Oceania\n      country continent year lifeExp      pop gdpPercap\n1   Australia   Oceania 1982  74.740 15184200  19477.01\n2 New Zealand   Oceania 1997  77.550  3676187  21050.41\n3 New Zealand   Oceania 2007  80.204  4115771  25185.01\n4   Australia   Oceania 2007  81.235 20434176  34435.37\n5 New Zealand   Oceania 1952  69.390  1994794  10556.58\n\n\ndiscard() does the opposite of keep(): it discards any elements that satisfy your logical condition."
  },
  {
    "objectID": "blog/2019-08-19_purrr.html#reduce",
    "href": "blog/2019-08-19_purrr.html#reduce",
    "title": "Learn to purrr",
    "section": "Reduce",
    "text": "Reduce\nreduce() is designed to combine (reduces) all of the elements of a list into a single object by iteratively applying a binary function (a function that takes two inputs).\nFor instance, applying a reduce function to add up all of the elements of the vector c(1, 2, 3) is like doing sum(sum(1, 2), 3): first it applies sum to 1 and 2, then it applies sum again to the output of sum(1, 2) and 3.\n\nreduce(c(1, 2, 3), sum)\n\n[1] 6\n\n\naccumulate() also returns the intermediate values.\n\naccumulate(c(1, 2, 3), sum)\n\n[1] 1 3 6\n\n\nAn example of when reduce() might come in handy is when you want to perform many left_join()s in a row, or to do repeated rbinds() (e.g. to bind the rows of the list back together into a single data frame)\n\ngapminder_list %&gt;%\n  reduce(rbind)\n\n              country continent year lifeExp       pop  gdpPercap\n1              Gambia    Africa 1967  35.857    439593   734.7829\n2        Sierra Leone    Africa 1967  34.113   2662190  1206.0435\n3             Namibia    Africa 1997  58.909   1774766  3899.5243\n4   Equatorial Guinea    Africa 1992  47.545    387838  1132.0550\n5       Cote d'Ivoire    Africa 2002  46.832  16252726  1648.8008\n6  Dominican Republic  Americas 1997  69.957   7992357  3614.1013\n7         Puerto Rico  Americas 1987  74.630   3444468 12281.3419\n8            Honduras  Americas 1992  66.399   5077347  3081.6946\n9             Uruguay  Americas 2007  76.384   3447496 10611.4630\n10         Costa Rica  Americas 1962  62.842   1345187  3460.9370\n11            Lebanon      Asia 1967  63.870   2186894  6006.9830\n12              Nepal      Asia 1962  39.393  10332057   652.3969\n13        Yemen, Rep.      Asia 1992  55.599  13367997  1879.4967\n14              India      Asia 1972  50.651 567000000   724.0325\n15           Cambodia      Asia 1952  39.417   4693836   368.4693\n16     United Kingdom    Europe 2002  78.471  59912431 29478.9992\n17             Greece    Europe 1997  77.869  10502372 18747.6981\n18            Belgium    Europe 2002  78.320  10311970 30485.8838\n19            Croatia    Europe 2002  74.876   4481020 11628.3890\n20        Netherlands    Europe 1967  73.820  12596822 15363.2514\n21          Australia   Oceania 1982  74.740  15184200 19477.0093\n22        New Zealand   Oceania 1997  77.550   3676187 21050.4138\n23        New Zealand   Oceania 2007  80.204   4115771 25185.0091\n24          Australia   Oceania 2007  81.235  20434176 34435.3674\n25        New Zealand   Oceania 1952  69.390   1994794 10556.5757"
  },
  {
    "objectID": "blog/2019-08-19_purrr.html#logical-statements-for-lists",
    "href": "blog/2019-08-19_purrr.html#logical-statements-for-lists",
    "title": "Learn to purrr",
    "section": "Logical statements for lists",
    "text": "Logical statements for lists\nAsking logical questions of a list can be done using every() and some(). For instance to ask whether every continent has average life expectancy greater than 70, you can use every()\n\ngapminder_list %&gt;% every(~{mean(.x$life) &gt; 70})\n\n[1] FALSE\n\n\nTo ask whether some continents have average life expectancy greater than 70, you can use some()\n\ngapminder_list %&gt;% some(~{mean(.x$life) &gt; 70})\n\n[1] TRUE\n\n\nAn equivalent of %in% for lists is has_element().\n\nlist(1, c(2, 5, 1), \"a\") %&gt;% has_element(\"a\")\n\n[1] TRUE\n\n\nMost of these functions also work on vectors.\nNow go forth and purrr!"
  },
  {
    "objectID": "blog/2017-02-17-anova.html",
    "href": "blog/2017-02-17-anova.html",
    "title": "ANOVA",
    "section": "",
    "text": "Last week, practical statistics met to discuss all things ANOVA. Below you will find the slides from my talk, but read on if you would like to learn all about ANOVA."
  },
  {
    "objectID": "blog/2017-02-17-anova.html#when-anova-is-used-and-who-uses-it",
    "href": "blog/2017-02-17-anova.html#when-anova-is-used-and-who-uses-it",
    "title": "ANOVA",
    "section": "When ANOVA is used and who uses it?",
    "text": "When ANOVA is used and who uses it?\nANOVA is used far and wide by the scientific community and beyond. Unfortunately, scientists also frequently misuse ANOVA. A study by Wu et al. (2011) showed that from a survey of 10 leading Chinese medical journals in 2008, 446 articles used ANOVA, and of those articles, 59% of them used ANOVA incorrectly.\nIn this post, I will describe the general framework for ANOVA, the assumptions it requires, and many common pitfalls. While I will introduce a few equations, I will not provide extensive details or derivations. Instead, I will focus on developing intuition for why these equations make sense for the type of problem at hand.\nLet’s start by getting an idea of what kind of questions one can answer using ANOVA. Three examples are presented below.\n\nIs there a difference between the average number of times articles are shared on social media based on day of the week?\nIs there a difference in average waiting room times for a set of 3 different hospitals?\nDoes the presence of other people have an influence on the amount of time taken for a person to help someone in distress?\n\nThe third question is not like the others; can you see why? The first two questions are asking about a difference between mean values of some outcome (article shares or waiting room time) across multiple groups (day of week or hospital). The last question, however, does not seem to satisfy this criteria: while it has a clear outcome (amount of time taken for a person to help someone in distress), it has no obvious groupings.\nDarley and Latané (1969) used ANOVA to answer the question of whether the presence of other people had an influence on the amount of time taken for a person to help someone in distress by setting up an appropriate experiment. In doing so, they were the first to demonstrate the “bystander effect”.\nIn their experiment, the experimenter had the subject wait in a room with either 0, 2, or 4 other individuals. The experimenter announces that the study will begin shortly and walks into an adjacent room. In a few moments the people in the waiting room hear her fall and cry out. The outcome variable is the number of seconds it takes the subject to help the experimenter, and the grouping variable is the number of other people in the room (0, 2, or 4).\nGiven these examples, the definition of ANOVA provided in the following paragraph shouldn’t come as much of a surprise."
  },
  {
    "objectID": "blog/2017-02-17-anova.html#anova-in-a-nutshell",
    "href": "blog/2017-02-17-anova.html#anova-in-a-nutshell",
    "title": "ANOVA",
    "section": "ANOVA in a nutshell",
    "text": "ANOVA in a nutshell\nANalysis Of VAriance (ANOVA), is a widely used method designed for comparing differences in means among three or more groups.\nFor example, we might be interested in comparing the average waiting times in the emergency rooms of three different hospitals. Or perhaps we have conducted a clinical trial comparing the effectiveness of five different drugs for reducing blood pressure.\nThe key is that each individual falls into a group described by a (categorical) grouping variable (e.g. hospital or drug group) and for each individual we measure some continuous outcome (e.g. waiting time or blood pressure).\nAlthough there are multiple types of ANOVA, the simplest (“one-way” ANOVA) can be thought of as a generalization of a two-sample t-test. One-way ANOVA involves testing the omnibus hypothesis that k population means are identical:\n\\[H_0: \\mu_1 = \\mu_2 = ... = \\mu_k.\\]\nNote that the two-sample t-test tested the hypothesis that two population means are equal (i.e. k = 2).\nThe alternative hypothesis for ANOVA states that any one of the population mean equalities does not hold:\n\\[H_1: \\mu_i \\neq \\mu_j~~ \\text{ for some } ~~i \\neq j.\\]\nIt is important to note that a rejection of the null hypothesis does not tell you which of the population means differ. It only tells you that there is some population whose mean is different from at least one other population (it could be that all of the means are different from one another!)\n\nA simple toy example: hospital waiting times\nSuppose that we have three hospitals, let’s call them A, B and C (creative names, I know). We are interested in whether all three hospitals have the same the average waiting time for the emergency room.\n\n\n\n\n\nThree hospitals: A, B and C.\n\n\n\n\nWe measured the waiting time for 20 unique individuals at each of these three hospitals (so there are 60 individuals in total). These waiting times (in hours) are recorded below.\n\n\n\n\n\nHospital A\nHospital B\nHospital C\n\n\n\n\n1.8\n0.9\n1.4\n\n\n1.4\n0.7\n2.1\n\n\n0.7\n2.6\n1.4\n\n\n0.8\n1.7\n1.2\n\n\n0.5\n2.5\n2.1\n\n\n2.1\n2.4\n2.3\n\n\n0.9\n2.4\n1.7\n\n\n2.2\n2.3\n1.2\n\n\n1.2\n2.0\n1.1\n\n\n1.3\n1.7\n1.3\n\n\n1.1\n2.1\n0.3\n\n\n1.1\n0.9\n1.7\n\n\n0.4\n2.7\n1.5\n\n\n1.4\n1.5\n1.7\n\n\n0.8\n2.0\n2.0\n\n\n1.1\n1.9\n0.8\n\n\n0.6\n2.6\n2.0\n\n\n1.1\n2.4\n2.4\n\n\n1.6\n1.5\n2.2\n\n\n0.9\n1.7\n2.0\n\n\n\n\n\nAs pictures tend to be more informative than tables, we present a plot of these waiting times below. To aid the visualization, the x-position of each point is jittered to gently increase the space between the points.\nMost people seem to wait over an hour, with some unlucky individuals waiting for almost 3 hours. The mean waiting time for each hospital is highlighted by a red bar.\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\nThe question of interest is whether or not the average waiting time for each of the three hospitals is the same. This question might be naively interpreted as: “are the red bars at the same height?”, i.e. is there a difference between the waiting times for the sample of 20 patients from each hospital. The intended question is actually asking about equality between the average waiting times from the population of all patients who have ever, and will ever, wait in these waiting rooms, regardless of whether they fall in our sample.\nAlthough the sample means clearly aren’t identical (the red bars are all at different heights), do we have enough evidence to show that the underlying population waiting time means are different, or are the differences that we observe are simply reflection of the inherent noise in the data?\nThis is the question that lies at the heart of hypothesis testing.\n\n\nWhy is ANOVA called “Analysis of Variance”?\nWhat does comparing means have to do with variability?\nQuite a lot it turns out… simply by asking “are the means different”, we are essentially asking a question about whether the variance of the means is large. However, the variability that we observe between the means themselves only makes sense relative to the overall variance in the data. For example, if the individual observations themselves are extremely variable, then it might be reasonable to expect that the observed group-specific averages might exhibit some variance (even if the underlying true averages are identical).\nThere are two types of variance at play here:\n\nwithin-group variability: the variance of the individual observations within a group, and\nbetween-group variability: the variance between the averages of the groups.\n\nIn the figure below, the red bars in the left panel highlight the within-group variance, while the red bars in the right panel highlight the between-group variance.\n\n\n\n\n\n\n\n\n\nThe basic idea is that if the variability between the groups is greater than the variability within the groups, then we have evidence that the differences between the groups is not simply reflecting random noise.\nQuantifying the within and between group variability is typically done by calculating a mean sum of squares: add up the squared vertical distances and divide by the degrees of freedom.\nThis means that we are comparing the\n\nbetween sum of squares (BSS): the squared distances from the group means (average over all individuals separately for each hospital) to the global mean (average over all individuals),\n\nto the\n\nwithin sum of squares (WSS): the squared distances from each individual to their group mean (average over all individuals within the same hospital).\n\nUsing this idea we can formulate a test statistic:\n\n\n\n\n\n\n\n\n\nTo spell it out mathematically, we can write these expressions as follows:\n\\[WSS =\\sum_{i = 1}^K \\sum_{j = 1}^{n_i} (y_{ij} - \\overline{y}_{i\\cdot})^2 ~~~~~ \\text{ and } ~~~~~ BSS = \\sum_{i=1}^K (\\overline{y}_{\\cdot \\cdot} - \\overline{y}_{i\\cdot})^2\\]\nwhere \\(y_{ij}\\), defines the waiting room time (outcome) for patient \\(j\\) from hospital \\(i\\), \\(\\overline{y}_{\\cdot \\cdot}\\) defines the global average waiting time and \\(\\overline{y}_{i \\cdot}\\) defines the average waiting time for hospital \\(i\\). \\(K\\) is the number of hospitals, and \\(n_i\\) is the number of patients sampled from hospital \\(i\\).\nNote that in the test statistic above, each quantity is scaled by its degrees of freedom, which when comparing the groups is the number of groups minus 1 (\\(K-1\\)), and when comparing individuals is the number of individuals minus the number of groups (\\(N-K\\))."
  },
  {
    "objectID": "blog/2017-02-17-anova.html#what-assumptions-are-required",
    "href": "blog/2017-02-17-anova.html#what-assumptions-are-required",
    "title": "ANOVA",
    "section": "What assumptions are required?",
    "text": "What assumptions are required?\nIf our data satisfies a few parametric assumptions, then we can show that this test statistic follows an \\(F\\) distribution and we can do a straightforward parametric hypothesis test:\n\\[\\text{p-value} = P\\left(F_{K-1, N-k} \\geq \\frac{BSS/(K-1)}{WSS/(N-K)}\\right).\\]\nThese assumptions are as follows\n\nAssumption 1: The samples are independent.\nIndependence is an extremely common assumption that is hard to test in general.\n\n\nAssumption 2: The data are normally distributed.\nNot being a fan of such distributional assumptions myself, I am inclined to point the reader in the direction of non-parametric versions of ANOVA, including the Kruskal-Wallis test, however since this is a blog post about ANOVA, we will leave non-parametric readings to the interested parties. Those wishing to test the normality of their data can do so using a variety of methods such as plotting a QQ-plot, or using a normality test (see the Wikipedia page on normality tests).\n\n\nAssumption 3: Each group has the same variance.\nThe common variance assumption can be tested using common tests, such as the Bartlett test and the Fligner-Killeen test, which are easily implemented in R."
  },
  {
    "objectID": "blog/2017-02-17-anova.html#working-through-our-waiting-times-example",
    "href": "blog/2017-02-17-anova.html#working-through-our-waiting-times-example",
    "title": "ANOVA",
    "section": "Working through our waiting times example",
    "text": "Working through our waiting times example\nLet’s examine these assumptions for our Hospital waiting times example.\nUnfortunately, independence is hard to judge statistically, but if, for example, each person was randomly selected from a splattering of visitors to the waiting room at different times (rather than, e.g. selecting 5 members of the same family all of whom came to the hospital together in some freak accident), then the assumption of independence is probably ok.\nThe figure below plots the density estimation for the waiting times from each hospital. We know that if our data is normally distributed, it should look vaguely like a bell-curve.\n\n\n\n\n\n\n\n\n\nOne might argue that these are some pretty funky-looking bell-curves…\nHowever, as I was the one who simulated this data in the first place, I can assure you that they are in fact normally distributed, and you can use this as a lesson on the difficult of drawing conclusions on normality from small samples (in this case, we have 20 observations in each group).\nA Shapiro-Wilk test for normality provides p-values of 0.39, 0.087, 0.52 for hospitals A, B and C, respectively. Although none of these values are “significant” (even unadjusted for multiple testing), we have stumbled upon another lesson: small p-values (\\(p = 0.087\\) for hospital B) can certainly occur when the null hypothesis is true (in this case, the null hypothesis is that the data are normally distributed)! Remember that when the null hypothesis is true, p-values are uniformly distributed.\nFinally, based on a visual assessment, the common variance assumption is probably fairly reasonable (and, again, since I simulated this data, I can confirm that the variance is the same for each hospital).\nTo test this formally, Bartlett’s test for homogeneity of variances yields a p-value of 0.68, indicating that we do not have evidence that the variances are different.\nWe have now concluded that the assumptions for ANOVA are satisfied, and can proceed to do our calculations.\nCalculating the between-sum-of-squares (BSS) and scaling by the degrees of freedom (the number of groups minus 1), and the within-sum-of-squares (WSS) and scaling by the degrees of freedom (the number of observations minus the number of groups), we get that\n\\[\\frac{BSS}{K - 1} = \\frac{5.94}{3 - 1} = 2.97 ~~~~~~ \\text{and} ~~~~~ \\frac{WSS}{N - K} = \\frac{16.96}{60 - 3} = 0.30.\\]\nOur test statistic turns out to be quite large indeed:\n\\[F = \\frac{BSS/(K-1)}{WSS/(N-k)} = \\frac{2.97}{0.3} = 9.98.\\]\nSince we are confident that the ANOVA assumptions are satisfied, this F-statistic must follow an F distribution with suitable degrees of freedom. Our p-value can thus be calculated as follows:\n\\[P(F_{2, 53} \\geq 9.98) = 0.000192\\]\nAnd we can claim to have evidence that the three group means are not all identical. Note that we can interpret this as the distances between the group means and the global mean is quite large relative to the distances between the individual observations and the group means. Recall that these distances are scaled by their respective degrees of freedom.\nRather than conducting these calculations by hand, in R, one could simply use the aov() function:\n\nsummary(aov(time ~ hospital, data = data))\n\n\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nvariable     2  5.938  2.9688   9.981 0.000192 ***\nResiduals   57 16.955  0.2975                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nobtaining the same p-value.\n\nANOVA as a linear model\nSo far we have discussed ANOVA purely as a hypothesis test comparing two different types of variability. It is, however, more common to talk about ANOVA as a linear model.\nThe anova linear model can be written as follows:\n\n\n\n\n\n\n\n\n\n\\(\\mu\\) represents the overall average wait time across all hospitals, and \\(\\tau_i\\) represents the amount of time that is either added or subtracted from the overall average as a result of being at hospital \\(i\\). To get the average wait time for hospital \\(i\\) we can calculate \\(\\mu_i := \\mu + \\tau_i\\).\nFinally, \\(\\epsilon_{ij}\\) represents the “noise” term; the quantity that defines how the waiting time for individual \\(j\\) differs from the mean (within their group).\nWe typically assume that the expected value (the average over the population) for \\(\\epsilon_{ij}\\) is equal to zero, and that the \\(\\tau_i\\)s add up to zero. Note that if we do not assume that \\(\\tau_i\\)s sum to zero, then the model is “over-parametrized” in that there would be an infinite number of ways to define the \\(\\mu\\) and \\(\\tau_i\\)s such that they add up to the group mean \\(\\mu_i\\).\nThe question that is sure to be on the edge of your tongue is “how is this possibly equivalent to the hypothesis test discussed above?”\nThe answer is simple and can be summarized by the diagram below.\n\n\n\n\n\n\n\n\n\nSpecifically, if the hospital-specific effects \\(\\tau_A, \\tau_B,\\) and \\(\\tau_C\\) are all equal to zero, then the average effect across all groups is the same: \\(\\mu_A = \\mu_B = \\mu_C = \\mu\\)."
  },
  {
    "objectID": "blog/2017-02-17-anova.html#common-pitfalls-of-anova-and-alternative-approaches",
    "href": "blog/2017-02-17-anova.html#common-pitfalls-of-anova-and-alternative-approaches",
    "title": "ANOVA",
    "section": "Common pitfalls of ANOVA and alternative approaches",
    "text": "Common pitfalls of ANOVA and alternative approaches\nDespite its perceived simplicity, scientists frequently misuse ANOVA. A study by Wu et al. (2011) showed that from a survey of 10 leading Chinese medical journals in 2008, 446 articles used ANOVA, and of those articles, 59% of them used ANOVA incorrectly.\nBelow we will discuss many of the common pitfalls.\n\nUsing one-way ANOVA when there is more than one grouping variable\nSuppose that instead of solely measuring waiting times from emergency waiting rooms at each hospital, we instead measured the waiting times from three medical departments from each hospital: surgery, pediatrics, and dentistry.\n\n\n\n\n\n\n\n\n\nIn this scenario we should adapt the model to take the second grouping variable into account. This is called two-way ANOVA, and the model can be adapted as follows:\n\\[y_{ijk} = \\mu + \\tau_i + \\gamma_j + \\beta_{ij} + \\epsilon_{ijk}\\]\nwhere \\(\\tau_i\\) represents the hospital-specific effect on waiting time and \\(\\gamma_j\\) represents the department-specific effect. \\(\\beta_{ij}\\) represents an interaction term between these two effects.\nAnother adaptation of ANOVA when the second grouping variable (medical department) is not the same across each hospital is called nested ANOVA (see fig below). For example, perhaps we are comparing both hospitals and medical departments, but we are not examining the same medical department in each hospital.\n\n\n\n\n\n\n\n\n\n\n\nConducting ANOVA multiple times for multiple outcomes\nSuppose that instead of simply being interested in whether there is a difference between waiting time for each hospital, we were also interested in differences in average length of hospital stay and cost of visit. Then the incorrect way to proceed would be to generate three separate ANOVA models and draw our conclusions separately for each model. This reeks of multiple testing issues and does not take into account any dependence between the different outcome variables.\nInstead, one should use Multivariate Analysis of Variance (MANOVA), which can be written as follows:\n\\[ \\left[\\begin{array}a  y_{ij1} \\\\ y_{ij2} \\\\ y_{ij3} \\end{array} \\right]= \\mu + \\tau_i +  \\epsilon_{ij}\\] where\n\n\\(y_{ij1}\\) is the waiting time for individual \\(j\\) from hospital \\(i\\),\n\\(y_{ij2}\\) is the length of hospital stay for individual \\(j\\) from hospital \\(i\\), and\n\\(y_{ij3}\\) is the cost of the visit for individual \\(j\\) from hospital \\(i\\).\n\n\n\nIncorrectly conducting multiple pair-wise comparisons following ANOVA\nUpon obtaining a “significant” ANOVA p-value, a common mistake is to then go and test all of the pairwise differences to identify which of the populations had different means. This is another example of multiple hypothesis testing, and corrections on these p-values must be made. See the Wikipedia page for details on this common issue.\n\n\nUsing ANOVA to analyse repeated-measures data\nWhat if, instead of having measured the waiting room times on a different set of 20 people at each hospital (left-panel in fig below), we instead measured the waiting room times on the same set of 20 people at each hospital (right-panel in fig below)?\n\n\n\n\n\n\n\n\n\nWe have certainly violated the assumption that our observations are independent. Fortunately, repeated measures ANOVA (rANOVA) is a method for exactly this situation.\nBasically, rANOVA simply splits the within sum of squares into the individual-level sum of squares and the random error sum of squares. An excellent article describing rANOVA can be found here. A common repeated measures experimental design involves observations being made at different time points (as opposed to at different hospitals).\nAccording to the Wikipedia article,\n\nrANOVA is not always the best statistical analysis for repeated measure designs. The rANOVA is vulnerable to effects from missing values, imputation, unequivalent time points between subjects and violations of sphericity. These issues can result in sampling bias and inflated rates of Type I error. In such cases it may be better to consider use of a linear mixed model."
  },
  {
    "objectID": "blog/2017-02-17-anova.html#freqently-asked-questions",
    "href": "blog/2017-02-17-anova.html#freqently-asked-questions",
    "title": "ANOVA",
    "section": "Freqently asked questions",
    "text": "Freqently asked questions\n\nCan I use ANOVA if my data violates the assumption of common variances?\nAccording to this post on Stats Stack Exchange, if the sample size in each group is similar, and the difference between variance isn’t too bad, you should be ok.\nAccording to user gung, with similar group sizes there is a rule of thumb that states that\n\nANOVA is robust to heterogeneity of variance so long as the largest variance is not more than 4 times the smallest variance.\n\nI’ll admit that I haven’t checked this claim, but I’d be willing to believe it.\nIf, however, your data has wildly different group variances or varying group sizes, then I’m not entirely sure of what options exist (note that the non-parametric alternative also assumes that the population variances are similar).\n\n\nIf my data are not normal, can I simply transform it and draw the conclusions as normal?\nYes, probably.\n\n\nHow does the ANOVA for model comparison work?\nIt is common to use the anova() function in R to compare two different models. Specifically, it compares nested models wherein one model consists of a subset of the set of variables of the other model.\nNote that the use of the word “nested” here has nothing to do with the nested anova discussed above in which the grouping variables themselves (rather than the models) were nested.\nThe comparison being made by ANOVA in this situation is whether the residual sum of squares (which is essentially the within sum of squares from one-way ANOVA) for model 1 (the larger model) is larger than the residual sum of squares for model 2 (the smaller model).\nSpecifically, it calculates the F-statistic\n\\[F = \\frac{(RSS_{\\text{model 2}} - RSS_{\\text{model 1}})/(p_1 - p_2)}{RSS_{\\text{model 1}}/(n - p_1)}\\]\nThe idea is that since model 2 is a special case of model 1, model 1 is more complex so \\(RSS_{\\text{model 2}}\\) will always be as least as large as \\(RSS_{\\text{model 1}}\\). The question is whether the difference is “statistically significant”.\nNote that if you call the anova() function with a single model, it will compare the first variable in the model to a baseline model with no predictors. If there is a second variable, it compares the model with both variables against the model with just one variable, and so on and so forth.\nSee this set of slides by James Steiger if you’re interested in further details."
  },
  {
    "objectID": "blog/2017-02-17-anova.html#the-end",
    "href": "blog/2017-02-17-anova.html#the-end",
    "title": "ANOVA",
    "section": "The end…",
    "text": "The end…\nWhile this post has only scratched the surface of all things ANOVA, I hope that you have developed a general intuition for how ANOVA works, what assumptions are needed to make things go, and common pitfalls to avoid."
  },
  {
    "objectID": "blog/2019-03-07_reproducible_pipeline.html",
    "href": "blog/2019-03-07_reproducible_pipeline.html",
    "title": "A quick guide to developing a reproducible and consistent data science workflow",
    "section": "",
    "text": "When you’re learning to code and perform data analysis, it can be overwhelming to figure out how to structure your projects. To help data scientists develop a reproducible and consistent workflow, I’ve put together a short GitHub-based document with some guiding advice: https://github.com/rlbarter/reproducibility-workflow\nIf you’re interested in contributing or improving this document, please get in touch, or even better, submit a pull request https://github.com/rlbarter/reproducibility-workflow!\nThe document as of writing is shown below."
  },
  {
    "objectID": "blog/2019-03-07_reproducible_pipeline.html#reproducbility-workflow",
    "href": "blog/2019-03-07_reproducible_pipeline.html#reproducbility-workflow",
    "title": "A quick guide to developing a reproducible and consistent data science workflow",
    "section": "Reproducbility workflow",
    "text": "Reproducbility workflow\nEvery project should consist of a single well structured directory with meaningful subdirectories. Every project should be its own git repository that is hosted on GitHub.\nData cleaning and analyses should be carefully documented in a Jupyter Notebook or R markdown file and should be created with reproducibility in mind. Everyone on the team (and future you) should be able to re-create what you have performed.\nThe overall purpose is to have an organized project structure in place so that the project is easily approachable to many different individuals.\n\nSomeone unfamiliar with your project should be able to look at your computer files and understand in detail what you did and why - Bill Noble\n\nAn example of a reproducible project that follows this workflow lives in the project_example/ folder.\n\nProject Structure\nThe project will typically consist of the following subdirectories:\n\nData\nOriginal raw data files data should be backed up on something like Google Drive, Dropbox or Box. The raw data itself should never be touched manually. Instead, you should have scripts or notebooks that load the raw data into an R or Python environment for in-environment data manipulation (this will not modify the raw data files themselves).\nAny data that is produced by code should be saved in the data/processed_data/ subdirectory.\n\n\nDocuments\nThis is a good place to keep meeting notes, data dictionaries, and any other associated materials.\n\n\nCode\nThere are three types of code documents:\n\nFunction scripts (.R, .py): scripts that contain reusable functions that will be called in the action scripts below (and possibly in the exploration notebooks). By convention, function scripts are given the name xx_funs_yy.R, where xx is a number and yy describes what the functions are for (e.g. 01_funs_clean_data.R).\nAction scripts (.R, .py): scripts that perform activities such as a detailed data cleaning pipeline, or running many models. Often these scripts will load in data, do something to it (e.g. clean it or fit a model to it) and will then save a new object (such as a cleaned dataset or model results). By convention, action scripts are given the name xx_do_yy.R, where xx is a number and yy describes what action is undertaken by running the script (e.g. 01_do_clean_data.R).\nExploration notebooks (.Rmd, .ipynb): R Markdown or Jupyter notebook files that are used to produce figures and explanatory files that contain figures and explanations of data cleaning steps and results of analyses. These are the files that an external viewer would find useful to understand your data and analysis.\n\nScripts that are run sequentially are numbered accordingly. An example of a project structure is shown below. Note that in the example below the functions folder is nested as a subdirectory of the scripts folder which contains the actionable scripts. This makes sense when the functions are only called in the actionable scripts (but not in the exploration notebooks).\nproject\n│   README.md\n└───data/\n│       └───raw_data/\n│           │   data_orig.csv\n│       └───processed_data/\n│           │   data_clean.csv\n│       └───results/\n│           │   model_results.csv\n└───documents/\n│       meeting_notes.md\n│       data_dictionary.md\n└───code/\n│       └───exploration/\n│           │   01_data_exploration.Rmd\n│           │   02_model_results.Rmd\n│       └───scripts/\n│           │   01_do_clean_data.R\n│           │   02_do_model_data.R\n│           └───functions/\n│               │   01_funs_clean_data.R\n│               │   02_funs_model_data.R\n\n\n\n\nSyntax and conventions\nAll filenames are always lowercase and use underscores to separate words.\nCode should follow an appropriate style guide:\n\nR: Tidyverse Style Guide (based on the R Google Style Guide)\nPython: Google Style Guide\n\n\n\nResources\n\nI wrote a much more detailed blog post on my workflow a few years ago that can be found here: http://www.rebeccabarter.com/blog/2017-08-16-data-science-workflow/. My workflow has changed a bit since then, but the underlying ideas are all more or less the same.\nWilliam Noble’s article on organizing computational biology projects: https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1000424\nMarwick, Boettiger and Mullen’s article on packaging data analytical work reproducibly: https://ro.uow.edu.au/cgi/viewcontent.cgi?article=6445&context=smhpapers\n\n\n\nAcknowledgements\nThanks very much to Ciera Martinez for sharing her project workflow.\nI’d also like to acknowledge the Meta Research and Best Practices working group (formerly the Reproducility working group) at the Berkeley Institute for Data Science (BIDS) for insightful discussions that have helped me form my own workflow over the years."
  },
  {
    "objectID": "blog/2020-02-05_rstudio_conf.html",
    "href": "blog/2020-02-05_rstudio_conf.html",
    "title": "5 useful R tips from rstudio::conf(2020) - tidy eval, piping, conflicts, bar charts and colors",
    "section": "",
    "text": "This was my second year attending rstudio::conf() as a diversity scholar (and my first time as a speaker), and I was yet again blown away by the friendliness of the community and the quality of the talks. Over the course of the week, I met so many wonderful and talented humans, and learnt so many incredibly useful things. This post is all about the little tips and tricks that I picked up from watching many fantastic presentations, attendees live tweeting, and having many insightful conversations.\n\nTip 1: Tidy evaluation\nTidy eval is one of those terms that seems to float around a lot in the R community, but I feel like 99% of us don’t really get what tidy eval is, nor why we should care about it. Turns out, unless we’re getting deep into package development, we probably don’t need to be up to speed with tidy eval. The only part of tidy eval that I know is this: how to supply column names as unquoted arguments in functions. All of the resources I found on tidy eval go into waaay more detail about how it works, and maybe one day I’ll care about that, but that time is not now.\nFor this (and the next few) example, I’ll use the midwest dataset from the ggplot2 package, the first 6 rows of which are shown below.\n\nlibrary(tidyverse)\n\n\nhead(midwest)\n\n# A tibble: 6 × 28\n    PID county    state  area poptotal popdens…¹ popwh…² popbl…³ popam…⁴ popas…⁵\n  &lt;int&gt; &lt;chr&gt;     &lt;chr&gt; &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;   &lt;int&gt;   &lt;int&gt;   &lt;int&gt;   &lt;int&gt;\n1   561 ADAMS     IL    0.052    66090     1271.   63917    1702      98     249\n2   562 ALEXANDER IL    0.014    10626      759     7054    3496      19      48\n3   563 BOND      IL    0.022    14991      681.   14477     429      35      16\n4   564 BOONE     IL    0.017    30806     1812.   29344     127      46     150\n5   565 BROWN     IL    0.018     5836      324.    5264     547      14       5\n6   566 BUREAU    IL    0.05     35688      714.   35157      50      65     195\n# … with 18 more variables: popother &lt;int&gt;, percwhite &lt;dbl&gt;, percblack &lt;dbl&gt;,\n#   percamerindan &lt;dbl&gt;, percasian &lt;dbl&gt;, percother &lt;dbl&gt;, popadults &lt;int&gt;,\n#   perchsd &lt;dbl&gt;, percollege &lt;dbl&gt;, percprof &lt;dbl&gt;, poppovertyknown &lt;int&gt;,\n#   percpovertyknown &lt;dbl&gt;, percbelowpoverty &lt;dbl&gt;, percchildbelowpovert &lt;dbl&gt;,\n#   percadultpoverty &lt;dbl&gt;, percelderlypoverty &lt;dbl&gt;, inmetro &lt;int&gt;,\n#   category &lt;chr&gt;, and abbreviated variable names ¹​popdensity, ²​popwhite,\n#   ³​popblack, ⁴​popamerindian, ⁵​popasian\n\n\nSuppose that you want to write a function that plays nicely with the tidyverse (e.g. can take unquoted column names just like tidyverse functions do). Such an example might be one that makes a plot where the user supplies the variables:\n\nplotMidwest &lt;- function(var1, var2) {\n  ggplot(midwest) +\n    geom_point(aes(x = var1, y = var2))\n}\n\nTheoretically, this plot should be able to take the arguments popdensity and percbelowpoverty and provide me with a scatterplot of these two columns against one another:\n\nplotMidwest(popdensity, percbelowpoverty)\n\nError in `geom_point()`:\n! Problem while computing aesthetics.\nℹ Error occurred in the 1st layer.\nCaused by error in `FUN()`:\n! object 'popdensity' not found\n\n\nbut this doesn’t work! Perhaps if I quote the variable names it will work:\n\nplotMidwest(\"popdensity\", \"percbelowpoverty\")\n\n\n\n\n\n\n\n\nThe above code didn’t throw an error, but this is literally plotting the word “popdensity” against the word “percbelowpoverty”, which isn’t what I wanted to do!\nThe secret to providing column names as arguments to a function is…. tidy eval! Fortunately even without understanding why it works (something about delaying evaluation until later in the execution path blah blah blah), you can use tidy eval. The way you do this is to envelop your arguments within the function in curly braces { }:\n\nplotMidwestTidy &lt;- function(var1, var2) {\n  ggplot(midwest) +\n    geom_point(aes(x = {{ var1 }}, y = {{ var2 }}))\n}\n\nNow when I provide my column names as unquoted variables, I actually get the scatterplot I wanted!\n\nplotMidwestTidy(popdensity, percbelowpoverty)\n\n\n\n\n\n\n\n\nNote that my tidy eval version of my plotMidwest() function isn’t designed to take quoted variable names. The following code yields the same plot as before of the word “popdensity” against the word “percbelowpoverty”.\n\nplotMidwestTidy(\"popdensity\", \"percbelowpoverty\")\n\n\n\n\n\n\n\n\nA neat little trick I learned from Dewey Dunnington’s talk is that you can use .data as placeholder for the data object inside aes(). This means that, if var is a character column name, you can access the quoted column from the data object using .data[[var]]:\n\nplotMidwestQuoted &lt;- function(var1, var2) {\n  ggplot(midwest) +\n    geom_point(aes(x = .data[[var1]], y = .data[[var2]]))\n}\nplotMidwestQuoted(\"popdensity\", \"percbelowpoverty\") \n\n\n\n\n\n\n\n\nDewey’s talk is full of gems (check out his slides: https://fishandwhistle.net/slides/rstudioconf2020/#1).\n\n\nTip 2: Pipe into later arguments of a function using .\nWhile I didn’t technically learn this one from a talk at rstudio::conf(2020), I did run into an old friend Garth Tarr who told me about this piping trick: if you want to pipe an object into any argument other than the first one, you can do so using the . placeholder. For instance, let’s say that you want to use the lm() function to fit a linear model, and because you, like me, are pipe-obsessed, you want to pipe the data into lm().\nSadly, when I try to pipe the midwest data into my lm() function for regressing population density (popdensity) against poverty rate (percbelowpoverty), I get an error.\n\nmidwest %&gt;%\n  lm(popdensity ~ percbelowpoverty)\n\nError in as.data.frame.default(data): cannot coerce class '\"formula\"' to a data.frame\n\n\nWhat’s going wrong? There are two things you need to understand about what’s happening here:\n\nThe argument order of lm() is: lm(formula, data), i.e. the data is the second argument of the lm function (whereas all tidyverse functions have the data as the first argument)\nThe pipe, %&gt;%, automatically places the object to the left of the pipe into the first argument of the function to the right of the pipe\n\nso midwest %&gt;% lm(popdensity ~ percbelowpoverty) above is equivalent to lm(formula = iris, data = Sepal.Length ~ Sepal.Width), which has the arguments reversed.\nInstead of abandoning the trusty pipe (the thought of which fills me with sadness), I can instead pipe the data into the second argument using . as a placeholder for the position into which I want to pipe the data:\n\nmidwest %&gt;% lm(popdensity ~ percbelowpoverty, .)\n\n\nCall:\nlm(formula = popdensity ~ percbelowpoverty, data = .)\n\nCoefficients:\n     (Intercept)  percbelowpoverty  \n         4068.06            -77.56  \n\n\nNote that I could alternatively name the formula argument, which would automatically pipe the object into the first unassigned argument (which in our case is the data argument), but this becomes cumbersome when you have many arguments.\n\nmidwest %&gt;% lm(formula = popdensity ~ percbelowpoverty)\n\n\nCall:\nlm(formula = popdensity ~ percbelowpoverty, data = .)\n\nCoefficients:\n     (Intercept)  percbelowpoverty  \n         4068.06            -77.56  \n\n\n\n\nTip 3: Function conflicts workaround (no more dplyr::select())\nHave you ever loaded the MASS R package and found that select() from dplyr no longer works? I regularly get this type of error when functions from different packages have the same name as tidyverse functions I use on the reg. Let’s pretend that I really want to select the cut variable from the diamonds dataset from the ggplot2 package.\nThe first 6 rows of the diamonds dataset are shown below:\n\nhead(diamonds)\n\n# A tibble: 6 × 10\n  carat cut       color clarity depth table price     x     y     z\n  &lt;dbl&gt; &lt;ord&gt;     &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43\n2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31\n3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31\n4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63\n5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75\n6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48\n\n\nSadly if I loaded the MASS library before trying to select cut from diamonds, I get the following error:\n\nlibrary(MASS)\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\ndiamonds %&gt;% select(cut)\n\nError in select(., cut): unused argument (cut)\n\n\nThis is because by loading MASS, I have overwritten dplyr’s select() function (hence the warning). The normal workaround is to be explicit about what select() function you want using dplyr::select() as in\n\ndiamonds %&gt;% dplyr::select(cut)\n\n# A tibble: 53,940 × 1\n   cut      \n   &lt;ord&gt;    \n 1 Ideal    \n 2 Premium  \n 3 Good     \n 4 Premium  \n 5 Good     \n 6 Very Good\n 7 Very Good\n 8 Very Good\n 9 Fair     \n10 Very Good\n# … with 53,930 more rows\n\n\nBut this can be really annoying if you have a lot of select()s in your code (because you have to go through and apply dplyr:: to each one). It turns out (as tweeted by Birunda Chelliah - I’m not where she learned it though) that a better workaround is to set conflict hierarchies at the top of your document (conflict_prefer(\"select\", \"dplyr\")) to specify that the select() function should always come from the dplyr package.\n\n# install.packages(\"conflicted\")\nlibrary(conflicted)\n# set conflict preference\nconflict_prefer(\"select\", \"dplyr\")\n\n[conflicted] Will prefer dplyr::select over any other package.\n\n\nNow when I use select() it works just fine!\n\n# no more error!\ndiamonds %&gt;% select(cut)\n\n# A tibble: 53,940 × 1\n   cut      \n   &lt;ord&gt;    \n 1 Ideal    \n 2 Premium  \n 3 Good     \n 4 Premium  \n 5 Good     \n 6 Very Good\n 7 Very Good\n 8 Very Good\n 9 Fair     \n10 Very Good\n# … with 53,930 more rows\n\n\n\n\nTip 4: geom_col(): you’ll never have to specify “stat = identity” for your bar plots ever again!\nMost of the time when I want to make a bar chart, I want to provide an x variable for the bar categories, and a y variable for the height of the bar for each category. Sadly, this isn’t what geom_bar() does by default - its default behaviour is to count each level of the provided x aesthetic. For instance, in the diamonds dataset, the cut variable takes 5 unique values: fair, good, very good, premium and ideal. Providing cut as the x-aesthetic for geom_bar() will, by default, count the number of times each of these unique values appear in the data and use these counts as the heights.\nThe default usage of geom_bar() is as follows\n\ndiamonds %&gt;%\n  ggplot() +\n  geom_bar(aes(x = cut))\n\n\n\n\n\n\n\n\nwhich automatically counts the number of time each cut appears in the data.\nHowever (putting aside arguments about what types of data bar charts are appropriate for), I frequently want to use a bar chart to display something other than a count. For instance, the average price of each cut, shown below.\n\naverage_price &lt;- diamonds %&gt;% \n  group_by(cut) %&gt;%\n  summarise(average_price = mean(price)) %&gt;%\n  ungroup() \naverage_price\n\n# A tibble: 5 × 2\n  cut       average_price\n  &lt;ord&gt;             &lt;dbl&gt;\n1 Fair              4359.\n2 Good              3929.\n3 Very Good         3982.\n4 Premium           4584.\n5 Ideal             3458.\n\n\nIf I try to set the y aesthetic of geom_bar to y = average_price, I get an annoying error:\n\naverage_price %&gt;% \n  ggplot() +\n  geom_bar(aes(x = cut, y = average_price))\n\nError in `geom_bar()`:\n! Problem while computing stat.\nℹ Error occurred in the 1st layer.\nCaused by error in `setup_params()`:\n! `stat_count()` must only have an x or y aesthetic.\n\n\nThe typical fix for this is to include stat = 'identity' as an argument of geom_bar().\n\naverage_price %&gt;% \n  ggplot() +\n  geom_bar(aes(x = cut, y = average_price), stat = \"identity\")\n\n\n\n\n\n\n\n\nBut it turns out there’s a better way!\nDuring his fantastic talk on his ggtext package, Claus Wilke) casually used the geom_col() function to create a column chart (is that what geom_col() stands for?), which, as it turns out, is what I’ve been trying to make the whole time:\n\naverage_price %&gt;% \n  ggplot() +\n  geom_col(aes(x = cut, y = average_price))\n\n\n\n\n\n\n\n\nThis probably wasn’t what Claus imagined me to take away from his talk, but I swear ggtext was super cool too!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip 5: Using show_col() for viewing colour palettes\nLast but not least, Dana Paige Seidel gave a great talk on the scales package, which does a lot of great things under the hood of ggplot. But scales also let’s you look at colours! The show_col() function shows you what colours look like:\n\nlibrary(scales)\nshow_col(c(\"#9C89B8\", \"#F0A6CA\", \"#EFC3E6\", \"#F0E6EF\"))\n\n\n\n\n\n\n\n\nAlso a quick plug for how I chose these colours (and how I choose colours in general): https://coolors.co/app.\nI’m super excited for rstudio::conf(2021)!!"
  },
  {
    "objectID": "blog/2017-08-16-data-science-workflow.html",
    "href": "blog/2017-08-16-data-science-workflow.html",
    "title": "A Basic Data Science Workflow",
    "section": "",
    "text": "Developing a seamless, clean workflow for data analysis is harder than it sounds, especially because this is something that is almost never explicitly taught. Apparently we are all just supposed to “figure it out for ourselves”. For most of us, when we start our first few analysis projects, we basically have no idea how we are going to structure all of our files, or even what files we will need to make. As we try more and more things in our analysis (perhaps generating a large number of unnecessary files called analysis2.R, next-analysis.R, analysis-writeup.Rmd, data_clean.csv, regression_results.csv and data_all.csv along the way), we find that our project folder gets jumbled and confusing. The frustration when we come back to the project 6 months later and can’t remember which file contained the code that lead to the final conclusions is all too real.\nThis is why I have decided to describe in (possibly too much) detail the data cleaning workflow that I have somehow ended up with. I find this workflow to be particularly useful when dealing with messy (and possibly large) datasets that need several cleaning steps. Note that I did not develop this workflow in conjunction with any resources, I simply figured out what worked best for me via trial-and-error (a process which took 5 years and is definitely still ongoing). There will be several other resources out there on the internet describing “optimal workflows”, and these are definitely worth a read too (although a quick google found surprisingly few with the level of detail needed for a beginner). The key is figuring out a workflow that works best for you. That may be similar to mine, or it may not be.\nIf you decide to keep reading (perhaps because you too suffer from messy-project-syndrome and want some relief), by the end of this post you will know far too much about me and how I spend my time. As you will discover, I am particularly thorough when I clean data, and can spend hours simply making sure that I know what is in the data and moulding it so that it exactly adheres precisely to the format that I consider “clean”.\nIn this post I will describe my thought process as I download, clean and prepare for analysis the data from the 2016 American Time Use Survey (ATUS). I have written my process in sufficient detail such that you can follow along if you’d like to.\nThe American Time Use Survey is a yearly survey administered by the U.S. Census Bureau and sponsored by the Bureau of Labor Statistics. As with all surveys, it is probably good practice to first get an idea of what kind of population its respondents are supposed to represent. According to their website, the survey is sent to a randomly selected individual from each household in a set of eligible households chosen so as to represent a range of demographic characteristics. The set of eligible households consist of those who have completed their final month of the Current Population Survey (a monthly survey of households conducted by the Bureau of Census for the Bureau of Labor Statistics).\nThis survey data has been featured heavily on Nathan Yau’s blog, Flowing Data, which is where I became aware of it (thanks Nathan!)."
  },
  {
    "objectID": "blog/2017-08-16-data-science-workflow.html#the-loaddata-function",
    "href": "blog/2017-08-16-data-science-workflow.html#the-loaddata-function",
    "title": "A Basic Data Science Workflow",
    "section": "The loadData() function",
    "text": "The loadData() function\nTo make things simple in the long-run, I turn the above commands into a reusable function called loadData(). This function will have only one argument that specifies the path of the data in the local directory (relative to the load.R file). I usually set the default path to be the actual path for my setup.\n\n# a function to load in the data\nloadData &lt;- function(path_to_data = \"data/atus_00002.csv.gz\") {\n  # open zipped file for reading\n  unz &lt;- gzfile(path_to_data)\n  # load in the data\n  read.csv(unz)\n}\n\nTo test my function, I simply run in my console by typing\n\ntime_use_orig &lt;- loadData()\n\nand look at the output of head(time_use_orig).\nObviously such a function is a bit redundant in this setting: it is just as easy to write read.csv(gzfile(\"data/atus_00002.csv.gz\")) in my eventual eda.Rmd file as it is to write loadData(\"data/atus_00002.csv.gz\"). The reason I keep the load.R file in this case is because this is just my default workflow. I always load in my data using a function called loadData. In some situations, there are many, many things that need to be done in order to load the data, meaning that my loadData function can be fairly complicated. For example, sometimes column names need to be read in separately and then attached to the data, and sometimes I need to play with the format of the data to get R to play nice."
  },
  {
    "objectID": "blog/2017-08-16-data-science-workflow.html#the-cleandata-function",
    "href": "blog/2017-08-16-data-science-workflow.html#the-cleandata-function",
    "title": "A Basic Data Science Workflow",
    "section": "The cleanData() function",
    "text": "The cleanData() function\nThe cleanData() function will actually call three separate functions, each performing a single task. These functions are\n\nrenameColumns(): an optional part of my workflow that changes the column names of each of my columns so that I can actually understand what they mean.\nconvertMissing(): a function which converts missing values to NA\nconvertClass: a function which sets factor variables to factors, sets character variables to characters, etc\n\n\nMaking columns human-readable: renameColumns()\nI hate reading column names that are all-caps, use ridiculous abbreviations and generally don’t adhere to my definition of “aesthetically pleasing”. Thus, whenever possible, I tend to convert my column names to human-readable versions. This is fairly tedious whenever the data has more than around 10 variables or so, but the process itself of renaming the variables is a very effective way of ensuring that you have a good idea of which variables are even in the data.\nA word of caution: it is extremely important to check that you have correctly renamed the variables, since it is very easy to assign the wrong name to a variable, resulting in misleading conclusions.\nObviously this step is not practical if you have more than 100 or so variables (although I once did it with a dataset that had around 300 variables!). In addition, if I will at some point need to present the data to people who are very familiar with the original variable names, I won’t do any renaming either.\nIn this case, however, I have no particular allegiance to the original variable names and I want to make it as clear as possible (to myself, at least) what they mean.\nTo change the variable names, the renameColumns() function will leverage the dplyr function, select(). Note that I also drop a few variables at this stage that I decided weren’t interesting.\n\nlibrary(dplyr)\nrenameColumns &lt;- function(data) {\n  data &lt;- data %&gt;% select(id = CASEID,\n                          year = YEAR,\n                          # number of attempted contacts\n                          num_contacts = NUMCONTACTS_CPS8,\n                          state = STATEFIP,\n                          household_size = HH_SIZE,\n                          family_income = FAMINCOME,\n                          num_children = HH_NUMKIDS,\n                          num_adults = HH_NUMADULTS,\n                          age = AGE,\n                          sex = SEX,\n                          race = RACE,\n                          marital_status = MARST,\n                          education_level = EDUC,\n                          education_years = EDUCYRS,\n                          employment_status = EMPSTAT,\n                          occupation_category = OCC2,\n                          occupation_industry = IND2,\n                          employed_full_time = FULLPART,\n                          hours_usually_worked = UHRSWORKT,\n                          weekly_earning = EARNWEEK,\n                          paid_hourly = PAIDHOUR,\n                          hourly_wage = HOURWAGE,\n                          hours_worked_hourly_rate = HRSATRATE,\n                          time_spent_caring_household = ACT_CAREHH,\n                          time_spent_caring_non_household = ACT_CARENHH,\n                          time_spent_education = ACT_EDUC,\n                          time_spent_eating = ACT_FOOD,\n                          time_spent_gov = ACT_GOVSERV,\n                          time_spent_household_activities = ACT_HHACT,\n                          time_spent_household_services = ACT_HHSERV,\n                          time_spent_personal_care = ACT_PCARE,\n                          time_spent_phone = ACT_PHONE,\n                          time_spent_personal_care_services = ACT_PROFSERV,\n                          time_spent_shopping = ACT_PURCH,\n                          time_spent_religion = ACT_RELIG,\n                          time_spent_leisure = ACT_SOCIAL,\n                          time_spent_sports = ACT_SPORTS,\n                          time_spent_travelling = ACT_TRAVEL,\n                          time_spent_volunteer = ACT_VOL,\n                          time_spent_working = ACT_WORK)\n  return(data)\n}\n\nI then test the function out by writing\n\ntime_use &lt;- renameColumns(time_use_orig)\n\nin the console, and looking at the output of head(time_use).\nNow, I am fully aware that this function I have just written is not generalizable to alternate subsets of the data variables. This will be one of only two places where I will need to change things if I want to re-run the analysis on a different subset of variables (the second place will be when I explicitly convert numeric variables to their character counterparts). I’m facing a trade-off between generalizability of my pipeline and having human-readable data. If I were intending to repeat this analysis on different variables, I would either remove the part of the workflow where I rename the variables (as well as the part where I convert numeric variables to meaningful factors later on), or I would set the variable names as an argument in the renameColumns() function (but sadly, select() doesn’t play very nicely with variables read in as character strings, so I try to avoid this).\n\n\nRecoding missing values as NA: convertMissing()\nIf you took a look at the codebook, you will have noticed that there are many different ways to say that data is missing. This can be very problematic.\nThe most common way to code missingness in this data is to code it as 99 999, 9999, etc (depending on whether the entries for the variable are two, three, four or more digit numbers, respectively). These entries are referred to in the codebook as NIU (Not in universe). Other types of missing values are recorded such as 996 corresponding to Refused, 997 corresponding to Don't know and 998 corresponding to Blank.\nI now need to decide what to convert to NA, keeping in mind that I need to be particularly careful for the variables with many different types of missingness (such as people who refused to answer, didn’t know or simply left the entry blank). I decide to take a look to see how widespread these types of missingness are by running the following code in the console:\n\n# identify how many times informative missingness occurs for each variable\ninformative_missing &lt;- sapply(time_use, \n                              function(x) sum(x %in% c(996, 997, 998)))\n# print out only the non-zero values\ninformative_missing[informative_missing != 0]\n\n          weekly_earning hours_worked_hourly_rate time_spent_personal_care \n                       1                       12                        1 \n      time_spent_leisure       time_spent_working \n                       1                        1 \n\n\nSince these types of missingness are extremely rare, I decide to simply lump them in with all of the other NA values.\nNext, I want to identify which variables have missing values coded as a varying number of 9s. Since the missing values are always supposed to correspond to the maximum, I printed out the maximum of each variable.\n\n# print out the maximum of each column\nsapply(time_use, max)\n\n                               id                              year \n                     2.016121e+13                      2.016000e+03 \n                     num_contacts                             state \n                     8.000000e+00                      5.600000e+01 \n                   household_size                     family_income \n                     1.300000e+01                      1.600000e+01 \n                     num_children                        num_adults \n                     9.000000e+00                      8.000000e+00 \n                              age                               sex \n                     8.500000e+01                      2.000000e+00 \n                             race                    marital_status \n                     4.000000e+02                      6.000000e+00 \n                  education_level                   education_years \n                     4.300000e+01                      3.210000e+02 \n                employment_status               occupation_category \n                     5.000000e+00                      9.999000e+03 \n              occupation_industry                employed_full_time \n                     9.999000e+03                      9.900000e+01 \n             hours_usually_worked                    weekly_earning \n                     9.999000e+03                      9.999999e+04 \n                      paid_hourly                       hourly_wage \n                     9.900000e+01                      9.999900e+02 \n         hours_worked_hourly_rate       time_spent_caring_household \n                     9.990000e+02                      8.800000e+02 \n  time_spent_caring_non_household              time_spent_education \n                     9.100000e+02                      1.000000e+03 \n                time_spent_eating                    time_spent_gov \n                     6.350000e+02                      4.700000e+02 \n  time_spent_household_activities     time_spent_household_services \n                     1.184000e+03                      4.500000e+02 \n         time_spent_personal_care                  time_spent_phone \n                     1.434000e+03                      4.200000e+02 \ntime_spent_personal_care_services               time_spent_shopping \n                     5.250000e+02                      6.000000e+02 \n              time_spent_religion                time_spent_leisure \n                     7.130000e+02                      1.392000e+03 \n                time_spent_sports             time_spent_travelling \n                     1.260000e+03                      1.265000e+03 \n             time_spent_volunteer                time_spent_working \n                     9.200000e+02                      1.350000e+03 \n\n\nI notice here that there are several variables with missing values as their maxima such as occupation_industry with a maximum of 9999, and hourly_wage with a maximum of 999.99. Since I don’t really want to manually convert these missing values to NA, I decide to automate it using the mutate_if() function from the dplyr package. First I write a few helper functions in the clean.R file for calculating the maximum of a vector and for identifying specific values in a vector.\n\n# Helper function for identifying missing values equal to 99, 999, etc\nequalFun &lt;- function(x, value) {\n  x == value\n}\n\n# Helper function for identifying if the max of a variable is equal to 99, ...\nmaxFun &lt;- function(x, value) {\n  max(x, na.rm = T) == value\n}\n\nThe first argument of mutate_if() is a function which returns a Boolean value specifying which columns to select. The second argument is wrapped in funs() and itself is a function which specifies what to do to each column. if_else(equalFun(., 99), NA_integer_, .) can be read aloud as “If the value is equal to 99, convert it to a NA of integer type, otherwise do nothing” (the . serves as a placeholder for the data, like x in function(x)).\n\nconvertMissing &lt;- function(data) {\n  # convert missing values to NA\n  data &lt;- data %&gt;%\n    # mutate all missing values coded as 99 to NA\n    mutate_if(function(x) maxFun(x, 99), \n              funs(if_else(equalFun(., 99), NA_integer_, .))) %&gt;%\n    # mutate all missing values coded as 999 to NA\n    mutate_if(function(x) maxFun(x, 999), \n              funs(if_else(equalFun(., 999), NA_integer_, .))) %&gt;%\n    # mutate all missing values coded as 9999 to NA\n    mutate_if(function(x) maxFun(x, 9999), \n              funs(if_else(equalFun(., 9999), NA_integer_, .))) %&gt;%\n    # mutate all missing values coded as 999.99 to NA\n    mutate_if(function(x) maxFun(x, 999.99), \n              funs(if_else(equalFun(., 999.99), NA_real_, .))) %&gt;%\n    # mutate all missing values coded as 99999.99 to NA\n    mutate_if(function(x) maxFun(x, 99999.99), \n              funs(if_else(equalFun(., 99999.99), NA_real_, .))) %&gt;%\n    # mutate all missing values coded as 998 to NA\n    mutate_if(function(x) maxFun(x, 998), \n              funs(if_else(equalFun(., 998), NA_integer_, .))) %&gt;%\n    # mutate all missing values coded as 997 to NA\n    mutate_if(function(x) maxFun(x, 997), \n              funs(if_else(equalFun(., 997), NA_integer_, .)))\n  return(data)    \n}\n\nIt took some playing around with running the body of the function in the console (with data defined as time_use) to get it to run without errors (I was getting errors to do with NA values and realized that I needed to add na.rm = T in the maxFun() function).\nOnce the body runs in the console, I then check to make sure that the complete function worked as expected by running it in the console and checking out the summary of the output.\n\n# convert the missing values to NAs\ntime_use &lt;- convertMissing(time_use)\n\nWarning: `funs()` was deprecated in dplyr 0.8.0.\nℹ Please use a list of either functions or lambdas:\n\n# Simple named list: list(mean = mean, median = median)\n\n# Auto named with `tibble::lst()`: tibble::lst(mean, median)\n\n# Using lambdas list(~ mean(., trim = .2), ~ median(., na.rm = TRUE))\n\n\nWarning: `funs()` was deprecated in dplyr 0.8.0.\nℹ Please use a list of either functions or lambdas:\n\n# Simple named list: list(mean = mean, median = median)\n\n# Auto named with `tibble::lst()`: tibble::lst(mean, median)\n\n# Using lambdas list(~ mean(., trim = .2), ~ median(., na.rm = TRUE))\n\n\nWarning: `funs()` was deprecated in dplyr 0.8.0.\nℹ Please use a list of either functions or lambdas:\n\n# Simple named list: list(mean = mean, median = median)\n\n# Auto named with `tibble::lst()`: tibble::lst(mean, median)\n\n# Using lambdas list(~ mean(., trim = .2), ~ median(., na.rm = TRUE))\n\n\nWarning: `funs()` was deprecated in dplyr 0.8.0.\nℹ Please use a list of either functions or lambdas:\n\n# Simple named list: list(mean = mean, median = median)\n\n# Auto named with `tibble::lst()`: tibble::lst(mean, median)\n\n# Using lambdas list(~ mean(., trim = .2), ~ median(., na.rm = TRUE))\n\n\nWarning: `funs()` was deprecated in dplyr 0.8.0.\nℹ Please use a list of either functions or lambdas:\n\n# Simple named list: list(mean = mean, median = median)\n\n# Auto named with `tibble::lst()`: tibble::lst(mean, median)\n\n# Using lambdas list(~ mean(., trim = .2), ~ median(., na.rm = TRUE))\n\n\nWarning: `funs()` was deprecated in dplyr 0.8.0.\nℹ Please use a list of either functions or lambdas:\n\n# Simple named list: list(mean = mean, median = median)\n\n# Auto named with `tibble::lst()`: tibble::lst(mean, median)\n\n# Using lambdas list(~ mean(., trim = .2), ~ median(., na.rm = TRUE))\n\n\nWarning: `funs()` was deprecated in dplyr 0.8.0.\nℹ Please use a list of either functions or lambdas:\n\n# Simple named list: list(mean = mean, median = median)\n\n# Auto named with `tibble::lst()`: tibble::lst(mean, median)\n\n# Using lambdas list(~ mean(., trim = .2), ~ median(., na.rm = TRUE))\n\n# check out the summary\nsummary(time_use)\n\n       id                 year       num_contacts        state      \n Min.   :2.016e+13   Min.   :2016   Min.   :0.0000   Min.   : 1.00  \n 1st Qu.:2.016e+13   1st Qu.:2016   1st Qu.:0.0000   1st Qu.:13.00  \n Median :2.016e+13   Median :2016   Median :0.0000   Median :27.00  \n Mean   :2.016e+13   Mean   :2016   Mean   :0.2574   Mean   :27.96  \n 3rd Qu.:2.016e+13   3rd Qu.:2016   3rd Qu.:0.0000   3rd Qu.:42.00  \n Max.   :2.016e+13   Max.   :2016   Max.   :8.0000   Max.   :56.00  \n                                                                    \n household_size   family_income    num_children      num_adults   \n Min.   : 1.000   Min.   : 1.00   Min.   :0.0000   Min.   :1.000  \n 1st Qu.: 1.000   1st Qu.: 8.00   1st Qu.:0.0000   1st Qu.:1.000  \n Median : 2.000   Median :12.00   Median :0.0000   Median :2.000  \n Mean   : 2.657   Mean   :10.85   Mean   :0.7706   Mean   :1.868  \n 3rd Qu.: 4.000   3rd Qu.:14.00   3rd Qu.:1.0000   3rd Qu.:2.000  \n Max.   :13.000   Max.   :16.00   Max.   :9.0000   Max.   :8.000  \n                                                                  \n      age             sex             race       marital_status \n Min.   :15.00   Min.   :1.000   Min.   :100.0   Min.   :1.000  \n 1st Qu.:35.00   1st Qu.:1.000   1st Qu.:100.0   1st Qu.:1.000  \n Median :49.00   Median :2.000   Median :100.0   Median :3.000  \n Mean   :49.42   Mean   :1.555   Mean   :104.5   Mean   :3.022  \n 3rd Qu.:64.00   3rd Qu.:2.000   3rd Qu.:100.0   3rd Qu.:6.000  \n Max.   :85.00   Max.   :2.000   Max.   :400.0   Max.   :6.000  \n                                                                \n education_level education_years employment_status occupation_category\n Min.   :10.00   Min.   :101.0   Min.   :1.000     Min.   :110.0      \n 1st Qu.:21.00   1st Qu.:112.0   1st Qu.:1.000     1st Qu.:122.0      \n Median :30.00   Median :214.0   Median :1.000     Median :131.0      \n Mean   :29.74   Mean   :187.8   Mean   :2.559     Mean   :138.1      \n 3rd Qu.:40.00   3rd Qu.:217.0   3rd Qu.:5.000     3rd Qu.:150.0      \n Max.   :43.00   Max.   :321.0   Max.   :5.000     Max.   :200.0      \n                                                   NA's   :4119       \n occupation_industry employed_full_time hours_usually_worked weekly_earning  \n Min.   :100.0       Min.   :1.000      Min.   :   0.0       Min.   :   0.0  \n 1st Qu.:160.0       1st Qu.:1.000      1st Qu.:  38.0       1st Qu.: 458.8  \n Median :220.0       Median :1.000      Median :  40.0       Median : 800.0  \n Mean   :210.4       Mean   :1.219      Mean   : 672.6       Mean   : 994.0  \n 3rd Qu.:251.0       3rd Qu.:1.000      3rd Qu.:  50.0       3rd Qu.:1346.2  \n Max.   :300.0       Max.   :2.000      Max.   :9995.0       Max.   :2884.6  \n NA's   :4119        NA's   :4119       NA's   :4119         NA's   :4833    \n  paid_hourly     hourly_wage    hours_worked_hourly_rate\n Min.   :1.000   Min.   : 0.00   Min.   : 1.00           \n 1st Qu.:1.000   1st Qu.:10.50   1st Qu.:28.00           \n Median :1.000   Median :15.00   Median :40.00           \n Mean   :1.444   Mean   :18.34   Mean   :33.86           \n 3rd Qu.:2.000   3rd Qu.:22.00   3rd Qu.:40.00           \n Max.   :2.000   Max.   :99.99   Max.   :90.00           \n NA's   :4833    NA's   :7348    NA's   :8438            \n time_spent_caring_household time_spent_caring_non_household\n Min.   :  0.00              Min.   :  0.000                \n 1st Qu.:  0.00              1st Qu.:  0.000                \n Median :  0.00              Median :  0.000                \n Mean   : 31.85              Mean   :  8.603                \n 3rd Qu.: 11.00              3rd Qu.:  0.000                \n Max.   :880.00              Max.   :910.000                \n                                                            \n time_spent_education time_spent_eating time_spent_gov    \n Min.   :   0.00      Min.   :  0.00    Min.   :  0.0000  \n 1st Qu.:   0.00      1st Qu.: 30.00    1st Qu.:  0.0000  \n Median :   0.00      Median : 60.00    Median :  0.0000  \n Mean   :  15.39      Mean   : 64.99    Mean   :  0.3647  \n 3rd Qu.:   0.00      3rd Qu.: 90.00    3rd Qu.:  0.0000  \n Max.   :1000.00      Max.   :635.00    Max.   :470.0000  \n                                                          \n time_spent_household_activities time_spent_household_services\n Min.   :   0.0                  Min.   :  0.0000             \n 1st Qu.:  15.0                  1st Qu.:  0.0000             \n Median :  75.0                  Median :  0.0000             \n Mean   : 121.1                  Mean   :  0.9868             \n 3rd Qu.: 180.0                  3rd Qu.:  0.0000             \n Max.   :1184.0                  Max.   :450.0000             \n                                                              \n time_spent_personal_care time_spent_phone  time_spent_personal_care_services\n Min.   :   0.0           Min.   :  0.000   Min.   :  0.000                  \n 1st Qu.: 495.0           1st Qu.:  0.000   1st Qu.:  0.000                  \n Median : 570.0           Median :  0.000   Median :  0.000                  \n Mean   : 580.3           Mean   :  6.774   Mean   :  4.747                  \n 3rd Qu.: 660.0           3rd Qu.:  0.000   3rd Qu.:  0.000                  \n Max.   :1434.0           Max.   :420.000   Max.   :525.000                  \n                                                                             \n time_spent_shopping time_spent_religion time_spent_leisure time_spent_sports\n Min.   :  0.00      Min.   :  0.0       Min.   :   0.0     Min.   :   0.00  \n 1st Qu.:  0.00      1st Qu.:  0.0       1st Qu.: 130.0     1st Qu.:   0.00  \n Median :  0.00      Median :  0.0       Median : 260.0     Median :   0.00  \n Mean   : 24.15      Mean   : 13.4       Mean   : 298.4     Mean   :  20.22  \n 3rd Qu.: 30.00      3rd Qu.:  0.0       3rd Qu.: 432.0     3rd Qu.:   0.00  \n Max.   :600.00      Max.   :713.0       Max.   :1392.0     Max.   :1260.00  \n                                                                             \n time_spent_travelling time_spent_volunteer time_spent_working\n Min.   :   0.0        Min.   :  0.000      Min.   :   0.0    \n 1st Qu.:  20.0        1st Qu.:  0.000      1st Qu.:   0.0    \n Median :  55.0        Median :  0.000      Median :   0.0    \n Mean   :  71.5        Mean   :  8.469      Mean   : 157.7    \n 3rd Qu.:  95.0        3rd Qu.:  0.000      3rd Qu.: 375.0    \n Max.   :1265.0        Max.   :920.000      Max.   :1350.0    \n                                                              \n\n\nScrolling through the summary, I notice a few peculiarities. In particular, there are several variables that have stupidly large values. For example the maximum value for hours_usually_worked is 9995 (this didn’t appear in the codebook!). I decided to look at a histogram of this variable to see how typical this value is. I ran the following code in the console:\n\nlibrary(ggplot2)\nggplot(time_use) + geom_histogram(aes(x = hours_usually_worked))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 4119 rows containing non-finite values (`stat_bin()`).\n\n\n\n\n\n\n\n\n\nFrom the histogram, it is fairly clear that there is an additional type of missing value (405 samples have a value of 9995) that was not mentioned in the documentation. I then go back and update my convertMissing() function to include this extra missing value.\n\nconvertMissing &lt;- function(data) {\n  # convert missing values to NA\n  data &lt;- data %&gt;%\n    # mutate all missing values coded as 99 to NA\n    mutate_if(function(x) maxFun(x, 99), \n              funs(if_else(equalFun(., 99), NA_integer_, .))) %&gt;%\n    # mutate all missing values coded as 999 to NA\n    mutate_if(function(x) maxFun(x, 999), \n              funs(if_else(equalFun(., 999), NA_integer_, .))) %&gt;%\n    # mutate all missing values coded as 9999 to NA\n    mutate_if(function(x) maxFun(x, 9999), \n              funs(if_else(equalFun(., 9999), NA_integer_, .))) %&gt;%\n    # mutate all missing values coded as 999.99 to NA\n    mutate_if(function(x) maxFun(x, 999.99), \n              funs(if_else(equalFun(., 999.99), NA_real_, .))) %&gt;%\n    # mutate all missing values coded as 99999.99 to NA\n    mutate_if(function(x) maxFun(x, 99999.99), \n              funs(if_else(equalFun(., 99999.99), NA_real_, .))) %&gt;%\n    # mutate all missing values coded as 998 to NA\n    mutate_if(function(x) maxFun(x, 998), \n              funs(if_else(equalFun(., 998), NA_integer_, .))) %&gt;%\n    # mutate all missing values coded as 997 to NA\n    mutate_if(function(x) maxFun(x, 997), \n              funs(if_else(equalFun(., 997), NA_integer_, .))) %&gt;%\n    # mutate all missing values coded as 9995 to NA\n    mutate_if(function(x) maxFun(x, 9995), \n              funs(if_else(equalFun(., 9995), NA_integer_, .))) \n  return(data)    \n}\n\nNext, I ran the convertMissing() function again the data and re-made the histogram to make sure that everything was going smoothly.\n\n# re-run the renameColumns() function\ntime_use &lt;- renameColumns(time_use_orig)\n# convert missing values to NA\ntime_use &lt;- convertMissing(time_use)\n\nWarning: `funs()` was deprecated in dplyr 0.8.0.\nℹ Please use a list of either functions or lambdas:\n\n# Simple named list: list(mean = mean, median = median)\n\n# Auto named with `tibble::lst()`: tibble::lst(mean, median)\n\n# Using lambdas list(~ mean(., trim = .2), ~ median(., na.rm = TRUE))\n\n\nWarning: `funs()` was deprecated in dplyr 0.8.0.\nℹ Please use a list of either functions or lambdas:\n\n# Simple named list: list(mean = mean, median = median)\n\n# Auto named with `tibble::lst()`: tibble::lst(mean, median)\n\n# Using lambdas list(~ mean(., trim = .2), ~ median(., na.rm = TRUE))\n\n\nWarning: `funs()` was deprecated in dplyr 0.8.0.\nℹ Please use a list of either functions or lambdas:\n\n# Simple named list: list(mean = mean, median = median)\n\n# Auto named with `tibble::lst()`: tibble::lst(mean, median)\n\n# Using lambdas list(~ mean(., trim = .2), ~ median(., na.rm = TRUE))\n\n\nWarning: `funs()` was deprecated in dplyr 0.8.0.\nℹ Please use a list of either functions or lambdas:\n\n# Simple named list: list(mean = mean, median = median)\n\n# Auto named with `tibble::lst()`: tibble::lst(mean, median)\n\n# Using lambdas list(~ mean(., trim = .2), ~ median(., na.rm = TRUE))\n\n\nWarning: `funs()` was deprecated in dplyr 0.8.0.\nℹ Please use a list of either functions or lambdas:\n\n# Simple named list: list(mean = mean, median = median)\n\n# Auto named with `tibble::lst()`: tibble::lst(mean, median)\n\n# Using lambdas list(~ mean(., trim = .2), ~ median(., na.rm = TRUE))\n\n\nWarning: `funs()` was deprecated in dplyr 0.8.0.\nℹ Please use a list of either functions or lambdas:\n\n# Simple named list: list(mean = mean, median = median)\n\n# Auto named with `tibble::lst()`: tibble::lst(mean, median)\n\n# Using lambdas list(~ mean(., trim = .2), ~ median(., na.rm = TRUE))\n\n\nWarning: `funs()` was deprecated in dplyr 0.8.0.\nℹ Please use a list of either functions or lambdas:\n\n# Simple named list: list(mean = mean, median = median)\n\n# Auto named with `tibble::lst()`: tibble::lst(mean, median)\n\n# Using lambdas list(~ mean(., trim = .2), ~ median(., na.rm = TRUE))\n\n\nWarning: `funs()` was deprecated in dplyr 0.8.0.\nℹ Please use a list of either functions or lambdas:\n\n# Simple named list: list(mean = mean, median = median)\n\n# Auto named with `tibble::lst()`: tibble::lst(mean, median)\n\n# Using lambdas list(~ mean(., trim = .2), ~ median(., na.rm = TRUE))\n\n# re-make the histogram\nggplot(time_use) + geom_histogram(aes(x = hours_usually_worked))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 4524 rows containing non-finite values (`stat_bin()`).\n\n\n\n\n\n\n\n\n\nNow that that was sorted out, it occurred to me that I wasn’t sure what kind of scale the time_spent variables were on (is it hours spent in the last week? In the last month? The last year? Perhaps it is minutes spent over the last day? It probably should have occurred to me to ask this earlier, but it didn’t. Whatever… I’m asking it now! After spending some time perusing the internet for a while, I found this table which summarised the average hours spend per day on a range of activities. For example, it said that on average, people spend 9.58 hours per day on “personal care activities”. The histogram below shows the distribution of values for the time_spent_personal_care.\n\nggplot(time_use) + geom_histogram(aes(x = time_spent_personal_care))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nThe mean value in the data is 580, which when divided by 60 gives 9.6. From this “evidence” I conclude that what the data contains is the number of minutes spent per day. Whether this is averaged over a week, or is based on one particular day, I honestly don’t know. But for my purposes, I’ll just take each value as the number of minutes spent on the activity on a “typical” day.\n\n\nEnsuring each variable has the correct class: convertClass()\nThe final cleaning task involves converting categorical values to have a categorical variable class (such as a factor), and other things along these lines involving variable classes.\nRecall that the person ID variable, CASEID, is currently coded as a numeric (which is printed in scientific notation). In general, it is good practice to code IDs as factors (or characters).\nThere are also many other variables that should be coded as factors: state, sex, race, marital_status, education_level, family_income, employment_status, occupation_category, and occupation_industry.\nNow begins the part of my cleaning process that often takes the longest: I am going to convert each of these numeric variables not only to factors, but to meaningful factors. I don’t want to make plots for genders 1 and 2, or for states 42 and 28; I want to make plots for males and females and for states Pennsylvania and Mississippi.\nFirst, for each variable I need to define a data frame that stores the conversion from number to meaningful category. Fortunately, this information was found in the codebook, and I can copy and paste these code conversions into separate .txt files and save them in the data/ folder: states.txt, occupation_industry.txt, occupation_category.txt, etc. I can then read them into R as tab-delimited text files.\nIn case you’re interested, after copying the subsets of the codebook, my project directory now looks like this:\n\ndata/\n  atus_00002.csv.gz\n  atus_00002.txt\n  education_level.txt\n  employment_status.txt\n  family_income.txt\n  marital_status.txt\n  occupation_category.txt\n  occupation_industry.txt\n  race.txt\n  sex.txt\n  state.txt\nR/\n  clean.R\n  load.R\n\nI now start work on a convertClass() function which will be the third component of my cleanData() function. The first thing I do in convertClass() is convert the id variable to a factor. I then loop through each of the other factor variables to read in the code conversions from the .txt files, join the meaningful factors onto the original data frame using left_join() and remove the numeric version of the variable. The function that I wrote is presented below. I spent a while playing around in the console with various versions of the function below (always running code from the .R file rather than typing directly in the console itself).\n\nconvertClass &lt;- function(data, path_to_codes = \"data/\") {\n  # convert id to a factor\n  data &lt;- data %&gt;% mutate(id = as.factor(id))\n  # loop through each of the factor variables and convert to meaningful\n  # factor then add to data frame\n  for (variable in c(\"state\", \"occupation_industry\", \"occupation_category\",\n                       \"education_level\", \"race\", \"marital_status\", \"sex\",\n                       \"employment_status\", \"family_income\")) {\n    # identify the path to the code file\n    path &lt;- paste0(path_to_codes, variable, \".txt\")\n    # read in the code file\n    codes &lt;- read.table(path, sep = \"\\t\")\n    # remove the second column (the entries are separated by two \\t's)\n    codes &lt;- codes[, -2]\n    # convert the column names\n    colnames(codes) &lt;- c(variable, paste0(variable, \"_name\"))\n    # add the code to the original data frame\n    data &lt;- left_join(data, codes, by = variable)\n    # remove old variable and replace with new variable\n    data[, variable] &lt;- data[, paste0(variable, \"_name\")]\n    data &lt;- data[, !(colnames(data) %in% paste0(variable, \"_name\"))]\n  }\n  return(data)\n}\n\nAfter I was done, I tested out that the convertClass() did what I hoped by running the following code in the console:\n\n# run the convertClass() function\ntime_use &lt;- convertClass(time_use)\n# compare the original variables with the meaningful versions\nhead(time_use)\n\n              id year num_contacts                state household_size\n1 20160101160045 2016            1              Georgia              3\n2 20160101160066 2016            1             Virginia              2\n3 20160101160069 2016            2 District of Columbia              4\n4 20160101160083 2016            1             Michigan              4\n5 20160101160084 2016            1             Missouri              2\n6 20160101160094 2016            0              Florida              5\n       family_income num_children num_adults age    sex\n1   $7,500 to $9,999            0          3  62 Female\n2 $15,000 to $19,999            0          2  69   Male\n3 $10,000 to $12,499            2          2  24 Female\n4 $25,000 to $29,999            3          1  31 Female\n5 $60,000 to $74,999            0          2  59 Female\n6 $12,500 to $14,999            4          1  16 Female\n                             race           marital_status\n1                      White only Married - spouse present\n2                      Black only Married - spouse present\n3                      Black only            Never married\n4                      White only                 Divorced\n5                      White only Married - spouse present\n6 American Indian, Alaskan Native            Never married\n                 education_level education_years employment_status\n1 High school graduate (diploma)             112              &lt;NA&gt;\n2                     11th grade             111              &lt;NA&gt;\n3     High school graduate (GED)             102              &lt;NA&gt;\n4     Some college but no degree             214         Full time\n5 High school graduate (diploma)             112         Full time\n6                     10th grade             110              &lt;NA&gt;\n                           occupation_category\n1                                         &lt;NA&gt;\n2                                         &lt;NA&gt;\n3                                         &lt;NA&gt;\n4               Healthcare support occupations\n5 Education, training, and library occupations\n6                                         &lt;NA&gt;\n                     occupation_industry employed_full_time\n1                                   &lt;NA&gt;                 NA\n2                                   &lt;NA&gt;                 NA\n3                                   &lt;NA&gt;                 NA\n4 Health care services, except hospitals                  2\n5                   Educational services                  2\n6                                   &lt;NA&gt;                 NA\n  hours_usually_worked weekly_earning paid_hourly hourly_wage\n1                   NA             NA          NA          NA\n2                   NA             NA          NA          NA\n3                   NA             NA          NA          NA\n4                   32         469.44           1       14.67\n5                   12         302.50           1       17.00\n6                   NA             NA          NA          NA\n  hours_worked_hourly_rate time_spent_caring_household\n1                       NA                           0\n2                       NA                           0\n3                       NA                           0\n4                       32                          60\n5                       NA                           0\n6                       NA                           0\n  time_spent_caring_non_household time_spent_education time_spent_eating\n1                               0                    0                40\n2                               0                    0                30\n3                               0                    0                75\n4                               0                    0               165\n5                               0                    0                30\n6                               0                    0               120\n  time_spent_gov time_spent_household_activities time_spent_household_services\n1              0                             190                             0\n2              0                             230                             0\n3              0                             105                             0\n4              0                             395                             0\n5              0                             250                             0\n6              0                             100                             0\n  time_spent_personal_care time_spent_phone time_spent_personal_care_services\n1                      715                0                                 0\n2                      620                0                                 0\n3                     1060                0                                 0\n4                      655               45                                 0\n5                      580              120                                 0\n6                      620                0                                 0\n  time_spent_shopping time_spent_religion time_spent_leisure time_spent_sports\n1                   0                   0                465                 0\n2                   0                   0                560                 0\n3                  60                   0                 20                 0\n4                   0                   0                120                 0\n5                  18                  60                177                 0\n6                   0                   0                355                50\n  time_spent_travelling time_spent_volunteer time_spent_working\n1                    30                    0                  0\n2                     0                    0                  0\n3                    60                    0                  0\n4                     0                    0                  0\n5                    75                  130                  0\n6                    35                    0                  0\n\n\nEverything looks good! I add renameColumns(), convertMissing() and convertClass() to the cleanData() function. I’m finally done with the cleaning component of my workflow. I may have to come back and add additional steps as I make unpleasant discoveries in my analysis, but for now, I can move on.\nBelow I print my final clean.R file\n\n# filename: clean.R\n\n# Main function for data cleaning stage\ncleanData &lt;- function(data) {\n  # rename each of the columns to be human-readable\n  # ignore some of the useless columns (such as alternative ID columns)\n  data &lt;- renameColumns(data)\n  # convert missing data to NA\n  data &lt;- convertMissing(data)\n  # convert integers to meaningful factors\n  data &lt;- convertClass(data)\n  return(data)\n}\n\n# rename each of the columns to be human-readable\nrenameColumns &lt;- function(data) {\n  data &lt;- data %&gt;% select(id = CASEID,\n                          year = YEAR,\n                          # number of attempted contacts\n                          num_contacts = NUMCONTACTS_CPS8,\n                          state = STATEFIP,\n                          household_size = HH_SIZE,\n                          family_income = FAMINCOME,\n                          num_children = HH_NUMKIDS,\n                          num_adults = HH_NUMADULTS,\n                          age = AGE,\n                          sex = SEX,\n                          race = RACE,\n                          marital_status = MARST,\n                          education_level = EDUC,\n                          employment_status = EMPSTAT,\n                          occupation_category = OCC2,\n                          occupation_industry = IND2,\n                          employed_full_time = FULLPART,\n                          hours_usually_worked = UHRSWORKT,\n                          weekly_earning = EARNWEEK,\n                          paid_hourly = PAIDHOUR,\n                          hourly_wage = HOURWAGE,\n                          hours_worked_hourly_rate = HRSATRATE,\n                          time_spent_caring_household = ACT_CAREHH,\n                          time_spent_caring_non_household = ACT_CARENHH,\n                          time_spent_education = ACT_EDUC,\n                          time_spent_eating = ACT_FOOD,\n                          time_spent_gov = ACT_GOVSERV,\n                          time_spent_household_activities = ACT_HHACT,\n                          time_spent_household_services = ACT_HHSERV,\n                          time_spent_personal_care = ACT_PCARE,\n                          time_spent_phone = ACT_PHONE,\n                          time_spent_personal_care_services = ACT_PROFSERV,\n                          time_spent_shopping = ACT_PURCH,\n                          time_spent_religion = ACT_RELIG,\n                          time_spent_leisure = ACT_SOCIAL,\n                          time_spent_sports = ACT_SPORTS,\n                          time_spent_travelling = ACT_TRAVEL,\n                          time_spent_volunteer = ACT_VOL,\n                          time_spent_working = ACT_WORK)\n  return(data)    \n}\n\n# identify missing values equal to 99, 999, etc\nequalFun &lt;- function(x, value) {\n  x == value\n}\n\n# identify if the max of a variable is equal to 99, 999, etc\nmaxFun &lt;- function(x, value) {\n  max(x, na.rm = T) == value\n}\n\n# convert weird missing values to NA\nconvertMissing &lt;- function(data) {\n  # convert missing values to NA\n  data &lt;- data %&gt;%\n    # mutate all missing values coded as 99 to NA\n    mutate_if(function(x) maxFun(x, 99), \n              funs(if_else(equalFun(., 99), NA_integer_, .))) %&gt;%\n    # mutate all missing values coded as 999 to NA\n    mutate_if(function(x) maxFun(x, 999), \n              funs(if_else(equalFun(., 999), NA_integer_, .))) %&gt;%\n    # mutate all missing values coded as 9999 to NA\n    mutate_if(function(x) maxFun(x, 9999), \n              funs(if_else(equalFun(., 9999), NA_integer_, .))) %&gt;%\n    # mutate all missing values coded as 999.99 to NA\n    mutate_if(function(x) maxFun(x, 999.99), \n              funs(if_else(equalFun(., 999.99), NA_real_, .))) %&gt;%\n    # mutate all missing values coded as 99999.99 to NA\n    mutate_if(function(x) maxFun(x, 99999.99), \n              funs(if_else(equalFun(., 99999.99), NA_real_, .))) %&gt;%\n    # mutate all missing values coded as 998 to NA\n    mutate_if(function(x) maxFun(x, 998), \n              funs(if_else(equalFun(., 998), NA_integer_, .))) %&gt;%\n    # mutate all missing values coded as 997 to NA\n    mutate_if(function(x) maxFun(x, 997), \n              funs(if_else(equalFun(., 997), NA_integer_, .))) %&gt;%\n    # mutate all missing values coded as 9995 to NA\n    mutate_if(function(x) maxFun(x, 9995), \n              funs(if_else(equalFun(., 9995), NA_integer_, .))) \n  return(data)\n}\n\n# change numerics to meaningful factors\nconvertClass &lt;- function(data, path_to_codes = \"data/\") {\n  # convert id to a factor\n  data &lt;- data %&gt;% mutate(id = as.factor(id))\n  # loop through each of the factor variables\n  for (variable in c(\"state\", \"occupation_industry\", \"occupation_category\",\n                       \"education_level\", \"race\", \"marital_status\", \"sex\",\n                       \"employment_status\", \"family_income\")) {\n    # identify the path to the code file\n    path &lt;- paste0(path_to_codes, variable, \".txt\")\n    # read in the code file\n    codes &lt;- read.table(path, sep = \"\\t\")\n    # remove the second column (the entries are separated by two \\t's)\n    codes &lt;- codes[, -2]\n    # convert the column names\n    colnames(codes) &lt;- c(variable, paste0(variable, \"_name\"))\n    # add the code to the original data frame\n    data &lt;- left_join(data, codes, by = variable)\n    # remove old variable and replace with new variable\n    data[, variable] &lt;- data[, paste0(variable, \"_name\")]\n    data &lt;- data[, !(colnames(data) %in% paste0(variable, \"_name\"))]\n  }\n  return(data)\n}"
  },
  {
    "objectID": "blog/2018-05-29_alternatives_dodged_bars.html",
    "href": "blog/2018-05-29_alternatives_dodged_bars.html",
    "title": "Alternatives to grouped bar charts",
    "section": "",
    "text": "At some point in your life you have probably found yourself standing face-to-face with a beast known as a grouped bar chart. Perhaps it was in a research paper where the authors were trying to compare the results of several models across different datasets, or it was in a talk given by a colleague who was trying to compare the popularity of different products among distinct groups of consumers.\nThe first time you encountered a grouped bar chart you might have thought “what a neat way to put so much information in a single plot!”. However, the moment you started trying to see whether the orange bar always does better than the green bar but worse than the blue bar.\nIn the example below (made with ggplot2), I show a simple grouped bar chart comparing research funding success rates across a variety of disciplines with the data split into two categories for each discipline: males and females. This data came from Rafael Irizarry’s excellent R package, dslabs, containing datasets for teaching data science.\n\n\n\n\n\n\n\n\n\nEven in this simple example, I find it really difficult to compare male and female funding success rates across genders. Are the males having more success than the females overall? It’s genuinely hard to tell unless I examine each discipline closely and see that men are doing better in the first two groups, but not the two after that, and then I think they are doing a bit better in the categories after that, but now I’m having a hard time and I really want to stop looking at this plot now…\nFortunately, people such as Ann K. Emery (in her post on grouped bar charts) have thought about how to represent this information in a better way. Below I will show 2 alternative approaches (slope charts and horizontal dot plots).\nWhile these are not necessarily “standard” plot types (as in there is no specific geom_ for them), true freedom in visualization comes with the realization that most plots are just arrangements of circles and lines in space. When you truly embrace this idea, you can make any type of figure you want! By means of demonstration, in my next post I show the ggplot2 R code for making each plot.\n\nSlope plots\nThe first alternative is similar to parallel coordinates plots for multi-variate data visualization. The slope plot contains two axes (or three or four, depending on how many groups you have), one for each group (male and female). The disciplines are each represented by a line connecting the male success rates to the female success rates in as coded by their y-position. This plot makes it very clear for which disciplines women have greater funding success than men (the upward sloping, darker colored lines) and it also makes it very clear which disciplines have higher funding success rates overall (Physics and Chemical sciences).\n\n\n\n\n\n\n\n\n\nWhat we see much more clearly is that the number of disciplines in which men are more successful is similar to the number of disciplines in which women are more successful. The discipline with the most similar success rates between men and women is the Chemical sciences. All of this information was very difficult to obtain from the grouped bar chart!\n\n\nHorizontal dot plots\nAnother alternative is the horizontal dot plot: In this case each discipline is again represented by a line, but instead of coding the success rates as the y-position at the end of the discipline slope, the success rate is coded as the x-position along the discipline-specific horizontal line. I have ordered the disciplines by the women’s funding success rate.\n\n\n\n\n\n\n\n\n\nFor me, the horizontal dot plot highlights that in the disciplines with lower funding rates overall (the bottom few rows), men have higher success rates.\nRegardless of your preference, it is fairly clear that both the slope plot and the horizontal dot plots are easier to digest than the grouped bar plot!\n\n\nCode\nMy next post shows the ggplot2 code that I wrote to produce the three plots in this post.\n\n\nReferences\n\nThe research funding data comes from Rafael Irizarry’s dslabs R package\nThe slope and dot plot ideas for this post come from Ann K. Emery’s post on the same topic."
  },
  {
    "objectID": "blog/2017-11-17-ggplot2_tutorial.html",
    "href": "blog/2017-11-17-ggplot2_tutorial.html",
    "title": "ggplot2: Mastering the basics",
    "section": "",
    "text": "An interactive Jupyter Notebook version of this tutorial can be found at https://github.com/rlbarter/ggplot2-thw. Feel free to download it and use for your own learning or teaching adventures!"
  },
  {
    "objectID": "blog/2017-11-17-ggplot2_tutorial.html#useful-resources-for-learning-ggplot2",
    "href": "blog/2017-11-17-ggplot2_tutorial.html#useful-resources-for-learning-ggplot2",
    "title": "ggplot2: Mastering the basics",
    "section": "Useful resources for learning ggplot2",
    "text": "Useful resources for learning ggplot2\n\nggplot2 book (https://www.amazon.com/dp/0387981403/ref=cm_sw_su_dp?tag=ggplot2-20) by Hadley Wickham\nThe layered grammar of graphics (http://vita.had.co.nz/papers/layered-grammar.pdf) by Hadley Wickham"
  },
  {
    "objectID": "blog/2017-11-17-ggplot2_tutorial.html#materials-outline",
    "href": "blog/2017-11-17-ggplot2_tutorial.html#materials-outline",
    "title": "ggplot2: Mastering the basics",
    "section": "Materials outline",
    "text": "Materials outline\nI will begin by providing an overview of the layered grammar of graphics upon which ggplot2 is built. I will then teach ggplot2 by layering examples on top of one another. Finally, I will introduce some advanced topics such as faceting and themes\n\nLayered grammar of graphics\nBasic ggplot2 plot types\nScales, axes, legends and positioning\nFaceting\nThemes: deviating from the defaults to produce beautiful graphics\n\nBy the end of this tutorial, you will be able to produce beautiful graphics such as this:"
  },
  {
    "objectID": "blog/2017-11-17-ggplot2_tutorial.html#the-layered-grammar-of-graphics",
    "href": "blog/2017-11-17-ggplot2_tutorial.html#the-layered-grammar-of-graphics",
    "title": "ggplot2: Mastering the basics",
    "section": "The layered grammar of graphics",
    "text": "The layered grammar of graphics\nHadley Wickham built ggplot2 based on a set of principles outlines in his layered grammar of graphics (inspired by Wilkinson’s original grammar of graphics). The basic idea is that a statistical graphic is a mapping from data to aesthetic attributes (such as colour, shape, and size) of geometric objects (such as points, lines, and bars).\nWe will use some of this terminology as we progress and discover that each piece of terminology corresponds to a type of object in ggplot2.\n\ndata: a data frame containing the variables that you want to visualize\ngeoms: geometric objects (circles, lines, text) that you will actually see\naesthetics: the mapping from the data to the geographic objects (e.g. by describing position, size, colour, etc)"
  },
  {
    "objectID": "blog/2017-11-17-ggplot2_tutorial.html#basic-ggplot2",
    "href": "blog/2017-11-17-ggplot2_tutorial.html#basic-ggplot2",
    "title": "ggplot2: Mastering the basics",
    "section": "Basic ggplot2",
    "text": "Basic ggplot2\nIn this section, we are going to make our first plot. This plot will be based on the gapminder dataset that can be found here. Below, we show the first 6 rows of the gapminder dataset.\n\nlibrary(ggplot2)\n# to download the data directly:\ngapminder &lt;- read.csv(\"https://raw.githubusercontent.com/swcarpentry/r-novice-gapminder/gh-pages/_episodes_rmd/data/gapminder-FiveYearData.csv\")\n\n\nhead(gapminder)\n\n      country year      pop continent lifeExp gdpPercap\n1 Afghanistan 1952  8425333      Asia  28.801  779.4453\n2 Afghanistan 1957  9240934      Asia  30.332  820.8530\n3 Afghanistan 1962 10267083      Asia  31.997  853.1007\n4 Afghanistan 1967 11537966      Asia  34.020  836.1971\n5 Afghanistan 1972 13079460      Asia  36.088  739.9811\n6 Afghanistan 1977 14880372      Asia  38.438  786.1134\n\n\nThe first function we will use is ggplot(). This function allows us to define the data that we will be using to make the plot, as well as the aesthetic properties that will be mapped to the geometric objects. That is, we will tell ggplot which data (a data frame) we are interested in and how each of the variables in our dataset will be used (e.g. as an x or y coordinate, as a coloring variable or a size variable, etc).\nBelow, we define our first ggplot object using the ggplot function, with the gapminder dataset and the x and y aesthetics defined by the gdpPercap and lifeExp variables, respectively.\nThe output of this function is a grid with gdpPercap as the x-axis and lifeExp as the y-axis. However, we have not yet told ggplot what type of geometric object the data will be mapped to, so no data has been displayed.\n\nggplot(gapminder, aes(x = gdpPercap, y = lifeExp))\n\n\n\n\n\n\n\n\n\nAesthetic mapping to layers\nNext, we will add a “geom” layer to our ggplot object. For example, we could add a points layer which would automatically adopt the aesthetic mapping described in the previous line of code.\n\n# describe the base ggplot object and tell it what data we are interested in along with the aesthetic mapping\nggplot(gapminder, aes(x = gdpPercap, y = lifeExp)) +\n  # add a points layer on top\n  geom_point()\n\n\n\n\n\n\n\n\nWhat we have done is map each country (row) in the data to a point in the space defined by the GDP and life expectancy value. The end result is an ugly blob of points. Fortunately, there are many things that we can do to make this blob of points prettier.\nFor example, we can change the transparency of all points by setting the alpha argument to a low value, changing the color of the points to be blue instead of black, and making the points smaller.\n\nggplot(gapminder, aes(x = gdpPercap, y = lifeExp)) +\n  geom_point(alpha = 0.5, col = \"cornflowerblue\", size = 0.5)\n\n\n\n\n\n\n\n\nNote that the above argument changed the alpha value and color for all of the points at once.\nOne of the truly powerful features of ggplot2 is the ability to change these aesthetics based on the data itself. For example, perhaps we want to color each point by its continent. Instead of separating the data into five different subsets (based on the possible values of continent), and adding the different colored points separately, we can simply add all the points once and add an colour aesthetic map for continent.\nNote that whenever we are using a variable from the data to describe an aesthetic property of a geom, this aesthetic property needs to be included in the aes() function.\n\nunique(gapminder$continent)\n\n[1] \"Asia\"     \"Europe\"   \"Africa\"   \"Americas\" \"Oceania\" \n\n\n\nggplot(gapminder, aes(x = gdpPercap, y = lifeExp, color = continent)) +\n  geom_point(alpha = 0.5, size = 0.5)\n\n\n\n\n\n\n\n\nWe could also add aesthetic mappings for other features such as shape, size, transparency (alpha), and more! For example, changing the size based on population:\n\nggplot(gapminder, aes(x = gdpPercap, y = lifeExp, color = continent, size = pop)) +\n  geom_point(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\nTypes of layers\nSo far, we have only seen scatterplots (point geoms), however, there are many other geoms we could add, including:\n\nlines\nhistograms\nboxplots and violin plots\nbarplots\nsmoothed curves\n\n\nggplot(gapminder, aes(x = year, y = lifeExp, group = country, color = continent)) +\n  geom_line(alpha = 0.5)\n\n\n\n\n\n\n\n\n\nggplot(gapminder, aes(x = continent, y = lifeExp, fill = continent)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\nggplot(gapminder, aes(x = lifeExp)) + \n  geom_histogram(binwidth = 3)\n\n\n\n\n\n\n\n\n\nggplot(gapminder, aes(x = gdpPercap, y = lifeExp, size = pop)) +\n  geom_point(aes(color = continent), alpha = 0.5) +\n  geom_smooth(se = FALSE, method = \"loess\", color = \"grey30\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: The following aesthetics were dropped during statistical transformation: size\nℹ This can happen when ggplot fails to infer the correct grouping structure in\n  the data.\nℹ Did you forget to specify a `group` aesthetic or to convert a numerical\n  variable into a factor?"
  },
  {
    "objectID": "blog/2017-11-17-ggplot2_tutorial.html#scales",
    "href": "blog/2017-11-17-ggplot2_tutorial.html#scales",
    "title": "ggplot2: Mastering the basics",
    "section": "Scales",
    "text": "Scales\nWe are going to return to our original scatterplot example to discuss scales, legend and positioning.\nTo remind you, this scatterplot showed GDP per capita against life expectancy for each country colored by continent and sized by population.\nTo keep things simple, let’s filter to a single year.\n\nlibrary(dplyr)\ngapminder_2007 &lt;- gapminder %&gt;% filter(year == 2007)\n\n\nggplot(gapminder_2007, aes(x = gdpPercap, y = lifeExp, color = continent, size = pop)) +\n  geom_point(alpha = 0.5)\n\n\n\n\n\n\n\n\nThe scale of a plot describes the features of the space in which it is plotted. Arguably, it would be better to show gdpPercap on a logarithmic scale, rather than in its raw form. Fortunately, this is easy to do using a scale function, which can be considered another layer that transforms our plot.\n\nggplot(gapminder_2007, aes(x = gdpPercap, y = lifeExp, color = continent, size = pop)) +\n  geom_point(alpha = 0.5) +\n  scale_x_log10()\n\n\n\n\n\n\n\n\nThe default x- (and y-) axes scales are scale_x_continuous and scale_y_continuous, but other options include scale_x_sqrt and scale_x_reverse.\nEach of these scale functions has many options including changing the limits, the breaks, etc. For example in the plot below, we manipulate the x-axis by providing arguments to our scale function of choice.\n\nggplot(gapminder_2007, aes(x = gdpPercap, y = lifeExp, color = continent, size = pop)) +\n  geom_point(alpha = 0.5) +\n  # clean the x-axis breaks\n  scale_x_log10(breaks = c(1, 10, 100, 1000, 10000),\n                limits = c(1, 120000))\n\n\n\n\n\n\n\n\nNotice that we changed the name of the x-axis in the plot using the name argument. This could also be done using the labs function. As an example, below we add a title and change the name of the y-axis and legends using the labs function.\n\nggplot(gapminder_2007, aes(x = gdpPercap, y = lifeExp, color = continent, size = pop)) +\n  # add scatter points\n  geom_point(alpha = 0.5) +\n  # log-scale the x-axis\n  scale_x_log10() +\n  # change labels\n  labs(title = \"GDP versus life expectancy in 2007\",\n       x = \"GDP per capita (log scale)\",\n       y = \"Life expectancy\",\n       size = \"Popoulation\",\n       color = \"Continent\")\n\n\n\n\n\n\n\n\nWe could also manipulate the scale of the size variable. Below, we expand the range of sizes and clean up the variable name. Since the variable we provided for size is a continuous variable (pop) we use the scale_size_continuous argument.\n\nggplot(gapminder_2007, aes(x = gdpPercap, y = lifeExp, color = continent, size = pop)) +\n  # add scatter points\n  geom_point(alpha = 0.5) +\n  # log-scale the x-axis\n  scale_x_log10() +\n  # change labels\n  labs(title = \"GDP versus life expectancy in 2007\",\n       x = \"GDP per capita (log scale)\",\n       y = \"Life expectancy\",\n       size = \"Popoulation (millions)\",\n       color = \"Continent\") +\n  # change the size scale\n  scale_size(range = c(0.1, 10),\n             breaks = 1000000 * c(250, 500, 750, 1000, 1250),\n             labels = c(\"250\", \"500\", \"750\", \"1000\", \"1250\")) \n\n\n\n\n\n\n\n\nScales also exist for other aesthetic features such as fill, color, alpha, etc."
  },
  {
    "objectID": "blog/2017-11-17-ggplot2_tutorial.html#faceting",
    "href": "blog/2017-11-17-ggplot2_tutorial.html#faceting",
    "title": "ggplot2: Mastering the basics",
    "section": "Faceting",
    "text": "Faceting\nSometimes we want to be able to make multiple plots of the same thing across different categories. This can be achieved with minimal repetition using faceting.\nIn the example below, we will remake the plot above individually for each continent.\n\nggplot(gapminder_2007, aes(x = gdpPercap, y = lifeExp, color = continent, size = pop)) +\n  # add scatter points\n  geom_point(alpha = 0.5) +\n  # log-scale the x-axis\n  scale_x_log10() +\n  # change labels\n  labs(title = \"GDP versus life expectancy in 2007\",\n       x = \"GDP per capita (log scale)\",\n       y = \"Life expectancy\",\n       size = \"Popoulation (millions)\",\n       color = \"Continent\") +\n  # change the size scale\n  scale_size(range = c(0.1, 10),\n             breaks = 1000000 * c(250, 500, 750, 1000, 1250),\n             labels = c(\"250\", \"500\", \"750\", \"1000\", \"1250\")) +\n  # add faceting\n  facet_wrap(~continent)"
  },
  {
    "objectID": "blog/2017-11-17-ggplot2_tutorial.html#themes-making-even-more-beautiful-figures-with-ggplot2",
    "href": "blog/2017-11-17-ggplot2_tutorial.html#themes-making-even-more-beautiful-figures-with-ggplot2",
    "title": "ggplot2: Mastering the basics",
    "section": "Themes: making even more beautiful figures with ggplot2",
    "text": "Themes: making even more beautiful figures with ggplot2\nOne of the first things I usually do when I make a ggplot is edit the default theme. I actually really don’t like the grey background, nor do I like having a grid unless it really helps with the plot interpretation.\nOne of the simplest themes is theme_classic, however there are several other themes to choose from. The ggthemes package offers many additional themes, but you could also make your own using the theme() function.\n\nggplot(gapminder_2007, aes(x = gdpPercap, y = lifeExp, color = continent, size = pop)) +\n  # add scatter points\n  geom_point(alpha = 0.5) +\n  # clean the axes names and breaks\n  scale_x_log10(breaks = c(1000, 10000),\n                limits = c(200, 120000)) +\n  # change labels\n  labs(title = \"GDP versus life expectancy in 2007\",\n       x = \"GDP per capita (log scale)\",\n       y = \"Life expectancy\",\n       size = \"Popoulation (millions)\",\n       color = \"Continent\") +\n  # change the size scale\n  scale_size(range = c(0.1, 10),\n             breaks = 1000000 * c(250, 500, 750, 1000, 1250),\n             labels = c(\"250\", \"500\", \"750\", \"1000\", \"1250\")) +\n  # add a nicer theme\n  theme_classic(base_family = \"Avenir\")\n\n\n\n\n\n\n\n\nAs an example of further customization of the ggplot theme, below we do the following:\n\nmove the legend to the top (set legend.position = \"top\" in theme())\nremoving the population legend (set guide = \"none\" in scale_size())\nremove the axes lines (set axis.line = element_blank() in theme())\nadd some text annotations (add geom_text layer)\n\n\nggplot(gapminder_2007) +\n  # add scatter points\n  geom_point(aes(x = gdpPercap, y = lifeExp, color = continent, size = pop),\n             alpha = 0.5) +\n  # add some text annotations for the very large countries\n  geom_text(aes(x = gdpPercap, y = lifeExp + 3, label = country),\n            color = \"grey50\",\n            data = filter(gapminder_2007, pop &gt; 1000000000 | country %in% c(\"Nigeria\", \"United States\"))) +\n  # clean the axes names and breaks\n  scale_x_log10(limits = c(200, 60000)) +\n  # change labels\n  labs(title = \"GDP versus life expectancy in 2007\",\n       x = \"GDP per capita (log scale)\",\n       y = \"Life expectancy\",\n       size = \"Popoulation\",\n       color = \"Continent\") +\n  # change the size scale\n  scale_size(range = c(0.1, 10),\n             # remove size legend\n             guide = \"none\") +\n  # add a nicer theme\n  theme_classic() +\n  # place legend at top and grey axis lines\n  theme(legend.position = \"top\",\n        axis.line = element_line(color = \"grey85\"),\n        axis.ticks = element_line(color = \"grey85\"))"
  },
  {
    "objectID": "blog/2017-11-17-ggplot2_tutorial.html#saving-your-plots",
    "href": "blog/2017-11-17-ggplot2_tutorial.html#saving-your-plots",
    "title": "ggplot2: Mastering the basics",
    "section": "Saving your plots",
    "text": "Saving your plots\nYou can save your plots using the ggsave() function.\n\np &lt;- ggplot(gapminder_2007) +\n  # add scatter points\n  geom_point(aes(x = gdpPercap, y = lifeExp, color = continent, size = pop),\n             alpha = 0.5) +\n  # add some text annotations for the very large countries\n  geom_text(aes(x = gdpPercap, y = lifeExp + 3, label = country),\n            color = \"grey50\",\n            data = filter(gapminder_2007, pop &gt; 1000000000 | country %in% c(\"Nigeria\", \"United States\"))) +\n  # clean the axes names and breaks\n  scale_x_log10(limits = c(200, 60000)) +\n  # change labels\n  labs(title = \"GDP versus life expectancy in 2007\",\n       x = \"GDP per capita (log scale)\",\n       y = \"Life expectancy\",\n       size = \"Popoulation\",\n       color = \"Continent\") +\n  # change the size scale\n  scale_size(range = c(0.1, 10),\n             # remove size legend\n             guide = \"none\") +\n  # add a nicer theme\n  theme_classic() +\n  # place legend at top and grey axis lines\n  theme(legend.position = \"top\",\n        axis.line = element_line(color = \"grey85\"),\n        axis.ticks = element_line(color = \"grey85\"))\n\n# save the plot\nggsave(\"beautiful_plot.png\", p, dpi = 300, width = 6, height = 4)"
  },
  {
    "objectID": "blog/2018-12-04_hypothesis_testing.html",
    "href": "blog/2018-12-04_hypothesis_testing.html",
    "title": "Which hypothesis test should I use? A flowchart",
    "section": "",
    "text": "Many years ago I taught a stats class for which one of the topics was hypothesis testing. Many of the students had a hard time remembering what situation each test was designed for, so I made a flowchart to help piece together the wild world of hypothesis tests.\nWhile the flowchart isn’t pretty (if I made it today, it would be much more attractive), I feel like it might be useful for others, so here it is:\n\n\n\n\n\n\n\n\n\n(if you all like it - I’ll make a pretty version!)"
  },
  {
    "objectID": "blog/2017-04-20-interactive.html",
    "href": "blog/2017-04-20-interactive.html",
    "title": "Interactive visualization in R",
    "section": "",
    "text": "Last week I gave an SGSA seminar on interactive visualizations in R.\nHere is a long-form version of the talk."
  },
  {
    "objectID": "blog/2017-04-20-interactive.html#examples-on-the-web",
    "href": "blog/2017-04-20-interactive.html#examples-on-the-web",
    "title": "Interactive visualization in R",
    "section": "Examples on the web",
    "text": "Examples on the web\nSome super cool examples of interactive data viz on the web include:\n\nOlympic medals\nVaccination simulation\nIncome\nAmerica’s opinions\nMusic\nSam talks too much\n\nWhile these were mostly made using D3, there are certainly ways of making simplified versions of several of these examples directly in R."
  },
  {
    "objectID": "blog/2017-04-20-interactive.html#main-tools-in-r",
    "href": "blog/2017-04-20-interactive.html#main-tools-in-r",
    "title": "Interactive visualization in R",
    "section": "Main tools in R",
    "text": "Main tools in R\nYou can make your scatter plots, line plots, bar plots, etc interactive using the following tools:\n\nPlotly\nHighcharts\ncrosstalk\n\nYou can also make many D3.js plots in R (no javascript required!)\n\nrCharts\nd3scatter\nnetworkD3"
  },
  {
    "objectID": "blog/2017-04-20-interactive.html#ggplot2",
    "href": "blog/2017-04-20-interactive.html#ggplot2",
    "title": "Interactive visualization in R",
    "section": "ggplot2",
    "text": "ggplot2\nA typical ggplot viz looks like this:\n\nlibrary(ggplot2)\ng &lt;- ggplot(txhousing, aes(x = date, y = sales, group = city)) +\n  geom_line(alpha = 0.4)\ng"
  },
  {
    "objectID": "blog/2017-04-20-interactive.html#ggplot2-plotly",
    "href": "blog/2017-04-20-interactive.html#ggplot2-plotly",
    "title": "Interactive visualization in R",
    "section": "ggplot2 + plotly",
    "text": "ggplot2 + plotly\nUsing plotly directly on your ggplot2 graphics makes them interactive!\n\nlibrary(plotly)\ng &lt;- ggplot(txhousing, aes(x = date, y = sales, group = city)) +\n  geom_line(alpha = 0.4) \nggplotly(g, tooltip = c(\"city\"))"
  },
  {
    "objectID": "blog/2017-04-20-interactive.html#plotly",
    "href": "blog/2017-04-20-interactive.html#plotly",
    "title": "Interactive visualization in R",
    "section": "plotly",
    "text": "plotly\nHowever, plotly can be used as a stand-alone function (integrated with the magrittr piping syntax rather than the ggplot + syntax), to create some powerful interactive visualizations based on line charts, scatterplots and barcharts.\n\ng &lt;- txhousing %&gt;% \n  # group by city\n  group_by(city) %&gt;%\n  # initiate a plotly object with date on x and median on y\n  plot_ly(x = ~date, y = ~median) %&gt;%\n  # add a line plot for all texan cities\n  add_lines(name = \"Texan Cities\", hoverinfo = \"none\", \n            type = \"scatter\", mode = \"lines\", \n            line = list(color = 'rgba(192,192,192,0.4)')) %&gt;%\n  # plot separate lines for Dallas and Houston\n  add_lines(name = \"Houston\", \n            data = filter(txhousing, \n                          city %in% c(\"Dallas\", \"Houston\")),\n            hoverinfo = \"city\",\n            line = list(color = c(\"red\", \"blue\")),\n            color = ~city)\ng\n\n\n\n\n\n\n\n\n\nIt is also super easy to add a range slider to your visualization using rangeslider()."
  },
  {
    "objectID": "blog/2017-04-20-interactive.html#linking-with-crosstalk",
    "href": "blog/2017-04-20-interactive.html#linking-with-crosstalk",
    "title": "Interactive visualization in R",
    "section": "Linking with Crosstalk",
    "text": "Linking with Crosstalk\nSometimes you have two plots of the same data and you want to be able to link the data from one plot to the data in the other plot. This, unsurprisingly, is called “linking”, and can be achieved using the crosstalk package.\n\nlibrary(crosstalk)\n# define a shared data object\nd &lt;- SharedData$new(mtcars)\n# make a scatterplot of disp vs mpg\nscatterplot &lt;- plot_ly(d, x = ~mpg, y = ~disp) %&gt;%\n  add_markers(color = I(\"navy\"))\n# define two subplots: boxplot and scatterplot\nsubplot(\n  # boxplot of disp\n  plot_ly(d, y = ~disp) %&gt;% \n    add_boxplot(name = \"overall\", \n                color = I(\"navy\")),\n  # scatterplot of disp vs mpg\n  scatterplot, \n  shareY = TRUE, titleX = T) %&gt;% \n  layout(dragmode = \"select\")\n\n\n\n\n\n\n\n\n\n\n# make subplots\np &lt;- subplot(\n  # histogram (counts) of gear\n  plot_ly(d, x = ~factor(gear)) %&gt;% \n    add_histogram(color = I(\"grey50\")),\n  # scatterplot of disp vs mpg\n  scatterplot, \n  titleX = T\n) \nlayout(p, barmode = \"overlay\")"
  },
  {
    "objectID": "blog/2017-04-20-interactive.html#easy-d3.js-in-r-force-networks",
    "href": "blog/2017-04-20-interactive.html#easy-d3.js-in-r-force-networks",
    "title": "Interactive visualization in R",
    "section": "Easy D3.js in R: Force networks",
    "text": "Easy D3.js in R: Force networks\nFor those who want to create cool D3 graphs directly in R, fortunately there are a few packages that do just that.\nMaking those jiggly force-directed networks can be achieved using the networkD3 package.\n\nlibrary(networkD3)\ndata(MisLinks, MisNodes)\nhead(MisLinks, 3)\n\n  source target value\n1      1      0     1\n2      2      0     8\n3      3      0    10\n\nhead(MisNodes, 3)\n\n             name group size\n1          Myriel     1   15\n2        Napoleon     1   20\n3 Mlle.Baptistine     1   23\n\n\n\nforceNetwork(Links = MisLinks, Nodes = MisNodes, Source = \"source\",\n             Target = \"target\", Value = \"value\", NodeID = \"name\",\n             Group = \"group\", opacity = 0.9, Nodesize = 3, \n             linkDistance = 100, fontSize = 20)"
  },
  {
    "objectID": "blog/2017-04-20-interactive.html#references",
    "href": "blog/2017-04-20-interactive.html#references",
    "title": "Interactive visualization in R",
    "section": "References",
    "text": "References\nplotly references:\nhttps://cpsievert.github.io/plotly_book/\nhttps://plot.ly/r/\ncrosstalk:\nhttps://rstudio.github.io/crosstalk/using.html\nchord diagram example:\nhttp://stackoverflow.com/questions/14599150/chord-diagram-in-r"
  },
  {
    "objectID": "blog/2019-08-05_base_r_to_tidyverse.html",
    "href": "blog/2019-08-05_base_r_to_tidyverse.html",
    "title": "Transitioning into the tidyverse (part 1)",
    "section": "",
    "text": "Most people who learned R before the tidyverse have likely started to feel a nibble of pressure to get aboard the tidyverse train. Sadly a fact of human nature is that once you’re comfortable doing something a certain way, it’s hard to find the motivation to learn a different way of doing something that you already know how to do. As someone first learnt R 10 years ago (long before the tidyverse) I’ve been there. Five years ago, I was pushed to stick my little toe into the shallow-end of the tidyverse pool by learning ggplot2, and I never went back.\nWhile the tidyverse is primarily made up of a set of super useful R packages (ggplot2, dplyr, purrr, tidyr, readr, tibble), it is also a way of thinking about implementing “tidy” data analysis. If you combine tidy thinking with the tidy packages, you will inevitably become a master of tidy analysis. From where I float now substantially closer to the deep-end of the tidyverse pool, I would provide the following arguments you as well as to my past self for why it is a good idea to learn the tidyverse:\nMuch of the initial efforts of the tidyverse were the brainchild of Hadley Wickham, but these days there are a huge number of people who contribute to, maintain, and develop the tidyverse. The tidyverse is open-source and collaborative (which means that you - yes you - could contribute to it if you wanted to), and is hosted on the tidyverse github: https://github.com/tidyverse.\nThe goal of this post is to summarise the overall goals of the tidyverse, provide short tutorials on each of the packages that form the tidyverse (and how they play together), and to provide links to additional resources for learning them.\nFor new tidyverse practitioners, I’d recommend focusing on getting a handle on piping %&gt;%, and learning the dplyr and ggplot2 packages (these form part one of this post). Once you feel comfortable with these core aspects of the tidyverse, you can move onto part two of this two-part series on the tidyverse to learn about the remaining packages.\nIt is important to remember that the tidyverse is constantly evolving. The best ways to keep up to date with the evolving tidyverse ecosystem is (1) to follow the RStudio blog (https://blog.rstudio.com/), and (2) start following R people on twitter. Mara Averick (@dataandme) and Hadley Wickham (@hadleywickham) are good people to follow. A great resource for learning about the tidyverse in more detail is R for Data Science by Garrett Grolemund and Hadley Wickham."
  },
  {
    "objectID": "blog/2019-08-05_base_r_to_tidyverse.html#select-select-columns",
    "href": "blog/2019-08-05_base_r_to_tidyverse.html#select-select-columns",
    "title": "Transitioning into the tidyverse (part 1)",
    "section": "select: select columns",
    "text": "select: select columns\nThe arguments of the select function specify which data frame variables should be kept. select() is like indexing columns by name. You do not need to quote the column names (but you can if you want to).\n\ngapminder %&gt;% \n  select(country, gdpPercap) \n\n                      country   gdpPercap\n1                 Afghanistan    779.4453\n2                 Afghanistan    820.8530\n3                 Afghanistan    853.1007\n4                 Afghanistan    836.1971\n5                 Afghanistan    739.9811\n6                 Afghanistan    786.1134\n7                 Afghanistan    978.0114\n8                 Afghanistan    852.3959\n9                 Afghanistan    649.3414\n10                Afghanistan    635.3414\n11                Afghanistan    726.7341\n12                Afghanistan    974.5803\n13                    Albania   1601.0561\n14                    Albania   1942.2842\n15                    Albania   2312.8890\n16                    Albania   2760.1969\n17                    Albania   3313.4222\n18                    Albania   3533.0039\n19                    Albania   3630.8807\n20                    Albania   3738.9327\n21                    Albania   2497.4379\n22                    Albania   3193.0546\n23                    Albania   4604.2117\n24                    Albania   5937.0295\n25                    Algeria   2449.0082\n26                    Algeria   3013.9760\n27                    Algeria   2550.8169\n28                    Algeria   3246.9918\n29                    Algeria   4182.6638\n30                    Algeria   4910.4168\n31                    Algeria   5745.1602\n32                    Algeria   5681.3585\n33                    Algeria   5023.2166\n34                    Algeria   4797.2951\n35                    Algeria   5288.0404\n36                    Algeria   6223.3675\n37                     Angola   3520.6103\n38                     Angola   3827.9405\n39                     Angola   4269.2767\n40                     Angola   5522.7764\n41                     Angola   5473.2880\n42                     Angola   3008.6474\n43                     Angola   2756.9537\n44                     Angola   2430.2083\n45                     Angola   2627.8457\n46                     Angola   2277.1409\n47                     Angola   2773.2873\n48                     Angola   4797.2313\n49                  Argentina   5911.3151\n50                  Argentina   6856.8562\n51                  Argentina   7133.1660\n52                  Argentina   8052.9530\n53                  Argentina   9443.0385\n54                  Argentina  10079.0267\n55                  Argentina   8997.8974\n56                  Argentina   9139.6714\n57                  Argentina   9308.4187\n58                  Argentina  10967.2820\n59                  Argentina   8797.6407\n60                  Argentina  12779.3796\n61                  Australia  10039.5956\n62                  Australia  10949.6496\n63                  Australia  12217.2269\n64                  Australia  14526.1246\n65                  Australia  16788.6295\n66                  Australia  18334.1975\n67                  Australia  19477.0093\n68                  Australia  21888.8890\n69                  Australia  23424.7668\n70                  Australia  26997.9366\n71                  Australia  30687.7547\n72                  Australia  34435.3674\n73                    Austria   6137.0765\n74                    Austria   8842.5980\n75                    Austria  10750.7211\n76                    Austria  12834.6024\n77                    Austria  16661.6256\n78                    Austria  19749.4223\n79                    Austria  21597.0836\n80                    Austria  23687.8261\n81                    Austria  27042.0187\n82                    Austria  29095.9207\n83                    Austria  32417.6077\n84                    Austria  36126.4927\n85                    Bahrain   9867.0848\n86                    Bahrain  11635.7995\n87                    Bahrain  12753.2751\n88                    Bahrain  14804.6727\n89                    Bahrain  18268.6584\n90                    Bahrain  19340.1020\n91                    Bahrain  19211.1473\n92                    Bahrain  18524.0241\n93                    Bahrain  19035.5792\n94                    Bahrain  20292.0168\n95                    Bahrain  23403.5593\n96                    Bahrain  29796.0483\n97                 Bangladesh    684.2442\n98                 Bangladesh    661.6375\n99                 Bangladesh    686.3416\n100                Bangladesh    721.1861\n101                Bangladesh    630.2336\n102                Bangladesh    659.8772\n103                Bangladesh    676.9819\n104                Bangladesh    751.9794\n105                Bangladesh    837.8102\n106                Bangladesh    972.7700\n107                Bangladesh   1136.3904\n108                Bangladesh   1391.2538\n109                   Belgium   8343.1051\n110                   Belgium   9714.9606\n111                   Belgium  10991.2068\n112                   Belgium  13149.0412\n113                   Belgium  16672.1436\n114                   Belgium  19117.9745\n115                   Belgium  20979.8459\n116                   Belgium  22525.5631\n117                   Belgium  25575.5707\n118                   Belgium  27561.1966\n119                   Belgium  30485.8838\n120                   Belgium  33692.6051\n121                     Benin   1062.7522\n122                     Benin    959.6011\n123                     Benin    949.4991\n124                     Benin   1035.8314\n125                     Benin   1085.7969\n126                     Benin   1029.1613\n127                     Benin   1277.8976\n128                     Benin   1225.8560\n129                     Benin   1191.2077\n130                     Benin   1232.9753\n131                     Benin   1372.8779\n132                     Benin   1441.2849\n133                   Bolivia   2677.3263\n134                   Bolivia   2127.6863\n135                   Bolivia   2180.9725\n136                   Bolivia   2586.8861\n137                   Bolivia   2980.3313\n138                   Bolivia   3548.0978\n139                   Bolivia   3156.5105\n140                   Bolivia   2753.6915\n141                   Bolivia   2961.6997\n142                   Bolivia   3326.1432\n143                   Bolivia   3413.2627\n144                   Bolivia   3822.1371\n145    Bosnia and Herzegovina    973.5332\n146    Bosnia and Herzegovina   1353.9892\n147    Bosnia and Herzegovina   1709.6837\n148    Bosnia and Herzegovina   2172.3524\n149    Bosnia and Herzegovina   2860.1698\n150    Bosnia and Herzegovina   3528.4813\n151    Bosnia and Herzegovina   4126.6132\n152    Bosnia and Herzegovina   4314.1148\n153    Bosnia and Herzegovina   2546.7814\n154    Bosnia and Herzegovina   4766.3559\n155    Bosnia and Herzegovina   6018.9752\n156    Bosnia and Herzegovina   7446.2988\n157                  Botswana    851.2411\n158                  Botswana    918.2325\n159                  Botswana    983.6540\n160                  Botswana   1214.7093\n161                  Botswana   2263.6111\n162                  Botswana   3214.8578\n163                  Botswana   4551.1421\n164                  Botswana   6205.8839\n165                  Botswana   7954.1116\n166                  Botswana   8647.1423\n167                  Botswana  11003.6051\n168                  Botswana  12569.8518\n169                    Brazil   2108.9444\n170                    Brazil   2487.3660\n171                    Brazil   3336.5858\n172                    Brazil   3429.8644\n173                    Brazil   4985.7115\n174                    Brazil   6660.1187\n175                    Brazil   7030.8359\n176                    Brazil   7807.0958\n177                    Brazil   6950.2830\n178                    Brazil   7957.9808\n179                    Brazil   8131.2128\n180                    Brazil   9065.8008\n181                  Bulgaria   2444.2866\n182                  Bulgaria   3008.6707\n183                  Bulgaria   4254.3378\n184                  Bulgaria   5577.0028\n185                  Bulgaria   6597.4944\n186                  Bulgaria   7612.2404\n187                  Bulgaria   8224.1916\n188                  Bulgaria   8239.8548\n189                  Bulgaria   6302.6234\n190                  Bulgaria   5970.3888\n191                  Bulgaria   7696.7777\n192                  Bulgaria  10680.7928\n193              Burkina Faso    543.2552\n194              Burkina Faso    617.1835\n195              Burkina Faso    722.5120\n196              Burkina Faso    794.8266\n197              Burkina Faso    854.7360\n198              Burkina Faso    743.3870\n199              Burkina Faso    807.1986\n200              Burkina Faso    912.0631\n201              Burkina Faso    931.7528\n202              Burkina Faso    946.2950\n203              Burkina Faso   1037.6452\n204              Burkina Faso   1217.0330\n205                   Burundi    339.2965\n206                   Burundi    379.5646\n207                   Burundi    355.2032\n208                   Burundi    412.9775\n209                   Burundi    464.0995\n210                   Burundi    556.1033\n211                   Burundi    559.6032\n212                   Burundi    621.8188\n213                   Burundi    631.6999\n214                   Burundi    463.1151\n215                   Burundi    446.4035\n216                   Burundi    430.0707\n217                  Cambodia    368.4693\n218                  Cambodia    434.0383\n219                  Cambodia    496.9136\n220                  Cambodia    523.4323\n221                  Cambodia    421.6240\n222                  Cambodia    524.9722\n223                  Cambodia    624.4755\n224                  Cambodia    683.8956\n225                  Cambodia    682.3032\n226                  Cambodia    734.2852\n227                  Cambodia    896.2260\n228                  Cambodia   1713.7787\n229                  Cameroon   1172.6677\n230                  Cameroon   1313.0481\n231                  Cameroon   1399.6074\n232                  Cameroon   1508.4531\n233                  Cameroon   1684.1465\n234                  Cameroon   1783.4329\n235                  Cameroon   2367.9833\n236                  Cameroon   2602.6642\n237                  Cameroon   1793.1633\n238                  Cameroon   1694.3375\n239                  Cameroon   1934.0114\n240                  Cameroon   2042.0952\n241                    Canada  11367.1611\n242                    Canada  12489.9501\n243                    Canada  13462.4855\n244                    Canada  16076.5880\n245                    Canada  18970.5709\n246                    Canada  22090.8831\n247                    Canada  22898.7921\n248                    Canada  26626.5150\n249                    Canada  26342.8843\n250                    Canada  28954.9259\n251                    Canada  33328.9651\n252                    Canada  36319.2350\n253  Central African Republic   1071.3107\n254  Central African Republic   1190.8443\n255  Central African Republic   1193.0688\n256  Central African Republic   1136.0566\n257  Central African Republic   1070.0133\n258  Central African Republic   1109.3743\n259  Central African Republic    956.7530\n260  Central African Republic    844.8764\n261  Central African Republic    747.9055\n262  Central African Republic    740.5063\n263  Central African Republic    738.6906\n264  Central African Republic    706.0165\n265                      Chad   1178.6659\n266                      Chad   1308.4956\n267                      Chad   1389.8176\n268                      Chad   1196.8106\n269                      Chad   1104.1040\n270                      Chad   1133.9850\n271                      Chad    797.9081\n272                      Chad    952.3861\n273                      Chad   1058.0643\n274                      Chad   1004.9614\n275                      Chad   1156.1819\n276                      Chad   1704.0637\n277                     Chile   3939.9788\n278                     Chile   4315.6227\n279                     Chile   4519.0943\n280                     Chile   5106.6543\n281                     Chile   5494.0244\n282                     Chile   4756.7638\n283                     Chile   5095.6657\n284                     Chile   5547.0638\n285                     Chile   7596.1260\n286                     Chile  10118.0532\n287                     Chile  10778.7838\n288                     Chile  13171.6388\n289                     China    400.4486\n290                     China    575.9870\n291                     China    487.6740\n292                     China    612.7057\n293                     China    676.9001\n294                     China    741.2375\n295                     China    962.4214\n296                     China   1378.9040\n297                     China   1655.7842\n298                     China   2289.2341\n299                     China   3119.2809\n300                     China   4959.1149\n301                  Colombia   2144.1151\n302                  Colombia   2323.8056\n303                  Colombia   2492.3511\n304                  Colombia   2678.7298\n305                  Colombia   3264.6600\n306                  Colombia   3815.8079\n307                  Colombia   4397.5757\n308                  Colombia   4903.2191\n309                  Colombia   5444.6486\n310                  Colombia   6117.3617\n311                  Colombia   5755.2600\n312                  Colombia   7006.5804\n313                   Comoros   1102.9909\n314                   Comoros   1211.1485\n315                   Comoros   1406.6483\n316                   Comoros   1876.0296\n317                   Comoros   1937.5777\n318                   Comoros   1172.6030\n319                   Comoros   1267.1001\n320                   Comoros   1315.9808\n321                   Comoros   1246.9074\n322                   Comoros   1173.6182\n323                   Comoros   1075.8116\n324                   Comoros    986.1479\n325           Congo Dem. Rep.    780.5423\n326           Congo Dem. Rep.    905.8602\n327           Congo Dem. Rep.    896.3146\n328           Congo Dem. Rep.    861.5932\n329           Congo Dem. Rep.    904.8961\n330           Congo Dem. Rep.    795.7573\n331           Congo Dem. Rep.    673.7478\n332           Congo Dem. Rep.    672.7748\n333           Congo Dem. Rep.    457.7192\n334           Congo Dem. Rep.    312.1884\n335           Congo Dem. Rep.    241.1659\n336           Congo Dem. Rep.    277.5519\n337                Congo Rep.   2125.6214\n338                Congo Rep.   2315.0566\n339                Congo Rep.   2464.7832\n340                Congo Rep.   2677.9396\n341                Congo Rep.   3213.1527\n342                Congo Rep.   3259.1790\n343                Congo Rep.   4879.5075\n344                Congo Rep.   4201.1949\n345                Congo Rep.   4016.2395\n346                Congo Rep.   3484.1644\n347                Congo Rep.   3484.0620\n348                Congo Rep.   3632.5578\n349                Costa Rica   2627.0095\n350                Costa Rica   2990.0108\n351                Costa Rica   3460.9370\n352                Costa Rica   4161.7278\n353                Costa Rica   5118.1469\n354                Costa Rica   5926.8770\n355                Costa Rica   5262.7348\n356                Costa Rica   5629.9153\n357                Costa Rica   6160.4163\n358                Costa Rica   6677.0453\n359                Costa Rica   7723.4472\n360                Costa Rica   9645.0614\n361             Cote d'Ivoire   1388.5947\n362             Cote d'Ivoire   1500.8959\n363             Cote d'Ivoire   1728.8694\n364             Cote d'Ivoire   2052.0505\n365             Cote d'Ivoire   2378.2011\n366             Cote d'Ivoire   2517.7365\n367             Cote d'Ivoire   2602.7102\n368             Cote d'Ivoire   2156.9561\n369             Cote d'Ivoire   1648.0738\n370             Cote d'Ivoire   1786.2654\n371             Cote d'Ivoire   1648.8008\n372             Cote d'Ivoire   1544.7501\n373                   Croatia   3119.2365\n374                   Croatia   4338.2316\n375                   Croatia   5477.8900\n376                   Croatia   6960.2979\n377                   Croatia   9164.0901\n378                   Croatia  11305.3852\n379                   Croatia  13221.8218\n380                   Croatia  13822.5839\n381                   Croatia   8447.7949\n382                   Croatia   9875.6045\n383                   Croatia  11628.3890\n384                   Croatia  14619.2227\n385                      Cuba   5586.5388\n386                      Cuba   6092.1744\n387                      Cuba   5180.7559\n388                      Cuba   5690.2680\n389                      Cuba   5305.4453\n390                      Cuba   6380.4950\n391                      Cuba   7316.9181\n392                      Cuba   7532.9248\n393                      Cuba   5592.8440\n394                      Cuba   5431.9904\n395                      Cuba   6340.6467\n396                      Cuba   8948.1029\n397            Czech Republic   6876.1403\n398            Czech Republic   8256.3439\n399            Czech Republic  10136.8671\n400            Czech Republic  11399.4449\n401            Czech Republic  13108.4536\n402            Czech Republic  14800.1606\n403            Czech Republic  15377.2285\n404            Czech Republic  16310.4434\n405            Czech Republic  14297.0212\n406            Czech Republic  16048.5142\n407            Czech Republic  17596.2102\n408            Czech Republic  22833.3085\n409                   Denmark   9692.3852\n410                   Denmark  11099.6593\n411                   Denmark  13583.3135\n412                   Denmark  15937.2112\n413                   Denmark  18866.2072\n414                   Denmark  20422.9015\n415                   Denmark  21688.0405\n416                   Denmark  25116.1758\n417                   Denmark  26406.7399\n418                   Denmark  29804.3457\n419                   Denmark  32166.5001\n420                   Denmark  35278.4187\n421                  Djibouti   2669.5295\n422                  Djibouti   2864.9691\n423                  Djibouti   3020.9893\n424                  Djibouti   3020.0505\n425                  Djibouti   3694.2124\n426                  Djibouti   3081.7610\n427                  Djibouti   2879.4681\n428                  Djibouti   2880.1026\n429                  Djibouti   2377.1562\n430                  Djibouti   1895.0170\n431                  Djibouti   1908.2609\n432                  Djibouti   2082.4816\n433        Dominican Republic   1397.7171\n434        Dominican Republic   1544.4030\n435        Dominican Republic   1662.1374\n436        Dominican Republic   1653.7230\n437        Dominican Republic   2189.8745\n438        Dominican Republic   2681.9889\n439        Dominican Republic   2861.0924\n440        Dominican Republic   2899.8422\n441        Dominican Republic   3044.2142\n442        Dominican Republic   3614.1013\n443        Dominican Republic   4563.8082\n444        Dominican Republic   6025.3748\n445                   Ecuador   3522.1107\n446                   Ecuador   3780.5467\n447                   Ecuador   4086.1141\n448                   Ecuador   4579.0742\n449                   Ecuador   5280.9947\n450                   Ecuador   6679.6233\n451                   Ecuador   7213.7913\n452                   Ecuador   6481.7770\n453                   Ecuador   7103.7026\n454                   Ecuador   7429.4559\n455                   Ecuador   5773.0445\n456                   Ecuador   6873.2623\n457                     Egypt   1418.8224\n458                     Egypt   1458.9153\n459                     Egypt   1693.3359\n460                     Egypt   1814.8807\n461                     Egypt   2024.0081\n462                     Egypt   2785.4936\n463                     Egypt   3503.7296\n464                     Egypt   3885.4607\n465                     Egypt   3794.7552\n466                     Egypt   4173.1818\n467                     Egypt   4754.6044\n468                     Egypt   5581.1810\n469               El Salvador   3048.3029\n470               El Salvador   3421.5232\n471               El Salvador   3776.8036\n472               El Salvador   4358.5954\n473               El Salvador   4520.2460\n474               El Salvador   5138.9224\n475               El Salvador   4098.3442\n476               El Salvador   4140.4421\n477               El Salvador   4444.2317\n478               El Salvador   5154.8255\n479               El Salvador   5351.5687\n480               El Salvador   5728.3535\n481         Equatorial Guinea    375.6431\n482         Equatorial Guinea    426.0964\n483         Equatorial Guinea    582.8420\n484         Equatorial Guinea    915.5960\n485         Equatorial Guinea    672.4123\n486         Equatorial Guinea    958.5668\n487         Equatorial Guinea    927.8253\n488         Equatorial Guinea    966.8968\n489         Equatorial Guinea   1132.0550\n490         Equatorial Guinea   2814.4808\n491         Equatorial Guinea   7703.4959\n492         Equatorial Guinea  12154.0897\n493                   Eritrea    328.9406\n494                   Eritrea    344.1619\n495                   Eritrea    380.9958\n496                   Eritrea    468.7950\n497                   Eritrea    514.3242\n498                   Eritrea    505.7538\n499                   Eritrea    524.8758\n500                   Eritrea    521.1341\n501                   Eritrea    582.8585\n502                   Eritrea    913.4708\n503                   Eritrea    765.3500\n504                   Eritrea    641.3695\n505                  Ethiopia    362.1463\n506                  Ethiopia    378.9042\n507                  Ethiopia    419.4564\n508                  Ethiopia    516.1186\n509                  Ethiopia    566.2439\n510                  Ethiopia    556.8084\n511                  Ethiopia    577.8607\n512                  Ethiopia    573.7413\n513                  Ethiopia    421.3535\n514                  Ethiopia    515.8894\n515                  Ethiopia    530.0535\n516                  Ethiopia    690.8056\n517                   Finland   6424.5191\n518                   Finland   7545.4154\n519                   Finland   9371.8426\n520                   Finland  10921.6363\n521                   Finland  14358.8759\n522                   Finland  15605.4228\n523                   Finland  18533.1576\n524                   Finland  21141.0122\n525                   Finland  20647.1650\n526                   Finland  23723.9502\n527                   Finland  28204.5906\n528                   Finland  33207.0844\n529                    France   7029.8093\n530                    France   8662.8349\n531                    France  10560.4855\n532                    France  12999.9177\n533                    France  16107.1917\n534                    France  18292.6351\n535                    France  20293.8975\n536                    France  22066.4421\n537                    France  24703.7961\n538                    France  25889.7849\n539                    France  28926.0323\n540                    France  30470.0167\n541                     Gabon   4293.4765\n542                     Gabon   4976.1981\n543                     Gabon   6631.4592\n544                     Gabon   8358.7620\n545                     Gabon  11401.9484\n546                     Gabon  21745.5733\n547                     Gabon  15113.3619\n548                     Gabon  11864.4084\n549                     Gabon  13522.1575\n550                     Gabon  14722.8419\n551                     Gabon  12521.7139\n552                     Gabon  13206.4845\n553                    Gambia    485.2307\n554                    Gambia    520.9267\n555                    Gambia    599.6503\n556                    Gambia    734.7829\n557                    Gambia    756.0868\n558                    Gambia    884.7553\n559                    Gambia    835.8096\n560                    Gambia    611.6589\n561                    Gambia    665.6244\n562                    Gambia    653.7302\n563                    Gambia    660.5856\n564                    Gambia    752.7497\n565                   Germany   7144.1144\n566                   Germany  10187.8267\n567                   Germany  12902.4629\n568                   Germany  14745.6256\n569                   Germany  18016.1803\n570                   Germany  20512.9212\n571                   Germany  22031.5327\n572                   Germany  24639.1857\n573                   Germany  26505.3032\n574                   Germany  27788.8842\n575                   Germany  30035.8020\n576                   Germany  32170.3744\n577                     Ghana    911.2989\n578                     Ghana   1043.5615\n579                     Ghana   1190.0411\n580                     Ghana   1125.6972\n581                     Ghana   1178.2237\n582                     Ghana    993.2240\n583                     Ghana    876.0326\n584                     Ghana    847.0061\n585                     Ghana    925.0602\n586                     Ghana   1005.2458\n587                     Ghana   1111.9846\n588                     Ghana   1327.6089\n589                    Greece   3530.6901\n590                    Greece   4916.2999\n591                    Greece   6017.1907\n592                    Greece   8513.0970\n593                    Greece  12724.8296\n594                    Greece  14195.5243\n595                    Greece  15268.4209\n596                    Greece  16120.5284\n597                    Greece  17541.4963\n598                    Greece  18747.6981\n599                    Greece  22514.2548\n600                    Greece  27538.4119\n601                 Guatemala   2428.2378\n602                 Guatemala   2617.1560\n603                 Guatemala   2750.3644\n604                 Guatemala   3242.5311\n605                 Guatemala   4031.4083\n606                 Guatemala   4879.9927\n607                 Guatemala   4820.4948\n608                 Guatemala   4246.4860\n609                 Guatemala   4439.4508\n610                 Guatemala   4684.3138\n611                 Guatemala   4858.3475\n612                 Guatemala   5186.0500\n613                    Guinea    510.1965\n614                    Guinea    576.2670\n615                    Guinea    686.3737\n616                    Guinea    708.7595\n617                    Guinea    741.6662\n618                    Guinea    874.6859\n619                    Guinea    857.2504\n620                    Guinea    805.5725\n621                    Guinea    794.3484\n622                    Guinea    869.4498\n623                    Guinea    945.5836\n624                    Guinea    942.6542\n625             Guinea-Bissau    299.8503\n626             Guinea-Bissau    431.7905\n627             Guinea-Bissau    522.0344\n628             Guinea-Bissau    715.5806\n629             Guinea-Bissau    820.2246\n630             Guinea-Bissau    764.7260\n631             Guinea-Bissau    838.1240\n632             Guinea-Bissau    736.4154\n633             Guinea-Bissau    745.5399\n634             Guinea-Bissau    796.6645\n635             Guinea-Bissau    575.7047\n636             Guinea-Bissau    579.2317\n637                     Haiti   1840.3669\n638                     Haiti   1726.8879\n639                     Haiti   1796.5890\n640                     Haiti   1452.0577\n641                     Haiti   1654.4569\n642                     Haiti   1874.2989\n643                     Haiti   2011.1595\n644                     Haiti   1823.0160\n645                     Haiti   1456.3095\n646                     Haiti   1341.7269\n647                     Haiti   1270.3649\n648                     Haiti   1201.6372\n649                  Honduras   2194.9262\n650                  Honduras   2220.4877\n651                  Honduras   2291.1568\n652                  Honduras   2538.2694\n653                  Honduras   2529.8423\n654                  Honduras   3203.2081\n655                  Honduras   3121.7608\n656                  Honduras   3023.0967\n657                  Honduras   3081.6946\n658                  Honduras   3160.4549\n659                  Honduras   3099.7287\n660                  Honduras   3548.3308\n661           Hong Kong China   3054.4212\n662           Hong Kong China   3629.0765\n663           Hong Kong China   4692.6483\n664           Hong Kong China   6197.9628\n665           Hong Kong China   8315.9281\n666           Hong Kong China  11186.1413\n667           Hong Kong China  14560.5305\n668           Hong Kong China  20038.4727\n669           Hong Kong China  24757.6030\n670           Hong Kong China  28377.6322\n671           Hong Kong China  30209.0152\n672           Hong Kong China  39724.9787\n673                   Hungary   5263.6738\n674                   Hungary   6040.1800\n675                   Hungary   7550.3599\n676                   Hungary   9326.6447\n677                   Hungary  10168.6561\n678                   Hungary  11674.8374\n679                   Hungary  12545.9907\n680                   Hungary  12986.4800\n681                   Hungary  10535.6285\n682                   Hungary  11712.7768\n683                   Hungary  14843.9356\n684                   Hungary  18008.9444\n685                   Iceland   7267.6884\n686                   Iceland   9244.0014\n687                   Iceland  10350.1591\n688                   Iceland  13319.8957\n689                   Iceland  15798.0636\n690                   Iceland  19654.9625\n691                   Iceland  23269.6075\n692                   Iceland  26923.2063\n693                   Iceland  25144.3920\n694                   Iceland  28061.0997\n695                   Iceland  31163.2020\n696                   Iceland  36180.7892\n697                     India    546.5657\n698                     India    590.0620\n699                     India    658.3472\n700                     India    700.7706\n701                     India    724.0325\n702                     India    813.3373\n703                     India    855.7235\n704                     India    976.5127\n705                     India   1164.4068\n706                     India   1458.8174\n707                     India   1746.7695\n708                     India   2452.2104\n709                 Indonesia    749.6817\n710                 Indonesia    858.9003\n711                 Indonesia    849.2898\n712                 Indonesia    762.4318\n713                 Indonesia   1111.1079\n714                 Indonesia   1382.7021\n715                 Indonesia   1516.8730\n716                 Indonesia   1748.3570\n717                 Indonesia   2383.1409\n718                 Indonesia   3119.3356\n719                 Indonesia   2873.9129\n720                 Indonesia   3540.6516\n721                      Iran   3035.3260\n722                      Iran   3290.2576\n723                      Iran   4187.3298\n724                      Iran   5906.7318\n725                      Iran   9613.8186\n726                      Iran  11888.5951\n727                      Iran   7608.3346\n728                      Iran   6642.8814\n729                      Iran   7235.6532\n730                      Iran   8263.5903\n731                      Iran   9240.7620\n732                      Iran  11605.7145\n733                      Iraq   4129.7661\n734                      Iraq   6229.3336\n735                      Iraq   8341.7378\n736                      Iraq   8931.4598\n737                      Iraq   9576.0376\n738                      Iraq  14688.2351\n739                      Iraq  14517.9071\n740                      Iraq  11643.5727\n741                      Iraq   3745.6407\n742                      Iraq   3076.2398\n743                      Iraq   4390.7173\n744                      Iraq   4471.0619\n745                   Ireland   5210.2803\n746                   Ireland   5599.0779\n747                   Ireland   6631.5973\n748                   Ireland   7655.5690\n749                   Ireland   9530.7729\n750                   Ireland  11150.9811\n751                   Ireland  12618.3214\n752                   Ireland  13872.8665\n753                   Ireland  17558.8155\n754                   Ireland  24521.9471\n755                   Ireland  34077.0494\n756                   Ireland  40675.9964\n757                    Israel   4086.5221\n758                    Israel   5385.2785\n759                    Israel   7105.6307\n760                    Israel   8393.7414\n761                    Israel  12786.9322\n762                    Israel  13306.6192\n763                    Israel  15367.0292\n764                    Israel  17122.4799\n765                    Israel  18051.5225\n766                    Israel  20896.6092\n767                    Israel  21905.5951\n768                    Israel  25523.2771\n769                     Italy   4931.4042\n770                     Italy   6248.6562\n771                     Italy   8243.5823\n772                     Italy  10022.4013\n773                     Italy  12269.2738\n774                     Italy  14255.9847\n775                     Italy  16537.4835\n776                     Italy  19207.2348\n777                     Italy  22013.6449\n778                     Italy  24675.0245\n779                     Italy  27968.0982\n780                     Italy  28569.7197\n781                   Jamaica   2898.5309\n782                   Jamaica   4756.5258\n783                   Jamaica   5246.1075\n784                   Jamaica   6124.7035\n785                   Jamaica   7433.8893\n786                   Jamaica   6650.1956\n787                   Jamaica   6068.0513\n788                   Jamaica   6351.2375\n789                   Jamaica   7404.9237\n790                   Jamaica   7121.9247\n791                   Jamaica   6994.7749\n792                   Jamaica   7320.8803\n793                     Japan   3216.9563\n794                     Japan   4317.6944\n795                     Japan   6576.6495\n796                     Japan   9847.7886\n797                     Japan  14778.7864\n798                     Japan  16610.3770\n799                     Japan  19384.1057\n800                     Japan  22375.9419\n801                     Japan  26824.8951\n802                     Japan  28816.5850\n803                     Japan  28604.5919\n804                     Japan  31656.0681\n805                    Jordan   1546.9078\n806                    Jordan   1886.0806\n807                    Jordan   2348.0092\n808                    Jordan   2741.7963\n809                    Jordan   2110.8563\n810                    Jordan   2852.3516\n811                    Jordan   4161.4160\n812                    Jordan   4448.6799\n813                    Jordan   3431.5936\n814                    Jordan   3645.3796\n815                    Jordan   3844.9172\n816                    Jordan   4519.4612\n817                     Kenya    853.5409\n818                     Kenya    944.4383\n819                     Kenya    896.9664\n820                     Kenya   1056.7365\n821                     Kenya   1222.3600\n822                     Kenya   1267.6132\n823                     Kenya   1348.2258\n824                     Kenya   1361.9369\n825                     Kenya   1341.9217\n826                     Kenya   1360.4850\n827                     Kenya   1287.5147\n828                     Kenya   1463.2493\n829           Korea Dem. Rep.   1088.2778\n830           Korea Dem. Rep.   1571.1347\n831           Korea Dem. Rep.   1621.6936\n832           Korea Dem. Rep.   2143.5406\n833           Korea Dem. Rep.   3701.6215\n834           Korea Dem. Rep.   4106.3012\n835           Korea Dem. Rep.   4106.5253\n836           Korea Dem. Rep.   4106.4923\n837           Korea Dem. Rep.   3726.0635\n838           Korea Dem. Rep.   1690.7568\n839           Korea Dem. Rep.   1646.7582\n840           Korea Dem. Rep.   1593.0655\n841                Korea Rep.   1030.5922\n842                Korea Rep.   1487.5935\n843                Korea Rep.   1536.3444\n844                Korea Rep.   2029.2281\n845                Korea Rep.   3030.8767\n846                Korea Rep.   4657.2210\n847                Korea Rep.   5622.9425\n848                Korea Rep.   8533.0888\n849                Korea Rep.  12104.2787\n850                Korea Rep.  15993.5280\n851                Korea Rep.  19233.9882\n852                Korea Rep.  23348.1397\n853                    Kuwait 108382.3529\n854                    Kuwait 113523.1329\n855                    Kuwait  95458.1118\n856                    Kuwait  80894.8833\n857                    Kuwait 109347.8670\n858                    Kuwait  59265.4771\n859                    Kuwait  31354.0357\n860                    Kuwait  28118.4300\n861                    Kuwait  34932.9196\n862                    Kuwait  40300.6200\n863                    Kuwait  35110.1057\n864                    Kuwait  47306.9898\n865                   Lebanon   4834.8041\n866                   Lebanon   6089.7869\n867                   Lebanon   5714.5606\n868                   Lebanon   6006.9830\n869                   Lebanon   7486.3843\n870                   Lebanon   8659.6968\n871                   Lebanon   7640.5195\n872                   Lebanon   5377.0913\n873                   Lebanon   6890.8069\n874                   Lebanon   8754.9639\n875                   Lebanon   9313.9388\n876                   Lebanon  10461.0587\n877                   Lesotho    298.8462\n878                   Lesotho    335.9971\n879                   Lesotho    411.8006\n880                   Lesotho    498.6390\n881                   Lesotho    496.5816\n882                   Lesotho    745.3695\n883                   Lesotho    797.2631\n884                   Lesotho    773.9932\n885                   Lesotho    977.4863\n886                   Lesotho   1186.1480\n887                   Lesotho   1275.1846\n888                   Lesotho   1569.3314\n889                   Liberia    575.5730\n890                   Liberia    620.9700\n891                   Liberia    634.1952\n892                   Liberia    713.6036\n893                   Liberia    803.0055\n894                   Liberia    640.3224\n895                   Liberia    572.1996\n896                   Liberia    506.1139\n897                   Liberia    636.6229\n898                   Liberia    609.1740\n899                   Liberia    531.4824\n900                   Liberia    414.5073\n901                     Libya   2387.5481\n902                     Libya   3448.2844\n903                     Libya   6757.0308\n904                     Libya  18772.7517\n905                     Libya  21011.4972\n906                     Libya  21951.2118\n907                     Libya  17364.2754\n908                     Libya  11770.5898\n909                     Libya   9640.1385\n910                     Libya   9467.4461\n911                     Libya   9534.6775\n912                     Libya  12057.4993\n913                Madagascar   1443.0117\n914                Madagascar   1589.2027\n915                Madagascar   1643.3871\n916                Madagascar   1634.0473\n917                Madagascar   1748.5630\n918                Madagascar   1544.2286\n919                Madagascar   1302.8787\n920                Madagascar   1155.4419\n921                Madagascar   1040.6762\n922                Madagascar    986.2959\n923                Madagascar    894.6371\n924                Madagascar   1044.7701\n925                    Malawi    369.1651\n926                    Malawi    416.3698\n927                    Malawi    427.9011\n928                    Malawi    495.5148\n929                    Malawi    584.6220\n930                    Malawi    663.2237\n931                    Malawi    632.8039\n932                    Malawi    635.5174\n933                    Malawi    563.2000\n934                    Malawi    692.2758\n935                    Malawi    665.4231\n936                    Malawi    759.3499\n937                  Malaysia   1831.1329\n938                  Malaysia   1810.0670\n939                  Malaysia   2036.8849\n940                  Malaysia   2277.7424\n941                  Malaysia   2849.0948\n942                  Malaysia   3827.9216\n943                  Malaysia   4920.3560\n944                  Malaysia   5249.8027\n945                  Malaysia   7277.9128\n946                  Malaysia  10132.9096\n947                  Malaysia  10206.9779\n948                  Malaysia  12451.6558\n949                      Mali    452.3370\n950                      Mali    490.3822\n951                      Mali    496.1743\n952                      Mali    545.0099\n953                      Mali    581.3689\n954                      Mali    686.3953\n955                      Mali    618.0141\n956                      Mali    684.1716\n957                      Mali    739.0144\n958                      Mali    790.2580\n959                      Mali    951.4098\n960                      Mali   1042.5816\n961                Mauritania    743.1159\n962                Mauritania    846.1203\n963                Mauritania   1055.8960\n964                Mauritania   1421.1452\n965                Mauritania   1586.8518\n966                Mauritania   1497.4922\n967                Mauritania   1481.1502\n968                Mauritania   1421.6036\n969                Mauritania   1361.3698\n970                Mauritania   1483.1361\n971                Mauritania   1579.0195\n972                Mauritania   1803.1515\n973                 Mauritius   1967.9557\n974                 Mauritius   2034.0380\n975                 Mauritius   2529.0675\n976                 Mauritius   2475.3876\n977                 Mauritius   2575.4842\n978                 Mauritius   3710.9830\n979                 Mauritius   3688.0377\n980                 Mauritius   4783.5869\n981                 Mauritius   6058.2538\n982                 Mauritius   7425.7053\n983                 Mauritius   9021.8159\n984                 Mauritius  10956.9911\n985                    Mexico   3478.1255\n986                    Mexico   4131.5466\n987                    Mexico   4581.6094\n988                    Mexico   5754.7339\n989                    Mexico   6809.4067\n990                    Mexico   7674.9291\n991                    Mexico   9611.1475\n992                    Mexico   8688.1560\n993                    Mexico   9472.3843\n994                    Mexico   9767.2975\n995                    Mexico  10742.4405\n996                    Mexico  11977.5750\n997                  Mongolia    786.5669\n998                  Mongolia    912.6626\n999                  Mongolia   1056.3540\n1000                 Mongolia   1226.0411\n1001                 Mongolia   1421.7420\n1002                 Mongolia   1647.5117\n1003                 Mongolia   2000.6031\n1004                 Mongolia   2338.0083\n1005                 Mongolia   1785.4020\n1006                 Mongolia   1902.2521\n1007                 Mongolia   2140.7393\n1008                 Mongolia   3095.7723\n1009               Montenegro   2647.5856\n1010               Montenegro   3682.2599\n1011               Montenegro   4649.5938\n1012               Montenegro   5907.8509\n1013               Montenegro   7778.4140\n1014               Montenegro   9595.9299\n1015               Montenegro  11222.5876\n1016               Montenegro  11732.5102\n1017               Montenegro   7003.3390\n1018               Montenegro   6465.6133\n1019               Montenegro   6557.1943\n1020               Montenegro   9253.8961\n1021                  Morocco   1688.2036\n1022                  Morocco   1642.0023\n1023                  Morocco   1566.3535\n1024                  Morocco   1711.0448\n1025                  Morocco   1930.1950\n1026                  Morocco   2370.6200\n1027                  Morocco   2702.6204\n1028                  Morocco   2755.0470\n1029                  Morocco   2948.0473\n1030                  Morocco   2982.1019\n1031                  Morocco   3258.4956\n1032                  Morocco   3820.1752\n1033               Mozambique    468.5260\n1034               Mozambique    495.5868\n1035               Mozambique    556.6864\n1036               Mozambique    566.6692\n1037               Mozambique    724.9178\n1038               Mozambique    502.3197\n1039               Mozambique    462.2114\n1040               Mozambique    389.8762\n1041               Mozambique    410.8968\n1042               Mozambique    472.3461\n1043               Mozambique    633.6179\n1044               Mozambique    823.6856\n1045                  Myanmar    331.0000\n1046                  Myanmar    350.0000\n1047                  Myanmar    388.0000\n1048                  Myanmar    349.0000\n1049                  Myanmar    357.0000\n1050                  Myanmar    371.0000\n1051                  Myanmar    424.0000\n1052                  Myanmar    385.0000\n1053                  Myanmar    347.0000\n1054                  Myanmar    415.0000\n1055                  Myanmar    611.0000\n1056                  Myanmar    944.0000\n1057                  Namibia   2423.7804\n1058                  Namibia   2621.4481\n1059                  Namibia   3173.2156\n1060                  Namibia   3793.6948\n1061                  Namibia   3746.0809\n1062                  Namibia   3876.4860\n1063                  Namibia   4191.1005\n1064                  Namibia   3693.7313\n1065                  Namibia   3804.5380\n1066                  Namibia   3899.5243\n1067                  Namibia   4072.3248\n1068                  Namibia   4811.0604\n1069                    Nepal    545.8657\n1070                    Nepal    597.9364\n1071                    Nepal    652.3969\n1072                    Nepal    676.4422\n1073                    Nepal    674.7881\n1074                    Nepal    694.1124\n1075                    Nepal    718.3731\n1076                    Nepal    775.6325\n1077                    Nepal    897.7404\n1078                    Nepal   1010.8921\n1079                    Nepal   1057.2063\n1080                    Nepal   1091.3598\n1081              Netherlands   8941.5719\n1082              Netherlands  11276.1934\n1083              Netherlands  12790.8496\n1084              Netherlands  15363.2514\n1085              Netherlands  18794.7457\n1086              Netherlands  21209.0592\n1087              Netherlands  21399.4605\n1088              Netherlands  23651.3236\n1089              Netherlands  26790.9496\n1090              Netherlands  30246.1306\n1091              Netherlands  33724.7578\n1092              Netherlands  36797.9333\n1093              New Zealand  10556.5757\n1094              New Zealand  12247.3953\n1095              New Zealand  13175.6780\n1096              New Zealand  14463.9189\n1097              New Zealand  16046.0373\n1098              New Zealand  16233.7177\n1099              New Zealand  17632.4104\n1100              New Zealand  19007.1913\n1101              New Zealand  18363.3249\n1102              New Zealand  21050.4138\n1103              New Zealand  23189.8014\n1104              New Zealand  25185.0091\n1105                Nicaragua   3112.3639\n1106                Nicaragua   3457.4159\n1107                Nicaragua   3634.3644\n1108                Nicaragua   4643.3935\n1109                Nicaragua   4688.5933\n1110                Nicaragua   5486.3711\n1111                Nicaragua   3470.3382\n1112                Nicaragua   2955.9844\n1113                Nicaragua   2170.1517\n1114                Nicaragua   2253.0230\n1115                Nicaragua   2474.5488\n1116                Nicaragua   2749.3210\n1117                    Niger    761.8794\n1118                    Niger    835.5234\n1119                    Niger    997.7661\n1120                    Niger   1054.3849\n1121                    Niger    954.2092\n1122                    Niger    808.8971\n1123                    Niger    909.7221\n1124                    Niger    668.3000\n1125                    Niger    581.1827\n1126                    Niger    580.3052\n1127                    Niger    601.0745\n1128                    Niger    619.6769\n1129                  Nigeria   1077.2819\n1130                  Nigeria   1100.5926\n1131                  Nigeria   1150.9275\n1132                  Nigeria   1014.5141\n1133                  Nigeria   1698.3888\n1134                  Nigeria   1981.9518\n1135                  Nigeria   1576.9738\n1136                  Nigeria   1385.0296\n1137                  Nigeria   1619.8482\n1138                  Nigeria   1624.9413\n1139                  Nigeria   1615.2864\n1140                  Nigeria   2013.9773\n1141                   Norway  10095.4217\n1142                   Norway  11653.9730\n1143                   Norway  13450.4015\n1144                   Norway  16361.8765\n1145                   Norway  18965.0555\n1146                   Norway  23311.3494\n1147                   Norway  26298.6353\n1148                   Norway  31540.9748\n1149                   Norway  33965.6611\n1150                   Norway  41283.1643\n1151                   Norway  44683.9753\n1152                   Norway  49357.1902\n1153                     Oman   1828.2303\n1154                     Oman   2242.7466\n1155                     Oman   2924.6381\n1156                     Oman   4720.9427\n1157                     Oman  10618.0385\n1158                     Oman  11848.3439\n1159                     Oman  12954.7910\n1160                     Oman  18115.2231\n1161                     Oman  18616.7069\n1162                     Oman  19702.0558\n1163                     Oman  19774.8369\n1164                     Oman  22316.1929\n1165                 Pakistan    684.5971\n1166                 Pakistan    747.0835\n1167                 Pakistan    803.3427\n1168                 Pakistan    942.4083\n1169                 Pakistan   1049.9390\n1170                 Pakistan   1175.9212\n1171                 Pakistan   1443.4298\n1172                 Pakistan   1704.6866\n1173                 Pakistan   1971.8295\n1174                 Pakistan   2049.3505\n1175                 Pakistan   2092.7124\n1176                 Pakistan   2605.9476\n1177                   Panama   2480.3803\n1178                   Panama   2961.8009\n1179                   Panama   3536.5403\n1180                   Panama   4421.0091\n1181                   Panama   5364.2497\n1182                   Panama   5351.9121\n1183                   Panama   7009.6016\n1184                   Panama   7034.7792\n1185                   Panama   6618.7431\n1186                   Panama   7113.6923\n1187                   Panama   7356.0319\n1188                   Panama   9809.1856\n1189                 Paraguay   1952.3087\n1190                 Paraguay   2046.1547\n1191                 Paraguay   2148.0271\n1192                 Paraguay   2299.3763\n1193                 Paraguay   2523.3380\n1194                 Paraguay   3248.3733\n1195                 Paraguay   4258.5036\n1196                 Paraguay   3998.8757\n1197                 Paraguay   4196.4111\n1198                 Paraguay   4247.4003\n1199                 Paraguay   3783.6742\n1200                 Paraguay   4172.8385\n1201                     Peru   3758.5234\n1202                     Peru   4245.2567\n1203                     Peru   4957.0380\n1204                     Peru   5788.0933\n1205                     Peru   5937.8273\n1206                     Peru   6281.2909\n1207                     Peru   6434.5018\n1208                     Peru   6360.9434\n1209                     Peru   4446.3809\n1210                     Peru   5838.3477\n1211                     Peru   5909.0201\n1212                     Peru   7408.9056\n1213              Philippines   1272.8810\n1214              Philippines   1547.9448\n1215              Philippines   1649.5522\n1216              Philippines   1814.1274\n1217              Philippines   1989.3741\n1218              Philippines   2373.2043\n1219              Philippines   2603.2738\n1220              Philippines   2189.6350\n1221              Philippines   2279.3240\n1222              Philippines   2536.5349\n1223              Philippines   2650.9211\n1224              Philippines   3190.4810\n1225                   Poland   4029.3297\n1226                   Poland   4734.2530\n1227                   Poland   5338.7521\n1228                   Poland   6557.1528\n1229                   Poland   8006.5070\n1230                   Poland   9508.1415\n1231                   Poland   8451.5310\n1232                   Poland   9082.3512\n1233                   Poland   7738.8812\n1234                   Poland  10159.5837\n1235                   Poland  12002.2391\n1236                   Poland  15389.9247\n1237                 Portugal   3068.3199\n1238                 Portugal   3774.5717\n1239                 Portugal   4727.9549\n1240                 Portugal   6361.5180\n1241                 Portugal   9022.2474\n1242                 Portugal  10172.4857\n1243                 Portugal  11753.8429\n1244                 Portugal  13039.3088\n1245                 Portugal  16207.2666\n1246                 Portugal  17641.0316\n1247                 Portugal  19970.9079\n1248                 Portugal  20509.6478\n1249              Puerto Rico   3081.9598\n1250              Puerto Rico   3907.1562\n1251              Puerto Rico   5108.3446\n1252              Puerto Rico   6929.2777\n1253              Puerto Rico   9123.0417\n1254              Puerto Rico   9770.5249\n1255              Puerto Rico  10330.9891\n1256              Puerto Rico  12281.3419\n1257              Puerto Rico  14641.5871\n1258              Puerto Rico  16999.4333\n1259              Puerto Rico  18855.6062\n1260              Puerto Rico  19328.7090\n1261                  Reunion   2718.8853\n1262                  Reunion   2769.4518\n1263                  Reunion   3173.7233\n1264                  Reunion   4021.1757\n1265                  Reunion   5047.6586\n1266                  Reunion   4319.8041\n1267                  Reunion   5267.2194\n1268                  Reunion   5303.3775\n1269                  Reunion   6101.2558\n1270                  Reunion   6071.9414\n1271                  Reunion   6316.1652\n1272                  Reunion   7670.1226\n1273                  Romania   3144.6132\n1274                  Romania   3943.3702\n1275                  Romania   4734.9976\n1276                  Romania   6470.8665\n1277                  Romania   8011.4144\n1278                  Romania   9356.3972\n1279                  Romania   9605.3141\n1280                  Romania   9696.2733\n1281                  Romania   6598.4099\n1282                  Romania   7346.5476\n1283                  Romania   7885.3601\n1284                  Romania  10808.4756\n1285                   Rwanda    493.3239\n1286                   Rwanda    540.2894\n1287                   Rwanda    597.4731\n1288                   Rwanda    510.9637\n1289                   Rwanda    590.5807\n1290                   Rwanda    670.0806\n1291                   Rwanda    881.5706\n1292                   Rwanda    847.9912\n1293                   Rwanda    737.0686\n1294                   Rwanda    589.9445\n1295                   Rwanda    785.6538\n1296                   Rwanda    863.0885\n1297    Sao Tome and Principe    879.5836\n1298    Sao Tome and Principe    860.7369\n1299    Sao Tome and Principe   1071.5511\n1300    Sao Tome and Principe   1384.8406\n1301    Sao Tome and Principe   1532.9853\n1302    Sao Tome and Principe   1737.5617\n1303    Sao Tome and Principe   1890.2181\n1304    Sao Tome and Principe   1516.5255\n1305    Sao Tome and Principe   1428.7778\n1306    Sao Tome and Principe   1339.0760\n1307    Sao Tome and Principe   1353.0924\n1308    Sao Tome and Principe   1598.4351\n1309             Saudi Arabia   6459.5548\n1310             Saudi Arabia   8157.5912\n1311             Saudi Arabia  11626.4197\n1312             Saudi Arabia  16903.0489\n1313             Saudi Arabia  24837.4287\n1314             Saudi Arabia  34167.7626\n1315             Saudi Arabia  33693.1753\n1316             Saudi Arabia  21198.2614\n1317             Saudi Arabia  24841.6178\n1318             Saudi Arabia  20586.6902\n1319             Saudi Arabia  19014.5412\n1320             Saudi Arabia  21654.8319\n1321                  Senegal   1450.3570\n1322                  Senegal   1567.6530\n1323                  Senegal   1654.9887\n1324                  Senegal   1612.4046\n1325                  Senegal   1597.7121\n1326                  Senegal   1561.7691\n1327                  Senegal   1518.4800\n1328                  Senegal   1441.7207\n1329                  Senegal   1367.8994\n1330                  Senegal   1392.3683\n1331                  Senegal   1519.6353\n1332                  Senegal   1712.4721\n1333                   Serbia   3581.4594\n1334                   Serbia   4981.0909\n1335                   Serbia   6289.6292\n1336                   Serbia   7991.7071\n1337                   Serbia  10522.0675\n1338                   Serbia  12980.6696\n1339                   Serbia  15181.0927\n1340                   Serbia  15870.8785\n1341                   Serbia   9325.0682\n1342                   Serbia   7914.3203\n1343                   Serbia   7236.0753\n1344                   Serbia   9786.5347\n1345             Sierra Leone    879.7877\n1346             Sierra Leone   1004.4844\n1347             Sierra Leone   1116.6399\n1348             Sierra Leone   1206.0435\n1349             Sierra Leone   1353.7598\n1350             Sierra Leone   1348.2852\n1351             Sierra Leone   1465.0108\n1352             Sierra Leone   1294.4478\n1353             Sierra Leone   1068.6963\n1354             Sierra Leone    574.6482\n1355             Sierra Leone    699.4897\n1356             Sierra Leone    862.5408\n1357                Singapore   2315.1382\n1358                Singapore   2843.1044\n1359                Singapore   3674.7356\n1360                Singapore   4977.4185\n1361                Singapore   8597.7562\n1362                Singapore  11210.0895\n1363                Singapore  15169.1611\n1364                Singapore  18861.5308\n1365                Singapore  24769.8912\n1366                Singapore  33519.4766\n1367                Singapore  36023.1054\n1368                Singapore  47143.1796\n1369          Slovak Republic   5074.6591\n1370          Slovak Republic   6093.2630\n1371          Slovak Republic   7481.1076\n1372          Slovak Republic   8412.9024\n1373          Slovak Republic   9674.1676\n1374          Slovak Republic  10922.6640\n1375          Slovak Republic  11348.5459\n1376          Slovak Republic  12037.2676\n1377          Slovak Republic   9498.4677\n1378          Slovak Republic  12126.2306\n1379          Slovak Republic  13638.7784\n1380          Slovak Republic  18678.3144\n1381                 Slovenia   4215.0417\n1382                 Slovenia   5862.2766\n1383                 Slovenia   7402.3034\n1384                 Slovenia   9405.4894\n1385                 Slovenia  12383.4862\n1386                 Slovenia  15277.0302\n1387                 Slovenia  17866.7218\n1388                 Slovenia  18678.5349\n1389                 Slovenia  14214.7168\n1390                 Slovenia  17161.1073\n1391                 Slovenia  20660.0194\n1392                 Slovenia  25768.2576\n1393                  Somalia   1135.7498\n1394                  Somalia   1258.1474\n1395                  Somalia   1369.4883\n1396                  Somalia   1284.7332\n1397                  Somalia   1254.5761\n1398                  Somalia   1450.9925\n1399                  Somalia   1176.8070\n1400                  Somalia   1093.2450\n1401                  Somalia    926.9603\n1402                  Somalia    930.5964\n1403                  Somalia    882.0818\n1404                  Somalia    926.1411\n1405             South Africa   4725.2955\n1406             South Africa   5487.1042\n1407             South Africa   5768.7297\n1408             South Africa   7114.4780\n1409             South Africa   7765.9626\n1410             South Africa   8028.6514\n1411             South Africa   8568.2662\n1412             South Africa   7825.8234\n1413             South Africa   7225.0693\n1414             South Africa   7479.1882\n1415             South Africa   7710.9464\n1416             South Africa   9269.6578\n1417                    Spain   3834.0347\n1418                    Spain   4564.8024\n1419                    Spain   5693.8439\n1420                    Spain   7993.5123\n1421                    Spain  10638.7513\n1422                    Spain  13236.9212\n1423                    Spain  13926.1700\n1424                    Spain  15764.9831\n1425                    Spain  18603.0645\n1426                    Spain  20445.2990\n1427                    Spain  24835.4717\n1428                    Spain  28821.0637\n1429                Sri Lanka   1083.5320\n1430                Sri Lanka   1072.5466\n1431                Sri Lanka   1074.4720\n1432                Sri Lanka   1135.5143\n1433                Sri Lanka   1213.3955\n1434                Sri Lanka   1348.7757\n1435                Sri Lanka   1648.0798\n1436                Sri Lanka   1876.7668\n1437                Sri Lanka   2153.7392\n1438                Sri Lanka   2664.4773\n1439                Sri Lanka   3015.3788\n1440                Sri Lanka   3970.0954\n1441                    Sudan   1615.9911\n1442                    Sudan   1770.3371\n1443                    Sudan   1959.5938\n1444                    Sudan   1687.9976\n1445                    Sudan   1659.6528\n1446                    Sudan   2202.9884\n1447                    Sudan   1895.5441\n1448                    Sudan   1507.8192\n1449                    Sudan   1492.1970\n1450                    Sudan   1632.2108\n1451                    Sudan   1993.3983\n1452                    Sudan   2602.3950\n1453                Swaziland   1148.3766\n1454                Swaziland   1244.7084\n1455                Swaziland   1856.1821\n1456                Swaziland   2613.1017\n1457                Swaziland   3364.8366\n1458                Swaziland   3781.4106\n1459                Swaziland   3895.3840\n1460                Swaziland   3984.8398\n1461                Swaziland   3553.0224\n1462                Swaziland   3876.7685\n1463                Swaziland   4128.1169\n1464                Swaziland   4513.4806\n1465                   Sweden   8527.8447\n1466                   Sweden   9911.8782\n1467                   Sweden  12329.4419\n1468                   Sweden  15258.2970\n1469                   Sweden  17832.0246\n1470                   Sweden  18855.7252\n1471                   Sweden  20667.3812\n1472                   Sweden  23586.9293\n1473                   Sweden  23880.0168\n1474                   Sweden  25266.5950\n1475                   Sweden  29341.6309\n1476                   Sweden  33859.7484\n1477              Switzerland  14734.2327\n1478              Switzerland  17909.4897\n1479              Switzerland  20431.0927\n1480              Switzerland  22966.1443\n1481              Switzerland  27195.1130\n1482              Switzerland  26982.2905\n1483              Switzerland  28397.7151\n1484              Switzerland  30281.7046\n1485              Switzerland  31871.5303\n1486              Switzerland  32135.3230\n1487              Switzerland  34480.9577\n1488              Switzerland  37506.4191\n1489                    Syria   1643.4854\n1490                    Syria   2117.2349\n1491                    Syria   2193.0371\n1492                    Syria   1881.9236\n1493                    Syria   2571.4230\n1494                    Syria   3195.4846\n1495                    Syria   3761.8377\n1496                    Syria   3116.7743\n1497                    Syria   3340.5428\n1498                    Syria   4014.2390\n1499                    Syria   4090.9253\n1500                    Syria   4184.5481\n1501                   Taiwan   1206.9479\n1502                   Taiwan   1507.8613\n1503                   Taiwan   1822.8790\n1504                   Taiwan   2643.8587\n1505                   Taiwan   4062.5239\n1506                   Taiwan   5596.5198\n1507                   Taiwan   7426.3548\n1508                   Taiwan  11054.5618\n1509                   Taiwan  15215.6579\n1510                   Taiwan  20206.8210\n1511                   Taiwan  23235.4233\n1512                   Taiwan  28718.2768\n1513                 Tanzania    716.6501\n1514                 Tanzania    698.5356\n1515                 Tanzania    722.0038\n1516                 Tanzania    848.2187\n1517                 Tanzania    915.9851\n1518                 Tanzania    962.4923\n1519                 Tanzania    874.2426\n1520                 Tanzania    831.8221\n1521                 Tanzania    825.6825\n1522                 Tanzania    789.1862\n1523                 Tanzania    899.0742\n1524                 Tanzania   1107.4822\n1525                 Thailand    757.7974\n1526                 Thailand    793.5774\n1527                 Thailand   1002.1992\n1528                 Thailand   1295.4607\n1529                 Thailand   1524.3589\n1530                 Thailand   1961.2246\n1531                 Thailand   2393.2198\n1532                 Thailand   2982.6538\n1533                 Thailand   4616.8965\n1534                 Thailand   5852.6255\n1535                 Thailand   5913.1875\n1536                 Thailand   7458.3963\n1537                     Togo    859.8087\n1538                     Togo    925.9083\n1539                     Togo   1067.5348\n1540                     Togo   1477.5968\n1541                     Togo   1649.6602\n1542                     Togo   1532.7770\n1543                     Togo   1344.5780\n1544                     Togo   1202.2014\n1545                     Togo   1034.2989\n1546                     Togo    982.2869\n1547                     Togo    886.2206\n1548                     Togo    882.9699\n1549      Trinidad and Tobago   3023.2719\n1550      Trinidad and Tobago   4100.3934\n1551      Trinidad and Tobago   4997.5240\n1552      Trinidad and Tobago   5621.3685\n1553      Trinidad and Tobago   6619.5514\n1554      Trinidad and Tobago   7899.5542\n1555      Trinidad and Tobago   9119.5286\n1556      Trinidad and Tobago   7388.5978\n1557      Trinidad and Tobago   7370.9909\n1558      Trinidad and Tobago   8792.5731\n1559      Trinidad and Tobago  11460.6002\n1560      Trinidad and Tobago  18008.5092\n1561                  Tunisia   1468.4756\n1562                  Tunisia   1395.2325\n1563                  Tunisia   1660.3032\n1564                  Tunisia   1932.3602\n1565                  Tunisia   2753.2860\n1566                  Tunisia   3120.8768\n1567                  Tunisia   3560.2332\n1568                  Tunisia   3810.4193\n1569                  Tunisia   4332.7202\n1570                  Tunisia   4876.7986\n1571                  Tunisia   5722.8957\n1572                  Tunisia   7092.9230\n1573                   Turkey   1969.1010\n1574                   Turkey   2218.7543\n1575                   Turkey   2322.8699\n1576                   Turkey   2826.3564\n1577                   Turkey   3450.6964\n1578                   Turkey   4269.1223\n1579                   Turkey   4241.3563\n1580                   Turkey   5089.0437\n1581                   Turkey   5678.3483\n1582                   Turkey   6601.4299\n1583                   Turkey   6508.0857\n1584                   Turkey   8458.2764\n1585                   Uganda    734.7535\n1586                   Uganda    774.3711\n1587                   Uganda    767.2717\n1588                   Uganda    908.9185\n1589                   Uganda    950.7359\n1590                   Uganda    843.7331\n1591                   Uganda    682.2662\n1592                   Uganda    617.7244\n1593                   Uganda    644.1708\n1594                   Uganda    816.5591\n1595                   Uganda    927.7210\n1596                   Uganda   1056.3801\n1597           United Kingdom   9979.5085\n1598           United Kingdom  11283.1779\n1599           United Kingdom  12477.1771\n1600           United Kingdom  14142.8509\n1601           United Kingdom  15895.1164\n1602           United Kingdom  17428.7485\n1603           United Kingdom  18232.4245\n1604           United Kingdom  21664.7877\n1605           United Kingdom  22705.0925\n1606           United Kingdom  26074.5314\n1607           United Kingdom  29478.9992\n1608           United Kingdom  33203.2613\n1609            United States  13990.4821\n1610            United States  14847.1271\n1611            United States  16173.1459\n1612            United States  19530.3656\n1613            United States  21806.0359\n1614            United States  24072.6321\n1615            United States  25009.5591\n1616            United States  29884.3504\n1617            United States  32003.9322\n1618            United States  35767.4330\n1619            United States  39097.0995\n1620            United States  42951.6531\n1621                  Uruguay   5716.7667\n1622                  Uruguay   6150.7730\n1623                  Uruguay   5603.3577\n1624                  Uruguay   5444.6196\n1625                  Uruguay   5703.4089\n1626                  Uruguay   6504.3397\n1627                  Uruguay   6920.2231\n1628                  Uruguay   7452.3990\n1629                  Uruguay   8137.0048\n1630                  Uruguay   9230.2407\n1631                  Uruguay   7727.0020\n1632                  Uruguay  10611.4630\n1633                Venezuela   7689.7998\n1634                Venezuela   9802.4665\n1635                Venezuela   8422.9742\n1636                Venezuela   9541.4742\n1637                Venezuela  10505.2597\n1638                Venezuela  13143.9510\n1639                Venezuela  11152.4101\n1640                Venezuela   9883.5846\n1641                Venezuela  10733.9263\n1642                Venezuela  10165.4952\n1643                Venezuela   8605.0478\n1644                Venezuela  11415.8057\n1645                  Vietnam    605.0665\n1646                  Vietnam    676.2854\n1647                  Vietnam    772.0492\n1648                  Vietnam    637.1233\n1649                  Vietnam    699.5016\n1650                  Vietnam    713.5371\n1651                  Vietnam    707.2358\n1652                  Vietnam    820.7994\n1653                  Vietnam    989.0231\n1654                  Vietnam   1385.8968\n1655                  Vietnam   1764.4567\n1656                  Vietnam   2441.5764\n1657       West Bank and Gaza   1515.5923\n1658       West Bank and Gaza   1827.0677\n1659       West Bank and Gaza   2198.9563\n1660       West Bank and Gaza   2649.7150\n1661       West Bank and Gaza   3133.4093\n1662       West Bank and Gaza   3682.8315\n1663       West Bank and Gaza   4336.0321\n1664       West Bank and Gaza   5107.1974\n1665       West Bank and Gaza   6017.6548\n1666       West Bank and Gaza   7110.6676\n1667       West Bank and Gaza   4515.4876\n1668       West Bank and Gaza   3025.3498\n1669               Yemen Rep.    781.7176\n1670               Yemen Rep.    804.8305\n1671               Yemen Rep.    825.6232\n1672               Yemen Rep.    862.4421\n1673               Yemen Rep.   1265.0470\n1674               Yemen Rep.   1829.7652\n1675               Yemen Rep.   1977.5570\n1676               Yemen Rep.   1971.7415\n1677               Yemen Rep.   1879.4967\n1678               Yemen Rep.   2117.4845\n1679               Yemen Rep.   2234.8208\n1680               Yemen Rep.   2280.7699\n1681                   Zambia   1147.3888\n1682                   Zambia   1311.9568\n1683                   Zambia   1452.7258\n1684                   Zambia   1777.0773\n1685                   Zambia   1773.4983\n1686                   Zambia   1588.6883\n1687                   Zambia   1408.6786\n1688                   Zambia   1213.3151\n1689                   Zambia   1210.8846\n1690                   Zambia   1071.3538\n1691                   Zambia   1071.6139\n1692                   Zambia   1271.2116\n1693                 Zimbabwe    406.8841\n1694                 Zimbabwe    518.7643\n1695                 Zimbabwe    527.2722\n1696                 Zimbabwe    569.7951\n1697                 Zimbabwe    799.3622\n1698                 Zimbabwe    685.5877\n1699                 Zimbabwe    788.8550\n1700                 Zimbabwe    706.1573\n1701                 Zimbabwe    693.4208\n1702                 Zimbabwe    792.4500\n1703                 Zimbabwe    672.0386\n1704                 Zimbabwe    469.7093\n\n\nYou can also specify columns to remove using negative selection, select(-varname)\n\ngapminder %&gt;% \n  select(-continent) \n\n                      country year        pop  lifeExp   gdpPercap\n1                 Afghanistan 1952    8425333 28.80100    779.4453\n2                 Afghanistan 1957    9240934 30.33200    820.8530\n3                 Afghanistan 1962   10267083 31.99700    853.1007\n4                 Afghanistan 1967   11537966 34.02000    836.1971\n5                 Afghanistan 1972   13079460 36.08800    739.9811\n6                 Afghanistan 1977   14880372 38.43800    786.1134\n7                 Afghanistan 1982   12881816 39.85400    978.0114\n8                 Afghanistan 1987   13867957 40.82200    852.3959\n9                 Afghanistan 1992   16317921 41.67400    649.3414\n10                Afghanistan 1997   22227415 41.76300    635.3414\n11                Afghanistan 2002   25268405 42.12900    726.7341\n12                Afghanistan 2007   31889923 43.82800    974.5803\n13                    Albania 1952    1282697 55.23000   1601.0561\n14                    Albania 1957    1476505 59.28000   1942.2842\n15                    Albania 1962    1728137 64.82000   2312.8890\n16                    Albania 1967    1984060 66.22000   2760.1969\n17                    Albania 1972    2263554 67.69000   3313.4222\n18                    Albania 1977    2509048 68.93000   3533.0039\n19                    Albania 1982    2780097 70.42000   3630.8807\n20                    Albania 1987    3075321 72.00000   3738.9327\n21                    Albania 1992    3326498 71.58100   2497.4379\n22                    Albania 1997    3428038 72.95000   3193.0546\n23                    Albania 2002    3508512 75.65100   4604.2117\n24                    Albania 2007    3600523 76.42300   5937.0295\n25                    Algeria 1952    9279525 43.07700   2449.0082\n26                    Algeria 1957   10270856 45.68500   3013.9760\n27                    Algeria 1962   11000948 48.30300   2550.8169\n28                    Algeria 1967   12760499 51.40700   3246.9918\n29                    Algeria 1972   14760787 54.51800   4182.6638\n30                    Algeria 1977   17152804 58.01400   4910.4168\n31                    Algeria 1982   20033753 61.36800   5745.1602\n32                    Algeria 1987   23254956 65.79900   5681.3585\n33                    Algeria 1992   26298373 67.74400   5023.2166\n34                    Algeria 1997   29072015 69.15200   4797.2951\n35                    Algeria 2002   31287142 70.99400   5288.0404\n36                    Algeria 2007   33333216 72.30100   6223.3675\n37                     Angola 1952    4232095 30.01500   3520.6103\n38                     Angola 1957    4561361 31.99900   3827.9405\n39                     Angola 1962    4826015 34.00000   4269.2767\n40                     Angola 1967    5247469 35.98500   5522.7764\n41                     Angola 1972    5894858 37.92800   5473.2880\n42                     Angola 1977    6162675 39.48300   3008.6474\n43                     Angola 1982    7016384 39.94200   2756.9537\n44                     Angola 1987    7874230 39.90600   2430.2083\n45                     Angola 1992    8735988 40.64700   2627.8457\n46                     Angola 1997    9875024 40.96300   2277.1409\n47                     Angola 2002   10866106 41.00300   2773.2873\n48                     Angola 2007   12420476 42.73100   4797.2313\n49                  Argentina 1952   17876956 62.48500   5911.3151\n50                  Argentina 1957   19610538 64.39900   6856.8562\n51                  Argentina 1962   21283783 65.14200   7133.1660\n52                  Argentina 1967   22934225 65.63400   8052.9530\n53                  Argentina 1972   24779799 67.06500   9443.0385\n54                  Argentina 1977   26983828 68.48100  10079.0267\n55                  Argentina 1982   29341374 69.94200   8997.8974\n56                  Argentina 1987   31620918 70.77400   9139.6714\n57                  Argentina 1992   33958947 71.86800   9308.4187\n58                  Argentina 1997   36203463 73.27500  10967.2820\n59                  Argentina 2002   38331121 74.34000   8797.6407\n60                  Argentina 2007   40301927 75.32000  12779.3796\n61                  Australia 1952    8691212 69.12000  10039.5956\n62                  Australia 1957    9712569 70.33000  10949.6496\n63                  Australia 1962   10794968 70.93000  12217.2269\n64                  Australia 1967   11872264 71.10000  14526.1246\n65                  Australia 1972   13177000 71.93000  16788.6295\n66                  Australia 1977   14074100 73.49000  18334.1975\n67                  Australia 1982   15184200 74.74000  19477.0093\n68                  Australia 1987   16257249 76.32000  21888.8890\n69                  Australia 1992   17481977 77.56000  23424.7668\n70                  Australia 1997   18565243 78.83000  26997.9366\n71                  Australia 2002   19546792 80.37000  30687.7547\n72                  Australia 2007   20434176 81.23500  34435.3674\n73                    Austria 1952    6927772 66.80000   6137.0765\n74                    Austria 1957    6965860 67.48000   8842.5980\n75                    Austria 1962    7129864 69.54000  10750.7211\n76                    Austria 1967    7376998 70.14000  12834.6024\n77                    Austria 1972    7544201 70.63000  16661.6256\n78                    Austria 1977    7568430 72.17000  19749.4223\n79                    Austria 1982    7574613 73.18000  21597.0836\n80                    Austria 1987    7578903 74.94000  23687.8261\n81                    Austria 1992    7914969 76.04000  27042.0187\n82                    Austria 1997    8069876 77.51000  29095.9207\n83                    Austria 2002    8148312 78.98000  32417.6077\n84                    Austria 2007    8199783 79.82900  36126.4927\n85                    Bahrain 1952     120447 50.93900   9867.0848\n86                    Bahrain 1957     138655 53.83200  11635.7995\n87                    Bahrain 1962     171863 56.92300  12753.2751\n88                    Bahrain 1967     202182 59.92300  14804.6727\n89                    Bahrain 1972     230800 63.30000  18268.6584\n90                    Bahrain 1977     297410 65.59300  19340.1020\n91                    Bahrain 1982     377967 69.05200  19211.1473\n92                    Bahrain 1987     454612 70.75000  18524.0241\n93                    Bahrain 1992     529491 72.60100  19035.5792\n94                    Bahrain 1997     598561 73.92500  20292.0168\n95                    Bahrain 2002     656397 74.79500  23403.5593\n96                    Bahrain 2007     708573 75.63500  29796.0483\n97                 Bangladesh 1952   46886859 37.48400    684.2442\n98                 Bangladesh 1957   51365468 39.34800    661.6375\n99                 Bangladesh 1962   56839289 41.21600    686.3416\n100                Bangladesh 1967   62821884 43.45300    721.1861\n101                Bangladesh 1972   70759295 45.25200    630.2336\n102                Bangladesh 1977   80428306 46.92300    659.8772\n103                Bangladesh 1982   93074406 50.00900    676.9819\n104                Bangladesh 1987  103764241 52.81900    751.9794\n105                Bangladesh 1992  113704579 56.01800    837.8102\n106                Bangladesh 1997  123315288 59.41200    972.7700\n107                Bangladesh 2002  135656790 62.01300   1136.3904\n108                Bangladesh 2007  150448339 64.06200   1391.2538\n109                   Belgium 1952    8730405 68.00000   8343.1051\n110                   Belgium 1957    8989111 69.24000   9714.9606\n111                   Belgium 1962    9218400 70.25000  10991.2068\n112                   Belgium 1967    9556500 70.94000  13149.0412\n113                   Belgium 1972    9709100 71.44000  16672.1436\n114                   Belgium 1977    9821800 72.80000  19117.9745\n115                   Belgium 1982    9856303 73.93000  20979.8459\n116                   Belgium 1987    9870200 75.35000  22525.5631\n117                   Belgium 1992   10045622 76.46000  25575.5707\n118                   Belgium 1997   10199787 77.53000  27561.1966\n119                   Belgium 2002   10311970 78.32000  30485.8838\n120                   Belgium 2007   10392226 79.44100  33692.6051\n121                     Benin 1952    1738315 38.22300   1062.7522\n122                     Benin 1957    1925173 40.35800    959.6011\n123                     Benin 1962    2151895 42.61800    949.4991\n124                     Benin 1967    2427334 44.88500   1035.8314\n125                     Benin 1972    2761407 47.01400   1085.7969\n126                     Benin 1977    3168267 49.19000   1029.1613\n127                     Benin 1982    3641603 50.90400   1277.8976\n128                     Benin 1987    4243788 52.33700   1225.8560\n129                     Benin 1992    4981671 53.91900   1191.2077\n130                     Benin 1997    6066080 54.77700   1232.9753\n131                     Benin 2002    7026113 54.40600   1372.8779\n132                     Benin 2007    8078314 56.72800   1441.2849\n133                   Bolivia 1952    2883315 40.41400   2677.3263\n134                   Bolivia 1957    3211738 41.89000   2127.6863\n135                   Bolivia 1962    3593918 43.42800   2180.9725\n136                   Bolivia 1967    4040665 45.03200   2586.8861\n137                   Bolivia 1972    4565872 46.71400   2980.3313\n138                   Bolivia 1977    5079716 50.02300   3548.0978\n139                   Bolivia 1982    5642224 53.85900   3156.5105\n140                   Bolivia 1987    6156369 57.25100   2753.6915\n141                   Bolivia 1992    6893451 59.95700   2961.6997\n142                   Bolivia 1997    7693188 62.05000   3326.1432\n143                   Bolivia 2002    8445134 63.88300   3413.2627\n144                   Bolivia 2007    9119152 65.55400   3822.1371\n145    Bosnia and Herzegovina 1952    2791000 53.82000    973.5332\n146    Bosnia and Herzegovina 1957    3076000 58.45000   1353.9892\n147    Bosnia and Herzegovina 1962    3349000 61.93000   1709.6837\n148    Bosnia and Herzegovina 1967    3585000 64.79000   2172.3524\n149    Bosnia and Herzegovina 1972    3819000 67.45000   2860.1698\n150    Bosnia and Herzegovina 1977    4086000 69.86000   3528.4813\n151    Bosnia and Herzegovina 1982    4172693 70.69000   4126.6132\n152    Bosnia and Herzegovina 1987    4338977 71.14000   4314.1148\n153    Bosnia and Herzegovina 1992    4256013 72.17800   2546.7814\n154    Bosnia and Herzegovina 1997    3607000 73.24400   4766.3559\n155    Bosnia and Herzegovina 2002    4165416 74.09000   6018.9752\n156    Bosnia and Herzegovina 2007    4552198 74.85200   7446.2988\n157                  Botswana 1952     442308 47.62200    851.2411\n158                  Botswana 1957     474639 49.61800    918.2325\n159                  Botswana 1962     512764 51.52000    983.6540\n160                  Botswana 1967     553541 53.29800   1214.7093\n161                  Botswana 1972     619351 56.02400   2263.6111\n162                  Botswana 1977     781472 59.31900   3214.8578\n163                  Botswana 1982     970347 61.48400   4551.1421\n164                  Botswana 1987    1151184 63.62200   6205.8839\n165                  Botswana 1992    1342614 62.74500   7954.1116\n166                  Botswana 1997    1536536 52.55600   8647.1423\n167                  Botswana 2002    1630347 46.63400  11003.6051\n168                  Botswana 2007    1639131 50.72800  12569.8518\n169                    Brazil 1952   56602560 50.91700   2108.9444\n170                    Brazil 1957   65551171 53.28500   2487.3660\n171                    Brazil 1962   76039390 55.66500   3336.5858\n172                    Brazil 1967   88049823 57.63200   3429.8644\n173                    Brazil 1972  100840058 59.50400   4985.7115\n174                    Brazil 1977  114313951 61.48900   6660.1187\n175                    Brazil 1982  128962939 63.33600   7030.8359\n176                    Brazil 1987  142938076 65.20500   7807.0958\n177                    Brazil 1992  155975974 67.05700   6950.2830\n178                    Brazil 1997  168546719 69.38800   7957.9808\n179                    Brazil 2002  179914212 71.00600   8131.2128\n180                    Brazil 2007  190010647 72.39000   9065.8008\n181                  Bulgaria 1952    7274900 59.60000   2444.2866\n182                  Bulgaria 1957    7651254 66.61000   3008.6707\n183                  Bulgaria 1962    8012946 69.51000   4254.3378\n184                  Bulgaria 1967    8310226 70.42000   5577.0028\n185                  Bulgaria 1972    8576200 70.90000   6597.4944\n186                  Bulgaria 1977    8797022 70.81000   7612.2404\n187                  Bulgaria 1982    8892098 71.08000   8224.1916\n188                  Bulgaria 1987    8971958 71.34000   8239.8548\n189                  Bulgaria 1992    8658506 71.19000   6302.6234\n190                  Bulgaria 1997    8066057 70.32000   5970.3888\n191                  Bulgaria 2002    7661799 72.14000   7696.7777\n192                  Bulgaria 2007    7322858 73.00500  10680.7928\n193              Burkina Faso 1952    4469979 31.97500    543.2552\n194              Burkina Faso 1957    4713416 34.90600    617.1835\n195              Burkina Faso 1962    4919632 37.81400    722.5120\n196              Burkina Faso 1967    5127935 40.69700    794.8266\n197              Burkina Faso 1972    5433886 43.59100    854.7360\n198              Burkina Faso 1977    5889574 46.13700    743.3870\n199              Burkina Faso 1982    6634596 48.12200    807.1986\n200              Burkina Faso 1987    7586551 49.55700    912.0631\n201              Burkina Faso 1992    8878303 50.26000    931.7528\n202              Burkina Faso 1997   10352843 50.32400    946.2950\n203              Burkina Faso 2002   12251209 50.65000   1037.6452\n204              Burkina Faso 2007   14326203 52.29500   1217.0330\n205                   Burundi 1952    2445618 39.03100    339.2965\n206                   Burundi 1957    2667518 40.53300    379.5646\n207                   Burundi 1962    2961915 42.04500    355.2032\n208                   Burundi 1967    3330989 43.54800    412.9775\n209                   Burundi 1972    3529983 44.05700    464.0995\n210                   Burundi 1977    3834415 45.91000    556.1033\n211                   Burundi 1982    4580410 47.47100    559.6032\n212                   Burundi 1987    5126023 48.21100    621.8188\n213                   Burundi 1992    5809236 44.73600    631.6999\n214                   Burundi 1997    6121610 45.32600    463.1151\n215                   Burundi 2002    7021078 47.36000    446.4035\n216                   Burundi 2007    8390505 49.58000    430.0707\n217                  Cambodia 1952    4693836 39.41700    368.4693\n218                  Cambodia 1957    5322536 41.36600    434.0383\n219                  Cambodia 1962    6083619 43.41500    496.9136\n220                  Cambodia 1967    6960067 45.41500    523.4323\n221                  Cambodia 1972    7450606 40.31700    421.6240\n222                  Cambodia 1977    6978607 31.22000    524.9722\n223                  Cambodia 1982    7272485 50.95700    624.4755\n224                  Cambodia 1987    8371791 53.91400    683.8956\n225                  Cambodia 1992   10150094 55.80300    682.3032\n226                  Cambodia 1997   11782962 56.53400    734.2852\n227                  Cambodia 2002   12926707 56.75200    896.2260\n228                  Cambodia 2007   14131858 59.72300   1713.7787\n229                  Cameroon 1952    5009067 38.52300   1172.6677\n230                  Cameroon 1957    5359923 40.42800   1313.0481\n231                  Cameroon 1962    5793633 42.64300   1399.6074\n232                  Cameroon 1967    6335506 44.79900   1508.4531\n233                  Cameroon 1972    7021028 47.04900   1684.1465\n234                  Cameroon 1977    7959865 49.35500   1783.4329\n235                  Cameroon 1982    9250831 52.96100   2367.9833\n236                  Cameroon 1987   10780667 54.98500   2602.6642\n237                  Cameroon 1992   12467171 54.31400   1793.1633\n238                  Cameroon 1997   14195809 52.19900   1694.3375\n239                  Cameroon 2002   15929988 49.85600   1934.0114\n240                  Cameroon 2007   17696293 50.43000   2042.0952\n241                    Canada 1952   14785584 68.75000  11367.1611\n242                    Canada 1957   17010154 69.96000  12489.9501\n243                    Canada 1962   18985849 71.30000  13462.4855\n244                    Canada 1967   20819767 72.13000  16076.5880\n245                    Canada 1972   22284500 72.88000  18970.5709\n246                    Canada 1977   23796400 74.21000  22090.8831\n247                    Canada 1982   25201900 75.76000  22898.7921\n248                    Canada 1987   26549700 76.86000  26626.5150\n249                    Canada 1992   28523502 77.95000  26342.8843\n250                    Canada 1997   30305843 78.61000  28954.9259\n251                    Canada 2002   31902268 79.77000  33328.9651\n252                    Canada 2007   33390141 80.65300  36319.2350\n253  Central African Republic 1952    1291695 35.46300   1071.3107\n254  Central African Republic 1957    1392284 37.46400   1190.8443\n255  Central African Republic 1962    1523478 39.47500   1193.0688\n256  Central African Republic 1967    1733638 41.47800   1136.0566\n257  Central African Republic 1972    1927260 43.45700   1070.0133\n258  Central African Republic 1977    2167533 46.77500   1109.3743\n259  Central African Republic 1982    2476971 48.29500    956.7530\n260  Central African Republic 1987    2840009 50.48500    844.8764\n261  Central African Republic 1992    3265124 49.39600    747.9055\n262  Central African Republic 1997    3696513 46.06600    740.5063\n263  Central African Republic 2002    4048013 43.30800    738.6906\n264  Central African Republic 2007    4369038 44.74100    706.0165\n265                      Chad 1952    2682462 38.09200   1178.6659\n266                      Chad 1957    2894855 39.88100   1308.4956\n267                      Chad 1962    3150417 41.71600   1389.8176\n268                      Chad 1967    3495967 43.60100   1196.8106\n269                      Chad 1972    3899068 45.56900   1104.1040\n270                      Chad 1977    4388260 47.38300   1133.9850\n271                      Chad 1982    4875118 49.51700    797.9081\n272                      Chad 1987    5498955 51.05100    952.3861\n273                      Chad 1992    6429417 51.72400   1058.0643\n274                      Chad 1997    7562011 51.57300   1004.9614\n275                      Chad 2002    8835739 50.52500   1156.1819\n276                      Chad 2007   10238807 50.65100   1704.0637\n277                     Chile 1952    6377619 54.74500   3939.9788\n278                     Chile 1957    7048426 56.07400   4315.6227\n279                     Chile 1962    7961258 57.92400   4519.0943\n280                     Chile 1967    8858908 60.52300   5106.6543\n281                     Chile 1972    9717524 63.44100   5494.0244\n282                     Chile 1977   10599793 67.05200   4756.7638\n283                     Chile 1982   11487112 70.56500   5095.6657\n284                     Chile 1987   12463354 72.49200   5547.0638\n285                     Chile 1992   13572994 74.12600   7596.1260\n286                     Chile 1997   14599929 75.81600  10118.0532\n287                     Chile 2002   15497046 77.86000  10778.7838\n288                     Chile 2007   16284741 78.55300  13171.6388\n289                     China 1952  556263528 44.00000    400.4486\n290                     China 1957  637408000 50.54896    575.9870\n291                     China 1962  665770000 44.50136    487.6740\n292                     China 1967  754550000 58.38112    612.7057\n293                     China 1972  862030000 63.11888    676.9001\n294                     China 1977  943455000 63.96736    741.2375\n295                     China 1982 1000281000 65.52500    962.4214\n296                     China 1987 1084035000 67.27400   1378.9040\n297                     China 1992 1164970000 68.69000   1655.7842\n298                     China 1997 1230075000 70.42600   2289.2341\n299                     China 2002 1280400000 72.02800   3119.2809\n300                     China 2007 1318683096 72.96100   4959.1149\n301                  Colombia 1952   12350771 50.64300   2144.1151\n302                  Colombia 1957   14485993 55.11800   2323.8056\n303                  Colombia 1962   17009885 57.86300   2492.3511\n304                  Colombia 1967   19764027 59.96300   2678.7298\n305                  Colombia 1972   22542890 61.62300   3264.6600\n306                  Colombia 1977   25094412 63.83700   3815.8079\n307                  Colombia 1982   27764644 66.65300   4397.5757\n308                  Colombia 1987   30964245 67.76800   4903.2191\n309                  Colombia 1992   34202721 68.42100   5444.6486\n310                  Colombia 1997   37657830 70.31300   6117.3617\n311                  Colombia 2002   41008227 71.68200   5755.2600\n312                  Colombia 2007   44227550 72.88900   7006.5804\n313                   Comoros 1952     153936 40.71500   1102.9909\n314                   Comoros 1957     170928 42.46000   1211.1485\n315                   Comoros 1962     191689 44.46700   1406.6483\n316                   Comoros 1967     217378 46.47200   1876.0296\n317                   Comoros 1972     250027 48.94400   1937.5777\n318                   Comoros 1977     304739 50.93900   1172.6030\n319                   Comoros 1982     348643 52.93300   1267.1001\n320                   Comoros 1987     395114 54.92600   1315.9808\n321                   Comoros 1992     454429 57.93900   1246.9074\n322                   Comoros 1997     527982 60.66000   1173.6182\n323                   Comoros 2002     614382 62.97400   1075.8116\n324                   Comoros 2007     710960 65.15200    986.1479\n325           Congo Dem. Rep. 1952   14100005 39.14300    780.5423\n326           Congo Dem. Rep. 1957   15577932 40.65200    905.8602\n327           Congo Dem. Rep. 1962   17486434 42.12200    896.3146\n328           Congo Dem. Rep. 1967   19941073 44.05600    861.5932\n329           Congo Dem. Rep. 1972   23007669 45.98900    904.8961\n330           Congo Dem. Rep. 1977   26480870 47.80400    795.7573\n331           Congo Dem. Rep. 1982   30646495 47.78400    673.7478\n332           Congo Dem. Rep. 1987   35481645 47.41200    672.7748\n333           Congo Dem. Rep. 1992   41672143 45.54800    457.7192\n334           Congo Dem. Rep. 1997   47798986 42.58700    312.1884\n335           Congo Dem. Rep. 2002   55379852 44.96600    241.1659\n336           Congo Dem. Rep. 2007   64606759 46.46200    277.5519\n337                Congo Rep. 1952     854885 42.11100   2125.6214\n338                Congo Rep. 1957     940458 45.05300   2315.0566\n339                Congo Rep. 1962    1047924 48.43500   2464.7832\n340                Congo Rep. 1967    1179760 52.04000   2677.9396\n341                Congo Rep. 1972    1340458 54.90700   3213.1527\n342                Congo Rep. 1977    1536769 55.62500   3259.1790\n343                Congo Rep. 1982    1774735 56.69500   4879.5075\n344                Congo Rep. 1987    2064095 57.47000   4201.1949\n345                Congo Rep. 1992    2409073 56.43300   4016.2395\n346                Congo Rep. 1997    2800947 52.96200   3484.1644\n347                Congo Rep. 2002    3328795 52.97000   3484.0620\n348                Congo Rep. 2007    3800610 55.32200   3632.5578\n349                Costa Rica 1952     926317 57.20600   2627.0095\n350                Costa Rica 1957    1112300 60.02600   2990.0108\n351                Costa Rica 1962    1345187 62.84200   3460.9370\n352                Costa Rica 1967    1588717 65.42400   4161.7278\n353                Costa Rica 1972    1834796 67.84900   5118.1469\n354                Costa Rica 1977    2108457 70.75000   5926.8770\n355                Costa Rica 1982    2424367 73.45000   5262.7348\n356                Costa Rica 1987    2799811 74.75200   5629.9153\n357                Costa Rica 1992    3173216 75.71300   6160.4163\n358                Costa Rica 1997    3518107 77.26000   6677.0453\n359                Costa Rica 2002    3834934 78.12300   7723.4472\n360                Costa Rica 2007    4133884 78.78200   9645.0614\n361             Cote d'Ivoire 1952    2977019 40.47700   1388.5947\n362             Cote d'Ivoire 1957    3300000 42.46900   1500.8959\n363             Cote d'Ivoire 1962    3832408 44.93000   1728.8694\n364             Cote d'Ivoire 1967    4744870 47.35000   2052.0505\n365             Cote d'Ivoire 1972    6071696 49.80100   2378.2011\n366             Cote d'Ivoire 1977    7459574 52.37400   2517.7365\n367             Cote d'Ivoire 1982    9025951 53.98300   2602.7102\n368             Cote d'Ivoire 1987   10761098 54.65500   2156.9561\n369             Cote d'Ivoire 1992   12772596 52.04400   1648.0738\n370             Cote d'Ivoire 1997   14625967 47.99100   1786.2654\n371             Cote d'Ivoire 2002   16252726 46.83200   1648.8008\n372             Cote d'Ivoire 2007   18013409 48.32800   1544.7501\n373                   Croatia 1952    3882229 61.21000   3119.2365\n374                   Croatia 1957    3991242 64.77000   4338.2316\n375                   Croatia 1962    4076557 67.13000   5477.8900\n376                   Croatia 1967    4174366 68.50000   6960.2979\n377                   Croatia 1972    4225310 69.61000   9164.0901\n378                   Croatia 1977    4318673 70.64000  11305.3852\n379                   Croatia 1982    4413368 70.46000  13221.8218\n380                   Croatia 1987    4484310 71.52000  13822.5839\n381                   Croatia 1992    4494013 72.52700   8447.7949\n382                   Croatia 1997    4444595 73.68000   9875.6045\n383                   Croatia 2002    4481020 74.87600  11628.3890\n384                   Croatia 2007    4493312 75.74800  14619.2227\n385                      Cuba 1952    6007797 59.42100   5586.5388\n386                      Cuba 1957    6640752 62.32500   6092.1744\n387                      Cuba 1962    7254373 65.24600   5180.7559\n388                      Cuba 1967    8139332 68.29000   5690.2680\n389                      Cuba 1972    8831348 70.72300   5305.4453\n390                      Cuba 1977    9537988 72.64900   6380.4950\n391                      Cuba 1982    9789224 73.71700   7316.9181\n392                      Cuba 1987   10239839 74.17400   7532.9248\n393                      Cuba 1992   10723260 74.41400   5592.8440\n394                      Cuba 1997   10983007 76.15100   5431.9904\n395                      Cuba 2002   11226999 77.15800   6340.6467\n396                      Cuba 2007   11416987 78.27300   8948.1029\n397            Czech Republic 1952    9125183 66.87000   6876.1403\n398            Czech Republic 1957    9513758 69.03000   8256.3439\n399            Czech Republic 1962    9620282 69.90000  10136.8671\n400            Czech Republic 1967    9835109 70.38000  11399.4449\n401            Czech Republic 1972    9862158 70.29000  13108.4536\n402            Czech Republic 1977   10161915 70.71000  14800.1606\n403            Czech Republic 1982   10303704 70.96000  15377.2285\n404            Czech Republic 1987   10311597 71.58000  16310.4434\n405            Czech Republic 1992   10315702 72.40000  14297.0212\n406            Czech Republic 1997   10300707 74.01000  16048.5142\n407            Czech Republic 2002   10256295 75.51000  17596.2102\n408            Czech Republic 2007   10228744 76.48600  22833.3085\n409                   Denmark 1952    4334000 70.78000   9692.3852\n410                   Denmark 1957    4487831 71.81000  11099.6593\n411                   Denmark 1962    4646899 72.35000  13583.3135\n412                   Denmark 1967    4838800 72.96000  15937.2112\n413                   Denmark 1972    4991596 73.47000  18866.2072\n414                   Denmark 1977    5088419 74.69000  20422.9015\n415                   Denmark 1982    5117810 74.63000  21688.0405\n416                   Denmark 1987    5127024 74.80000  25116.1758\n417                   Denmark 1992    5171393 75.33000  26406.7399\n418                   Denmark 1997    5283663 76.11000  29804.3457\n419                   Denmark 2002    5374693 77.18000  32166.5001\n420                   Denmark 2007    5468120 78.33200  35278.4187\n421                  Djibouti 1952      63149 34.81200   2669.5295\n422                  Djibouti 1957      71851 37.32800   2864.9691\n423                  Djibouti 1962      89898 39.69300   3020.9893\n424                  Djibouti 1967     127617 42.07400   3020.0505\n425                  Djibouti 1972     178848 44.36600   3694.2124\n426                  Djibouti 1977     228694 46.51900   3081.7610\n427                  Djibouti 1982     305991 48.81200   2879.4681\n428                  Djibouti 1987     311025 50.04000   2880.1026\n429                  Djibouti 1992     384156 51.60400   2377.1562\n430                  Djibouti 1997     417908 53.15700   1895.0170\n431                  Djibouti 2002     447416 53.37300   1908.2609\n432                  Djibouti 2007     496374 54.79100   2082.4816\n433        Dominican Republic 1952    2491346 45.92800   1397.7171\n434        Dominican Republic 1957    2923186 49.82800   1544.4030\n435        Dominican Republic 1962    3453434 53.45900   1662.1374\n436        Dominican Republic 1967    4049146 56.75100   1653.7230\n437        Dominican Republic 1972    4671329 59.63100   2189.8745\n438        Dominican Republic 1977    5302800 61.78800   2681.9889\n439        Dominican Republic 1982    5968349 63.72700   2861.0924\n440        Dominican Republic 1987    6655297 66.04600   2899.8422\n441        Dominican Republic 1992    7351181 68.45700   3044.2142\n442        Dominican Republic 1997    7992357 69.95700   3614.1013\n443        Dominican Republic 2002    8650322 70.84700   4563.8082\n444        Dominican Republic 2007    9319622 72.23500   6025.3748\n445                   Ecuador 1952    3548753 48.35700   3522.1107\n446                   Ecuador 1957    4058385 51.35600   3780.5467\n447                   Ecuador 1962    4681707 54.64000   4086.1141\n448                   Ecuador 1967    5432424 56.67800   4579.0742\n449                   Ecuador 1972    6298651 58.79600   5280.9947\n450                   Ecuador 1977    7278866 61.31000   6679.6233\n451                   Ecuador 1982    8365850 64.34200   7213.7913\n452                   Ecuador 1987    9545158 67.23100   6481.7770\n453                   Ecuador 1992   10748394 69.61300   7103.7026\n454                   Ecuador 1997   11911819 72.31200   7429.4559\n455                   Ecuador 2002   12921234 74.17300   5773.0445\n456                   Ecuador 2007   13755680 74.99400   6873.2623\n457                     Egypt 1952   22223309 41.89300   1418.8224\n458                     Egypt 1957   25009741 44.44400   1458.9153\n459                     Egypt 1962   28173309 46.99200   1693.3359\n460                     Egypt 1967   31681188 49.29300   1814.8807\n461                     Egypt 1972   34807417 51.13700   2024.0081\n462                     Egypt 1977   38783863 53.31900   2785.4936\n463                     Egypt 1982   45681811 56.00600   3503.7296\n464                     Egypt 1987   52799062 59.79700   3885.4607\n465                     Egypt 1992   59402198 63.67400   3794.7552\n466                     Egypt 1997   66134291 67.21700   4173.1818\n467                     Egypt 2002   73312559 69.80600   4754.6044\n468                     Egypt 2007   80264543 71.33800   5581.1810\n469               El Salvador 1952    2042865 45.26200   3048.3029\n470               El Salvador 1957    2355805 48.57000   3421.5232\n471               El Salvador 1962    2747687 52.30700   3776.8036\n472               El Salvador 1967    3232927 55.85500   4358.5954\n473               El Salvador 1972    3790903 58.20700   4520.2460\n474               El Salvador 1977    4282586 56.69600   5138.9224\n475               El Salvador 1982    4474873 56.60400   4098.3442\n476               El Salvador 1987    4842194 63.15400   4140.4421\n477               El Salvador 1992    5274649 66.79800   4444.2317\n478               El Salvador 1997    5783439 69.53500   5154.8255\n479               El Salvador 2002    6353681 70.73400   5351.5687\n480               El Salvador 2007    6939688 71.87800   5728.3535\n481         Equatorial Guinea 1952     216964 34.48200    375.6431\n482         Equatorial Guinea 1957     232922 35.98300    426.0964\n483         Equatorial Guinea 1962     249220 37.48500    582.8420\n484         Equatorial Guinea 1967     259864 38.98700    915.5960\n485         Equatorial Guinea 1972     277603 40.51600    672.4123\n486         Equatorial Guinea 1977     192675 42.02400    958.5668\n487         Equatorial Guinea 1982     285483 43.66200    927.8253\n488         Equatorial Guinea 1987     341244 45.66400    966.8968\n489         Equatorial Guinea 1992     387838 47.54500   1132.0550\n490         Equatorial Guinea 1997     439971 48.24500   2814.4808\n491         Equatorial Guinea 2002     495627 49.34800   7703.4959\n492         Equatorial Guinea 2007     551201 51.57900  12154.0897\n493                   Eritrea 1952    1438760 35.92800    328.9406\n494                   Eritrea 1957    1542611 38.04700    344.1619\n495                   Eritrea 1962    1666618 40.15800    380.9958\n496                   Eritrea 1967    1820319 42.18900    468.7950\n497                   Eritrea 1972    2260187 44.14200    514.3242\n498                   Eritrea 1977    2512642 44.53500    505.7538\n499                   Eritrea 1982    2637297 43.89000    524.8758\n500                   Eritrea 1987    2915959 46.45300    521.1341\n501                   Eritrea 1992    3668440 49.99100    582.8585\n502                   Eritrea 1997    4058319 53.37800    913.4708\n503                   Eritrea 2002    4414865 55.24000    765.3500\n504                   Eritrea 2007    4906585 58.04000    641.3695\n505                  Ethiopia 1952   20860941 34.07800    362.1463\n506                  Ethiopia 1957   22815614 36.66700    378.9042\n507                  Ethiopia 1962   25145372 40.05900    419.4564\n508                  Ethiopia 1967   27860297 42.11500    516.1186\n509                  Ethiopia 1972   30770372 43.51500    566.2439\n510                  Ethiopia 1977   34617799 44.51000    556.8084\n511                  Ethiopia 1982   38111756 44.91600    577.8607\n512                  Ethiopia 1987   42999530 46.68400    573.7413\n513                  Ethiopia 1992   52088559 48.09100    421.3535\n514                  Ethiopia 1997   59861301 49.40200    515.8894\n515                  Ethiopia 2002   67946797 50.72500    530.0535\n516                  Ethiopia 2007   76511887 52.94700    690.8056\n517                   Finland 1952    4090500 66.55000   6424.5191\n518                   Finland 1957    4324000 67.49000   7545.4154\n519                   Finland 1962    4491443 68.75000   9371.8426\n520                   Finland 1967    4605744 69.83000  10921.6363\n521                   Finland 1972    4639657 70.87000  14358.8759\n522                   Finland 1977    4738902 72.52000  15605.4228\n523                   Finland 1982    4826933 74.55000  18533.1576\n524                   Finland 1987    4931729 74.83000  21141.0122\n525                   Finland 1992    5041039 75.70000  20647.1650\n526                   Finland 1997    5134406 77.13000  23723.9502\n527                   Finland 2002    5193039 78.37000  28204.5906\n528                   Finland 2007    5238460 79.31300  33207.0844\n529                    France 1952   42459667 67.41000   7029.8093\n530                    France 1957   44310863 68.93000   8662.8349\n531                    France 1962   47124000 70.51000  10560.4855\n532                    France 1967   49569000 71.55000  12999.9177\n533                    France 1972   51732000 72.38000  16107.1917\n534                    France 1977   53165019 73.83000  18292.6351\n535                    France 1982   54433565 74.89000  20293.8975\n536                    France 1987   55630100 76.34000  22066.4421\n537                    France 1992   57374179 77.46000  24703.7961\n538                    France 1997   58623428 78.64000  25889.7849\n539                    France 2002   59925035 79.59000  28926.0323\n540                    France 2007   61083916 80.65700  30470.0167\n541                     Gabon 1952     420702 37.00300   4293.4765\n542                     Gabon 1957     434904 38.99900   4976.1981\n543                     Gabon 1962     455661 40.48900   6631.4592\n544                     Gabon 1967     489004 44.59800   8358.7620\n545                     Gabon 1972     537977 48.69000  11401.9484\n546                     Gabon 1977     706367 52.79000  21745.5733\n547                     Gabon 1982     753874 56.56400  15113.3619\n548                     Gabon 1987     880397 60.19000  11864.4084\n549                     Gabon 1992     985739 61.36600  13522.1575\n550                     Gabon 1997    1126189 60.46100  14722.8419\n551                     Gabon 2002    1299304 56.76100  12521.7139\n552                     Gabon 2007    1454867 56.73500  13206.4845\n553                    Gambia 1952     284320 30.00000    485.2307\n554                    Gambia 1957     323150 32.06500    520.9267\n555                    Gambia 1962     374020 33.89600    599.6503\n556                    Gambia 1967     439593 35.85700    734.7829\n557                    Gambia 1972     517101 38.30800    756.0868\n558                    Gambia 1977     608274 41.84200    884.7553\n559                    Gambia 1982     715523 45.58000    835.8096\n560                    Gambia 1987     848406 49.26500    611.6589\n561                    Gambia 1992    1025384 52.64400    665.6244\n562                    Gambia 1997    1235767 55.86100    653.7302\n563                    Gambia 2002    1457766 58.04100    660.5856\n564                    Gambia 2007    1688359 59.44800    752.7497\n565                   Germany 1952   69145952 67.50000   7144.1144\n566                   Germany 1957   71019069 69.10000  10187.8267\n567                   Germany 1962   73739117 70.30000  12902.4629\n568                   Germany 1967   76368453 70.80000  14745.6256\n569                   Germany 1972   78717088 71.00000  18016.1803\n570                   Germany 1977   78160773 72.50000  20512.9212\n571                   Germany 1982   78335266 73.80000  22031.5327\n572                   Germany 1987   77718298 74.84700  24639.1857\n573                   Germany 1992   80597764 76.07000  26505.3032\n574                   Germany 1997   82011073 77.34000  27788.8842\n575                   Germany 2002   82350671 78.67000  30035.8020\n576                   Germany 2007   82400996 79.40600  32170.3744\n577                     Ghana 1952    5581001 43.14900    911.2989\n578                     Ghana 1957    6391288 44.77900   1043.5615\n579                     Ghana 1962    7355248 46.45200   1190.0411\n580                     Ghana 1967    8490213 48.07200   1125.6972\n581                     Ghana 1972    9354120 49.87500   1178.2237\n582                     Ghana 1977   10538093 51.75600    993.2240\n583                     Ghana 1982   11400338 53.74400    876.0326\n584                     Ghana 1987   14168101 55.72900    847.0061\n585                     Ghana 1992   16278738 57.50100    925.0602\n586                     Ghana 1997   18418288 58.55600   1005.2458\n587                     Ghana 2002   20550751 58.45300   1111.9846\n588                     Ghana 2007   22873338 60.02200   1327.6089\n589                    Greece 1952    7733250 65.86000   3530.6901\n590                    Greece 1957    8096218 67.86000   4916.2999\n591                    Greece 1962    8448233 69.51000   6017.1907\n592                    Greece 1967    8716441 71.00000   8513.0970\n593                    Greece 1972    8888628 72.34000  12724.8296\n594                    Greece 1977    9308479 73.68000  14195.5243\n595                    Greece 1982    9786480 75.24000  15268.4209\n596                    Greece 1987    9974490 76.67000  16120.5284\n597                    Greece 1992   10325429 77.03000  17541.4963\n598                    Greece 1997   10502372 77.86900  18747.6981\n599                    Greece 2002   10603863 78.25600  22514.2548\n600                    Greece 2007   10706290 79.48300  27538.4119\n601                 Guatemala 1952    3146381 42.02300   2428.2378\n602                 Guatemala 1957    3640876 44.14200   2617.1560\n603                 Guatemala 1962    4208858 46.95400   2750.3644\n604                 Guatemala 1967    4690773 50.01600   3242.5311\n605                 Guatemala 1972    5149581 53.73800   4031.4083\n606                 Guatemala 1977    5703430 56.02900   4879.9927\n607                 Guatemala 1982    6395630 58.13700   4820.4948\n608                 Guatemala 1987    7326406 60.78200   4246.4860\n609                 Guatemala 1992    8486949 63.37300   4439.4508\n610                 Guatemala 1997    9803875 66.32200   4684.3138\n611                 Guatemala 2002   11178650 68.97800   4858.3475\n612                 Guatemala 2007   12572928 70.25900   5186.0500\n613                    Guinea 1952    2664249 33.60900    510.1965\n614                    Guinea 1957    2876726 34.55800    576.2670\n615                    Guinea 1962    3140003 35.75300    686.3737\n616                    Guinea 1967    3451418 37.19700    708.7595\n617                    Guinea 1972    3811387 38.84200    741.6662\n618                    Guinea 1977    4227026 40.76200    874.6859\n619                    Guinea 1982    4710497 42.89100    857.2504\n620                    Guinea 1987    5650262 45.55200    805.5725\n621                    Guinea 1992    6990574 48.57600    794.3484\n622                    Guinea 1997    8048834 51.45500    869.4498\n623                    Guinea 2002    8807818 53.67600    945.5836\n624                    Guinea 2007    9947814 56.00700    942.6542\n625             Guinea-Bissau 1952     580653 32.50000    299.8503\n626             Guinea-Bissau 1957     601095 33.48900    431.7905\n627             Guinea-Bissau 1962     627820 34.48800    522.0344\n628             Guinea-Bissau 1967     601287 35.49200    715.5806\n629             Guinea-Bissau 1972     625361 36.48600    820.2246\n630             Guinea-Bissau 1977     745228 37.46500    764.7260\n631             Guinea-Bissau 1982     825987 39.32700    838.1240\n632             Guinea-Bissau 1987     927524 41.24500    736.4154\n633             Guinea-Bissau 1992    1050938 43.26600    745.5399\n634             Guinea-Bissau 1997    1193708 44.87300    796.6645\n635             Guinea-Bissau 2002    1332459 45.50400    575.7047\n636             Guinea-Bissau 2007    1472041 46.38800    579.2317\n637                     Haiti 1952    3201488 37.57900   1840.3669\n638                     Haiti 1957    3507701 40.69600   1726.8879\n639                     Haiti 1962    3880130 43.59000   1796.5890\n640                     Haiti 1967    4318137 46.24300   1452.0577\n641                     Haiti 1972    4698301 48.04200   1654.4569\n642                     Haiti 1977    4908554 49.92300   1874.2989\n643                     Haiti 1982    5198399 51.46100   2011.1595\n644                     Haiti 1987    5756203 53.63600   1823.0160\n645                     Haiti 1992    6326682 55.08900   1456.3095\n646                     Haiti 1997    6913545 56.67100   1341.7269\n647                     Haiti 2002    7607651 58.13700   1270.3649\n648                     Haiti 2007    8502814 60.91600   1201.6372\n649                  Honduras 1952    1517453 41.91200   2194.9262\n650                  Honduras 1957    1770390 44.66500   2220.4877\n651                  Honduras 1962    2090162 48.04100   2291.1568\n652                  Honduras 1967    2500689 50.92400   2538.2694\n653                  Honduras 1972    2965146 53.88400   2529.8423\n654                  Honduras 1977    3055235 57.40200   3203.2081\n655                  Honduras 1982    3669448 60.90900   3121.7608\n656                  Honduras 1987    4372203 64.49200   3023.0967\n657                  Honduras 1992    5077347 66.39900   3081.6946\n658                  Honduras 1997    5867957 67.65900   3160.4549\n659                  Honduras 2002    6677328 68.56500   3099.7287\n660                  Honduras 2007    7483763 70.19800   3548.3308\n661           Hong Kong China 1952    2125900 60.96000   3054.4212\n662           Hong Kong China 1957    2736300 64.75000   3629.0765\n663           Hong Kong China 1962    3305200 67.65000   4692.6483\n664           Hong Kong China 1967    3722800 70.00000   6197.9628\n665           Hong Kong China 1972    4115700 72.00000   8315.9281\n666           Hong Kong China 1977    4583700 73.60000  11186.1413\n667           Hong Kong China 1982    5264500 75.45000  14560.5305\n668           Hong Kong China 1987    5584510 76.20000  20038.4727\n669           Hong Kong China 1992    5829696 77.60100  24757.6030\n670           Hong Kong China 1997    6495918 80.00000  28377.6322\n671           Hong Kong China 2002    6762476 81.49500  30209.0152\n672           Hong Kong China 2007    6980412 82.20800  39724.9787\n673                   Hungary 1952    9504000 64.03000   5263.6738\n674                   Hungary 1957    9839000 66.41000   6040.1800\n675                   Hungary 1962   10063000 67.96000   7550.3599\n676                   Hungary 1967   10223422 69.50000   9326.6447\n677                   Hungary 1972   10394091 69.76000  10168.6561\n678                   Hungary 1977   10637171 69.95000  11674.8374\n679                   Hungary 1982   10705535 69.39000  12545.9907\n680                   Hungary 1987   10612740 69.58000  12986.4800\n681                   Hungary 1992   10348684 69.17000  10535.6285\n682                   Hungary 1997   10244684 71.04000  11712.7768\n683                   Hungary 2002   10083313 72.59000  14843.9356\n684                   Hungary 2007    9956108 73.33800  18008.9444\n685                   Iceland 1952     147962 72.49000   7267.6884\n686                   Iceland 1957     165110 73.47000   9244.0014\n687                   Iceland 1962     182053 73.68000  10350.1591\n688                   Iceland 1967     198676 73.73000  13319.8957\n689                   Iceland 1972     209275 74.46000  15798.0636\n690                   Iceland 1977     221823 76.11000  19654.9625\n691                   Iceland 1982     233997 76.99000  23269.6075\n692                   Iceland 1987     244676 77.23000  26923.2063\n693                   Iceland 1992     259012 78.77000  25144.3920\n694                   Iceland 1997     271192 78.95000  28061.0997\n695                   Iceland 2002     288030 80.50000  31163.2020\n696                   Iceland 2007     301931 81.75700  36180.7892\n697                     India 1952  372000000 37.37300    546.5657\n698                     India 1957  409000000 40.24900    590.0620\n699                     India 1962  454000000 43.60500    658.3472\n700                     India 1967  506000000 47.19300    700.7706\n701                     India 1972  567000000 50.65100    724.0325\n702                     India 1977  634000000 54.20800    813.3373\n703                     India 1982  708000000 56.59600    855.7235\n704                     India 1987  788000000 58.55300    976.5127\n705                     India 1992  872000000 60.22300   1164.4068\n706                     India 1997  959000000 61.76500   1458.8174\n707                     India 2002 1034172547 62.87900   1746.7695\n708                     India 2007 1110396331 64.69800   2452.2104\n709                 Indonesia 1952   82052000 37.46800    749.6817\n710                 Indonesia 1957   90124000 39.91800    858.9003\n711                 Indonesia 1962   99028000 42.51800    849.2898\n712                 Indonesia 1967  109343000 45.96400    762.4318\n713                 Indonesia 1972  121282000 49.20300   1111.1079\n714                 Indonesia 1977  136725000 52.70200   1382.7021\n715                 Indonesia 1982  153343000 56.15900   1516.8730\n716                 Indonesia 1987  169276000 60.13700   1748.3570\n717                 Indonesia 1992  184816000 62.68100   2383.1409\n718                 Indonesia 1997  199278000 66.04100   3119.3356\n719                 Indonesia 2002  211060000 68.58800   2873.9129\n720                 Indonesia 2007  223547000 70.65000   3540.6516\n721                      Iran 1952   17272000 44.86900   3035.3260\n722                      Iran 1957   19792000 47.18100   3290.2576\n723                      Iran 1962   22874000 49.32500   4187.3298\n724                      Iran 1967   26538000 52.46900   5906.7318\n725                      Iran 1972   30614000 55.23400   9613.8186\n726                      Iran 1977   35480679 57.70200  11888.5951\n727                      Iran 1982   43072751 59.62000   7608.3346\n728                      Iran 1987   51889696 63.04000   6642.8814\n729                      Iran 1992   60397973 65.74200   7235.6532\n730                      Iran 1997   63327987 68.04200   8263.5903\n731                      Iran 2002   66907826 69.45100   9240.7620\n732                      Iran 2007   69453570 70.96400  11605.7145\n733                      Iraq 1952    5441766 45.32000   4129.7661\n734                      Iraq 1957    6248643 48.43700   6229.3336\n735                      Iraq 1962    7240260 51.45700   8341.7378\n736                      Iraq 1967    8519282 54.45900   8931.4598\n737                      Iraq 1972   10061506 56.95000   9576.0376\n738                      Iraq 1977   11882916 60.41300  14688.2351\n739                      Iraq 1982   14173318 62.03800  14517.9071\n740                      Iraq 1987   16543189 65.04400  11643.5727\n741                      Iraq 1992   17861905 59.46100   3745.6407\n742                      Iraq 1997   20775703 58.81100   3076.2398\n743                      Iraq 2002   24001816 57.04600   4390.7173\n744                      Iraq 2007   27499638 59.54500   4471.0619\n745                   Ireland 1952    2952156 66.91000   5210.2803\n746                   Ireland 1957    2878220 68.90000   5599.0779\n747                   Ireland 1962    2830000 70.29000   6631.5973\n748                   Ireland 1967    2900100 71.08000   7655.5690\n749                   Ireland 1972    3024400 71.28000   9530.7729\n750                   Ireland 1977    3271900 72.03000  11150.9811\n751                   Ireland 1982    3480000 73.10000  12618.3214\n752                   Ireland 1987    3539900 74.36000  13872.8665\n753                   Ireland 1992    3557761 75.46700  17558.8155\n754                   Ireland 1997    3667233 76.12200  24521.9471\n755                   Ireland 2002    3879155 77.78300  34077.0494\n756                   Ireland 2007    4109086 78.88500  40675.9964\n757                    Israel 1952    1620914 65.39000   4086.5221\n758                    Israel 1957    1944401 67.84000   5385.2785\n759                    Israel 1962    2310904 69.39000   7105.6307\n760                    Israel 1967    2693585 70.75000   8393.7414\n761                    Israel 1972    3095893 71.63000  12786.9322\n762                    Israel 1977    3495918 73.06000  13306.6192\n763                    Israel 1982    3858421 74.45000  15367.0292\n764                    Israel 1987    4203148 75.60000  17122.4799\n765                    Israel 1992    4936550 76.93000  18051.5225\n766                    Israel 1997    5531387 78.26900  20896.6092\n767                    Israel 2002    6029529 79.69600  21905.5951\n768                    Israel 2007    6426679 80.74500  25523.2771\n769                     Italy 1952   47666000 65.94000   4931.4042\n770                     Italy 1957   49182000 67.81000   6248.6562\n771                     Italy 1962   50843200 69.24000   8243.5823\n772                     Italy 1967   52667100 71.06000  10022.4013\n773                     Italy 1972   54365564 72.19000  12269.2738\n774                     Italy 1977   56059245 73.48000  14255.9847\n775                     Italy 1982   56535636 74.98000  16537.4835\n776                     Italy 1987   56729703 76.42000  19207.2348\n777                     Italy 1992   56840847 77.44000  22013.6449\n778                     Italy 1997   57479469 78.82000  24675.0245\n779                     Italy 2002   57926999 80.24000  27968.0982\n780                     Italy 2007   58147733 80.54600  28569.7197\n781                   Jamaica 1952    1426095 58.53000   2898.5309\n782                   Jamaica 1957    1535090 62.61000   4756.5258\n783                   Jamaica 1962    1665128 65.61000   5246.1075\n784                   Jamaica 1967    1861096 67.51000   6124.7035\n785                   Jamaica 1972    1997616 69.00000   7433.8893\n786                   Jamaica 1977    2156814 70.11000   6650.1956\n787                   Jamaica 1982    2298309 71.21000   6068.0513\n788                   Jamaica 1987    2326606 71.77000   6351.2375\n789                   Jamaica 1992    2378618 71.76600   7404.9237\n790                   Jamaica 1997    2531311 72.26200   7121.9247\n791                   Jamaica 2002    2664659 72.04700   6994.7749\n792                   Jamaica 2007    2780132 72.56700   7320.8803\n793                     Japan 1952   86459025 63.03000   3216.9563\n794                     Japan 1957   91563009 65.50000   4317.6944\n795                     Japan 1962   95831757 68.73000   6576.6495\n796                     Japan 1967  100825279 71.43000   9847.7886\n797                     Japan 1972  107188273 73.42000  14778.7864\n798                     Japan 1977  113872473 75.38000  16610.3770\n799                     Japan 1982  118454974 77.11000  19384.1057\n800                     Japan 1987  122091325 78.67000  22375.9419\n801                     Japan 1992  124329269 79.36000  26824.8951\n802                     Japan 1997  125956499 80.69000  28816.5850\n803                     Japan 2002  127065841 82.00000  28604.5919\n804                     Japan 2007  127467972 82.60300  31656.0681\n805                    Jordan 1952     607914 43.15800   1546.9078\n806                    Jordan 1957     746559 45.66900   1886.0806\n807                    Jordan 1962     933559 48.12600   2348.0092\n808                    Jordan 1967    1255058 51.62900   2741.7963\n809                    Jordan 1972    1613551 56.52800   2110.8563\n810                    Jordan 1977    1937652 61.13400   2852.3516\n811                    Jordan 1982    2347031 63.73900   4161.4160\n812                    Jordan 1987    2820042 65.86900   4448.6799\n813                    Jordan 1992    3867409 68.01500   3431.5936\n814                    Jordan 1997    4526235 69.77200   3645.3796\n815                    Jordan 2002    5307470 71.26300   3844.9172\n816                    Jordan 2007    6053193 72.53500   4519.4612\n817                     Kenya 1952    6464046 42.27000    853.5409\n818                     Kenya 1957    7454779 44.68600    944.4383\n819                     Kenya 1962    8678557 47.94900    896.9664\n820                     Kenya 1967   10191512 50.65400   1056.7365\n821                     Kenya 1972   12044785 53.55900   1222.3600\n822                     Kenya 1977   14500404 56.15500   1267.6132\n823                     Kenya 1982   17661452 58.76600   1348.2258\n824                     Kenya 1987   21198082 59.33900   1361.9369\n825                     Kenya 1992   25020539 59.28500   1341.9217\n826                     Kenya 1997   28263827 54.40700   1360.4850\n827                     Kenya 2002   31386842 50.99200   1287.5147\n828                     Kenya 2007   35610177 54.11000   1463.2493\n829           Korea Dem. Rep. 1952    8865488 50.05600   1088.2778\n830           Korea Dem. Rep. 1957    9411381 54.08100   1571.1347\n831           Korea Dem. Rep. 1962   10917494 56.65600   1621.6936\n832           Korea Dem. Rep. 1967   12617009 59.94200   2143.5406\n833           Korea Dem. Rep. 1972   14781241 63.98300   3701.6215\n834           Korea Dem. Rep. 1977   16325320 67.15900   4106.3012\n835           Korea Dem. Rep. 1982   17647518 69.10000   4106.5253\n836           Korea Dem. Rep. 1987   19067554 70.64700   4106.4923\n837           Korea Dem. Rep. 1992   20711375 69.97800   3726.0635\n838           Korea Dem. Rep. 1997   21585105 67.72700   1690.7568\n839           Korea Dem. Rep. 2002   22215365 66.66200   1646.7582\n840           Korea Dem. Rep. 2007   23301725 67.29700   1593.0655\n841                Korea Rep. 1952   20947571 47.45300   1030.5922\n842                Korea Rep. 1957   22611552 52.68100   1487.5935\n843                Korea Rep. 1962   26420307 55.29200   1536.3444\n844                Korea Rep. 1967   30131000 57.71600   2029.2281\n845                Korea Rep. 1972   33505000 62.61200   3030.8767\n846                Korea Rep. 1977   36436000 64.76600   4657.2210\n847                Korea Rep. 1982   39326000 67.12300   5622.9425\n848                Korea Rep. 1987   41622000 69.81000   8533.0888\n849                Korea Rep. 1992   43805450 72.24400  12104.2787\n850                Korea Rep. 1997   46173816 74.64700  15993.5280\n851                Korea Rep. 2002   47969150 77.04500  19233.9882\n852                Korea Rep. 2007   49044790 78.62300  23348.1397\n853                    Kuwait 1952     160000 55.56500 108382.3529\n854                    Kuwait 1957     212846 58.03300 113523.1329\n855                    Kuwait 1962     358266 60.47000  95458.1118\n856                    Kuwait 1967     575003 64.62400  80894.8833\n857                    Kuwait 1972     841934 67.71200 109347.8670\n858                    Kuwait 1977    1140357 69.34300  59265.4771\n859                    Kuwait 1982    1497494 71.30900  31354.0357\n860                    Kuwait 1987    1891487 74.17400  28118.4300\n861                    Kuwait 1992    1418095 75.19000  34932.9196\n862                    Kuwait 1997    1765345 76.15600  40300.6200\n863                    Kuwait 2002    2111561 76.90400  35110.1057\n864                    Kuwait 2007    2505559 77.58800  47306.9898\n865                   Lebanon 1952    1439529 55.92800   4834.8041\n866                   Lebanon 1957    1647412 59.48900   6089.7869\n867                   Lebanon 1962    1886848 62.09400   5714.5606\n868                   Lebanon 1967    2186894 63.87000   6006.9830\n869                   Lebanon 1972    2680018 65.42100   7486.3843\n870                   Lebanon 1977    3115787 66.09900   8659.6968\n871                   Lebanon 1982    3086876 66.98300   7640.5195\n872                   Lebanon 1987    3089353 67.92600   5377.0913\n873                   Lebanon 1992    3219994 69.29200   6890.8069\n874                   Lebanon 1997    3430388 70.26500   8754.9639\n875                   Lebanon 2002    3677780 71.02800   9313.9388\n876                   Lebanon 2007    3921278 71.99300  10461.0587\n877                   Lesotho 1952     748747 42.13800    298.8462\n878                   Lesotho 1957     813338 45.04700    335.9971\n879                   Lesotho 1962     893143 47.74700    411.8006\n880                   Lesotho 1967     996380 48.49200    498.6390\n881                   Lesotho 1972    1116779 49.76700    496.5816\n882                   Lesotho 1977    1251524 52.20800    745.3695\n883                   Lesotho 1982    1411807 55.07800    797.2631\n884                   Lesotho 1987    1599200 57.18000    773.9932\n885                   Lesotho 1992    1803195 59.68500    977.4863\n886                   Lesotho 1997    1982823 55.55800   1186.1480\n887                   Lesotho 2002    2046772 44.59300   1275.1846\n888                   Lesotho 2007    2012649 42.59200   1569.3314\n889                   Liberia 1952     863308 38.48000    575.5730\n890                   Liberia 1957     975950 39.48600    620.9700\n891                   Liberia 1962    1112796 40.50200    634.1952\n892                   Liberia 1967    1279406 41.53600    713.6036\n893                   Liberia 1972    1482628 42.61400    803.0055\n894                   Liberia 1977    1703617 43.76400    640.3224\n895                   Liberia 1982    1956875 44.85200    572.1996\n896                   Liberia 1987    2269414 46.02700    506.1139\n897                   Liberia 1992    1912974 40.80200    636.6229\n898                   Liberia 1997    2200725 42.22100    609.1740\n899                   Liberia 2002    2814651 43.75300    531.4824\n900                   Liberia 2007    3193942 45.67800    414.5073\n901                     Libya 1952    1019729 42.72300   2387.5481\n902                     Libya 1957    1201578 45.28900   3448.2844\n903                     Libya 1962    1441863 47.80800   6757.0308\n904                     Libya 1967    1759224 50.22700  18772.7517\n905                     Libya 1972    2183877 52.77300  21011.4972\n906                     Libya 1977    2721783 57.44200  21951.2118\n907                     Libya 1982    3344074 62.15500  17364.2754\n908                     Libya 1987    3799845 66.23400  11770.5898\n909                     Libya 1992    4364501 68.75500   9640.1385\n910                     Libya 1997    4759670 71.55500   9467.4461\n911                     Libya 2002    5368585 72.73700   9534.6775\n912                     Libya 2007    6036914 73.95200  12057.4993\n913                Madagascar 1952    4762912 36.68100   1443.0117\n914                Madagascar 1957    5181679 38.86500   1589.2027\n915                Madagascar 1962    5703324 40.84800   1643.3871\n916                Madagascar 1967    6334556 42.88100   1634.0473\n917                Madagascar 1972    7082430 44.85100   1748.5630\n918                Madagascar 1977    8007166 46.88100   1544.2286\n919                Madagascar 1982    9171477 48.96900   1302.8787\n920                Madagascar 1987   10568642 49.35000   1155.4419\n921                Madagascar 1992   12210395 52.21400   1040.6762\n922                Madagascar 1997   14165114 54.97800    986.2959\n923                Madagascar 2002   16473477 57.28600    894.6371\n924                Madagascar 2007   19167654 59.44300   1044.7701\n925                    Malawi 1952    2917802 36.25600    369.1651\n926                    Malawi 1957    3221238 37.20700    416.3698\n927                    Malawi 1962    3628608 38.41000    427.9011\n928                    Malawi 1967    4147252 39.48700    495.5148\n929                    Malawi 1972    4730997 41.76600    584.6220\n930                    Malawi 1977    5637246 43.76700    663.2237\n931                    Malawi 1982    6502825 45.64200    632.8039\n932                    Malawi 1987    7824747 47.45700    635.5174\n933                    Malawi 1992   10014249 49.42000    563.2000\n934                    Malawi 1997   10419991 47.49500    692.2758\n935                    Malawi 2002   11824495 45.00900    665.4231\n936                    Malawi 2007   13327079 48.30300    759.3499\n937                  Malaysia 1952    6748378 48.46300   1831.1329\n938                  Malaysia 1957    7739235 52.10200   1810.0670\n939                  Malaysia 1962    8906385 55.73700   2036.8849\n940                  Malaysia 1967   10154878 59.37100   2277.7424\n941                  Malaysia 1972   11441462 63.01000   2849.0948\n942                  Malaysia 1977   12845381 65.25600   3827.9216\n943                  Malaysia 1982   14441916 68.00000   4920.3560\n944                  Malaysia 1987   16331785 69.50000   5249.8027\n945                  Malaysia 1992   18319502 70.69300   7277.9128\n946                  Malaysia 1997   20476091 71.93800  10132.9096\n947                  Malaysia 2002   22662365 73.04400  10206.9779\n948                  Malaysia 2007   24821286 74.24100  12451.6558\n949                      Mali 1952    3838168 33.68500    452.3370\n950                      Mali 1957    4241884 35.30700    490.3822\n951                      Mali 1962    4690372 36.93600    496.1743\n952                      Mali 1967    5212416 38.48700    545.0099\n953                      Mali 1972    5828158 39.97700    581.3689\n954                      Mali 1977    6491649 41.71400    686.3953\n955                      Mali 1982    6998256 43.91600    618.0141\n956                      Mali 1987    7634008 46.36400    684.1716\n957                      Mali 1992    8416215 48.38800    739.0144\n958                      Mali 1997    9384984 49.90300    790.2580\n959                      Mali 2002   10580176 51.81800    951.4098\n960                      Mali 2007   12031795 54.46700   1042.5816\n961                Mauritania 1952    1022556 40.54300    743.1159\n962                Mauritania 1957    1076852 42.33800    846.1203\n963                Mauritania 1962    1146757 44.24800   1055.8960\n964                Mauritania 1967    1230542 46.28900   1421.1452\n965                Mauritania 1972    1332786 48.43700   1586.8518\n966                Mauritania 1977    1456688 50.85200   1497.4922\n967                Mauritania 1982    1622136 53.59900   1481.1502\n968                Mauritania 1987    1841240 56.14500   1421.6036\n969                Mauritania 1992    2119465 58.33300   1361.3698\n970                Mauritania 1997    2444741 60.43000   1483.1361\n971                Mauritania 2002    2828858 62.24700   1579.0195\n972                Mauritania 2007    3270065 64.16400   1803.1515\n973                 Mauritius 1952     516556 50.98600   1967.9557\n974                 Mauritius 1957     609816 58.08900   2034.0380\n975                 Mauritius 1962     701016 60.24600   2529.0675\n976                 Mauritius 1967     789309 61.55700   2475.3876\n977                 Mauritius 1972     851334 62.94400   2575.4842\n978                 Mauritius 1977     913025 64.93000   3710.9830\n979                 Mauritius 1982     992040 66.71100   3688.0377\n980                 Mauritius 1987    1042663 68.74000   4783.5869\n981                 Mauritius 1992    1096202 69.74500   6058.2538\n982                 Mauritius 1997    1149818 70.73600   7425.7053\n983                 Mauritius 2002    1200206 71.95400   9021.8159\n984                 Mauritius 2007    1250882 72.80100  10956.9911\n985                    Mexico 1952   30144317 50.78900   3478.1255\n986                    Mexico 1957   35015548 55.19000   4131.5466\n987                    Mexico 1962   41121485 58.29900   4581.6094\n988                    Mexico 1967   47995559 60.11000   5754.7339\n989                    Mexico 1972   55984294 62.36100   6809.4067\n990                    Mexico 1977   63759976 65.03200   7674.9291\n991                    Mexico 1982   71640904 67.40500   9611.1475\n992                    Mexico 1987   80122492 69.49800   8688.1560\n993                    Mexico 1992   88111030 71.45500   9472.3843\n994                    Mexico 1997   95895146 73.67000   9767.2975\n995                    Mexico 2002  102479927 74.90200  10742.4405\n996                    Mexico 2007  108700891 76.19500  11977.5750\n997                  Mongolia 1952     800663 42.24400    786.5669\n998                  Mongolia 1957     882134 45.24800    912.6626\n999                  Mongolia 1962    1010280 48.25100   1056.3540\n1000                 Mongolia 1967    1149500 51.25300   1226.0411\n1001                 Mongolia 1972    1320500 53.75400   1421.7420\n1002                 Mongolia 1977    1528000 55.49100   1647.5117\n1003                 Mongolia 1982    1756032 57.48900   2000.6031\n1004                 Mongolia 1987    2015133 60.22200   2338.0083\n1005                 Mongolia 1992    2312802 61.27100   1785.4020\n1006                 Mongolia 1997    2494803 63.62500   1902.2521\n1007                 Mongolia 2002    2674234 65.03300   2140.7393\n1008                 Mongolia 2007    2874127 66.80300   3095.7723\n1009               Montenegro 1952     413834 59.16400   2647.5856\n1010               Montenegro 1957     442829 61.44800   3682.2599\n1011               Montenegro 1962     474528 63.72800   4649.5938\n1012               Montenegro 1967     501035 67.17800   5907.8509\n1013               Montenegro 1972     527678 70.63600   7778.4140\n1014               Montenegro 1977     560073 73.06600   9595.9299\n1015               Montenegro 1982     562548 74.10100  11222.5876\n1016               Montenegro 1987     569473 74.86500  11732.5102\n1017               Montenegro 1992     621621 75.43500   7003.3390\n1018               Montenegro 1997     692651 75.44500   6465.6133\n1019               Montenegro 2002     720230 73.98100   6557.1943\n1020               Montenegro 2007     684736 74.54300   9253.8961\n1021                  Morocco 1952    9939217 42.87300   1688.2036\n1022                  Morocco 1957   11406350 45.42300   1642.0023\n1023                  Morocco 1962   13056604 47.92400   1566.3535\n1024                  Morocco 1967   14770296 50.33500   1711.0448\n1025                  Morocco 1972   16660670 52.86200   1930.1950\n1026                  Morocco 1977   18396941 55.73000   2370.6200\n1027                  Morocco 1982   20198730 59.65000   2702.6204\n1028                  Morocco 1987   22987397 62.67700   2755.0470\n1029                  Morocco 1992   25798239 65.39300   2948.0473\n1030                  Morocco 1997   28529501 67.66000   2982.1019\n1031                  Morocco 2002   31167783 69.61500   3258.4956\n1032                  Morocco 2007   33757175 71.16400   3820.1752\n1033               Mozambique 1952    6446316 31.28600    468.5260\n1034               Mozambique 1957    7038035 33.77900    495.5868\n1035               Mozambique 1962    7788944 36.16100    556.6864\n1036               Mozambique 1967    8680909 38.11300    566.6692\n1037               Mozambique 1972    9809596 40.32800    724.9178\n1038               Mozambique 1977   11127868 42.49500    502.3197\n1039               Mozambique 1982   12587223 42.79500    462.2114\n1040               Mozambique 1987   12891952 42.86100    389.8762\n1041               Mozambique 1992   13160731 44.28400    410.8968\n1042               Mozambique 1997   16603334 46.34400    472.3461\n1043               Mozambique 2002   18473780 44.02600    633.6179\n1044               Mozambique 2007   19951656 42.08200    823.6856\n1045                  Myanmar 1952   20092996 36.31900    331.0000\n1046                  Myanmar 1957   21731844 41.90500    350.0000\n1047                  Myanmar 1962   23634436 45.10800    388.0000\n1048                  Myanmar 1967   25870271 49.37900    349.0000\n1049                  Myanmar 1972   28466390 53.07000    357.0000\n1050                  Myanmar 1977   31528087 56.05900    371.0000\n1051                  Myanmar 1982   34680442 58.05600    424.0000\n1052                  Myanmar 1987   38028578 58.33900    385.0000\n1053                  Myanmar 1992   40546538 59.32000    347.0000\n1054                  Myanmar 1997   43247867 60.32800    415.0000\n1055                  Myanmar 2002   45598081 59.90800    611.0000\n1056                  Myanmar 2007   47761980 62.06900    944.0000\n1057                  Namibia 1952     485831 41.72500   2423.7804\n1058                  Namibia 1957     548080 45.22600   2621.4481\n1059                  Namibia 1962     621392 48.38600   3173.2156\n1060                  Namibia 1967     706640 51.15900   3793.6948\n1061                  Namibia 1972     821782 53.86700   3746.0809\n1062                  Namibia 1977     977026 56.43700   3876.4860\n1063                  Namibia 1982    1099010 58.96800   4191.1005\n1064                  Namibia 1987    1278184 60.83500   3693.7313\n1065                  Namibia 1992    1554253 61.99900   3804.5380\n1066                  Namibia 1997    1774766 58.90900   3899.5243\n1067                  Namibia 2002    1972153 51.47900   4072.3248\n1068                  Namibia 2007    2055080 52.90600   4811.0604\n1069                    Nepal 1952    9182536 36.15700    545.8657\n1070                    Nepal 1957    9682338 37.68600    597.9364\n1071                    Nepal 1962   10332057 39.39300    652.3969\n1072                    Nepal 1967   11261690 41.47200    676.4422\n1073                    Nepal 1972   12412593 43.97100    674.7881\n1074                    Nepal 1977   13933198 46.74800    694.1124\n1075                    Nepal 1982   15796314 49.59400    718.3731\n1076                    Nepal 1987   17917180 52.53700    775.6325\n1077                    Nepal 1992   20326209 55.72700    897.7404\n1078                    Nepal 1997   23001113 59.42600   1010.8921\n1079                    Nepal 2002   25873917 61.34000   1057.2063\n1080                    Nepal 2007   28901790 63.78500   1091.3598\n1081              Netherlands 1952   10381988 72.13000   8941.5719\n1082              Netherlands 1957   11026383 72.99000  11276.1934\n1083              Netherlands 1962   11805689 73.23000  12790.8496\n1084              Netherlands 1967   12596822 73.82000  15363.2514\n1085              Netherlands 1972   13329874 73.75000  18794.7457\n1086              Netherlands 1977   13852989 75.24000  21209.0592\n1087              Netherlands 1982   14310401 76.05000  21399.4605\n1088              Netherlands 1987   14665278 76.83000  23651.3236\n1089              Netherlands 1992   15174244 77.42000  26790.9496\n1090              Netherlands 1997   15604464 78.03000  30246.1306\n1091              Netherlands 2002   16122830 78.53000  33724.7578\n1092              Netherlands 2007   16570613 79.76200  36797.9333\n1093              New Zealand 1952    1994794 69.39000  10556.5757\n1094              New Zealand 1957    2229407 70.26000  12247.3953\n1095              New Zealand 1962    2488550 71.24000  13175.6780\n1096              New Zealand 1967    2728150 71.52000  14463.9189\n1097              New Zealand 1972    2929100 71.89000  16046.0373\n1098              New Zealand 1977    3164900 72.22000  16233.7177\n1099              New Zealand 1982    3210650 73.84000  17632.4104\n1100              New Zealand 1987    3317166 74.32000  19007.1913\n1101              New Zealand 1992    3437674 76.33000  18363.3249\n1102              New Zealand 1997    3676187 77.55000  21050.4138\n1103              New Zealand 2002    3908037 79.11000  23189.8014\n1104              New Zealand 2007    4115771 80.20400  25185.0091\n1105                Nicaragua 1952    1165790 42.31400   3112.3639\n1106                Nicaragua 1957    1358828 45.43200   3457.4159\n1107                Nicaragua 1962    1590597 48.63200   3634.3644\n1108                Nicaragua 1967    1865490 51.88400   4643.3935\n1109                Nicaragua 1972    2182908 55.15100   4688.5933\n1110                Nicaragua 1977    2554598 57.47000   5486.3711\n1111                Nicaragua 1982    2979423 59.29800   3470.3382\n1112                Nicaragua 1987    3344353 62.00800   2955.9844\n1113                Nicaragua 1992    4017939 65.84300   2170.1517\n1114                Nicaragua 1997    4609572 68.42600   2253.0230\n1115                Nicaragua 2002    5146848 70.83600   2474.5488\n1116                Nicaragua 2007    5675356 72.89900   2749.3210\n1117                    Niger 1952    3379468 37.44400    761.8794\n1118                    Niger 1957    3692184 38.59800    835.5234\n1119                    Niger 1962    4076008 39.48700    997.7661\n1120                    Niger 1967    4534062 40.11800   1054.3849\n1121                    Niger 1972    5060262 40.54600    954.2092\n1122                    Niger 1977    5682086 41.29100    808.8971\n1123                    Niger 1982    6437188 42.59800    909.7221\n1124                    Niger 1987    7332638 44.55500    668.3000\n1125                    Niger 1992    8392818 47.39100    581.1827\n1126                    Niger 1997    9666252 51.31300    580.3052\n1127                    Niger 2002   11140655 54.49600    601.0745\n1128                    Niger 2007   12894865 56.86700    619.6769\n1129                  Nigeria 1952   33119096 36.32400   1077.2819\n1130                  Nigeria 1957   37173340 37.80200   1100.5926\n1131                  Nigeria 1962   41871351 39.36000   1150.9275\n1132                  Nigeria 1967   47287752 41.04000   1014.5141\n1133                  Nigeria 1972   53740085 42.82100   1698.3888\n1134                  Nigeria 1977   62209173 44.51400   1981.9518\n1135                  Nigeria 1982   73039376 45.82600   1576.9738\n1136                  Nigeria 1987   81551520 46.88600   1385.0296\n1137                  Nigeria 1992   93364244 47.47200   1619.8482\n1138                  Nigeria 1997  106207839 47.46400   1624.9413\n1139                  Nigeria 2002  119901274 46.60800   1615.2864\n1140                  Nigeria 2007  135031164 46.85900   2013.9773\n1141                   Norway 1952    3327728 72.67000  10095.4217\n1142                   Norway 1957    3491938 73.44000  11653.9730\n1143                   Norway 1962    3638919 73.47000  13450.4015\n1144                   Norway 1967    3786019 74.08000  16361.8765\n1145                   Norway 1972    3933004 74.34000  18965.0555\n1146                   Norway 1977    4043205 75.37000  23311.3494\n1147                   Norway 1982    4114787 75.97000  26298.6353\n1148                   Norway 1987    4186147 75.89000  31540.9748\n1149                   Norway 1992    4286357 77.32000  33965.6611\n1150                   Norway 1997    4405672 78.32000  41283.1643\n1151                   Norway 2002    4535591 79.05000  44683.9753\n1152                   Norway 2007    4627926 80.19600  49357.1902\n1153                     Oman 1952     507833 37.57800   1828.2303\n1154                     Oman 1957     561977 40.08000   2242.7466\n1155                     Oman 1962     628164 43.16500   2924.6381\n1156                     Oman 1967     714775 46.98800   4720.9427\n1157                     Oman 1972     829050 52.14300  10618.0385\n1158                     Oman 1977    1004533 57.36700  11848.3439\n1159                     Oman 1982    1301048 62.72800  12954.7910\n1160                     Oman 1987    1593882 67.73400  18115.2231\n1161                     Oman 1992    1915208 71.19700  18616.7069\n1162                     Oman 1997    2283635 72.49900  19702.0558\n1163                     Oman 2002    2713462 74.19300  19774.8369\n1164                     Oman 2007    3204897 75.64000  22316.1929\n1165                 Pakistan 1952   41346560 43.43600    684.5971\n1166                 Pakistan 1957   46679944 45.55700    747.0835\n1167                 Pakistan 1962   53100671 47.67000    803.3427\n1168                 Pakistan 1967   60641899 49.80000    942.4083\n1169                 Pakistan 1972   69325921 51.92900   1049.9390\n1170                 Pakistan 1977   78152686 54.04300   1175.9212\n1171                 Pakistan 1982   91462088 56.15800   1443.4298\n1172                 Pakistan 1987  105186881 58.24500   1704.6866\n1173                 Pakistan 1992  120065004 60.83800   1971.8295\n1174                 Pakistan 1997  135564834 61.81800   2049.3505\n1175                 Pakistan 2002  153403524 63.61000   2092.7124\n1176                 Pakistan 2007  169270617 65.48300   2605.9476\n1177                   Panama 1952     940080 55.19100   2480.3803\n1178                   Panama 1957    1063506 59.20100   2961.8009\n1179                   Panama 1962    1215725 61.81700   3536.5403\n1180                   Panama 1967    1405486 64.07100   4421.0091\n1181                   Panama 1972    1616384 66.21600   5364.2497\n1182                   Panama 1977    1839782 68.68100   5351.9121\n1183                   Panama 1982    2036305 70.47200   7009.6016\n1184                   Panama 1987    2253639 71.52300   7034.7792\n1185                   Panama 1992    2484997 72.46200   6618.7431\n1186                   Panama 1997    2734531 73.73800   7113.6923\n1187                   Panama 2002    2990875 74.71200   7356.0319\n1188                   Panama 2007    3242173 75.53700   9809.1856\n1189                 Paraguay 1952    1555876 62.64900   1952.3087\n1190                 Paraguay 1957    1770902 63.19600   2046.1547\n1191                 Paraguay 1962    2009813 64.36100   2148.0271\n1192                 Paraguay 1967    2287985 64.95100   2299.3763\n1193                 Paraguay 1972    2614104 65.81500   2523.3380\n1194                 Paraguay 1977    2984494 66.35300   3248.3733\n1195                 Paraguay 1982    3366439 66.87400   4258.5036\n1196                 Paraguay 1987    3886512 67.37800   3998.8757\n1197                 Paraguay 1992    4483945 68.22500   4196.4111\n1198                 Paraguay 1997    5154123 69.40000   4247.4003\n1199                 Paraguay 2002    5884491 70.75500   3783.6742\n1200                 Paraguay 2007    6667147 71.75200   4172.8385\n1201                     Peru 1952    8025700 43.90200   3758.5234\n1202                     Peru 1957    9146100 46.26300   4245.2567\n1203                     Peru 1962   10516500 49.09600   4957.0380\n1204                     Peru 1967   12132200 51.44500   5788.0933\n1205                     Peru 1972   13954700 55.44800   5937.8273\n1206                     Peru 1977   15990099 58.44700   6281.2909\n1207                     Peru 1982   18125129 61.40600   6434.5018\n1208                     Peru 1987   20195924 64.13400   6360.9434\n1209                     Peru 1992   22430449 66.45800   4446.3809\n1210                     Peru 1997   24748122 68.38600   5838.3477\n1211                     Peru 2002   26769436 69.90600   5909.0201\n1212                     Peru 2007   28674757 71.42100   7408.9056\n1213              Philippines 1952   22438691 47.75200   1272.8810\n1214              Philippines 1957   26072194 51.33400   1547.9448\n1215              Philippines 1962   30325264 54.75700   1649.5522\n1216              Philippines 1967   35356600 56.39300   1814.1274\n1217              Philippines 1972   40850141 58.06500   1989.3741\n1218              Philippines 1977   46850962 60.06000   2373.2043\n1219              Philippines 1982   53456774 62.08200   2603.2738\n1220              Philippines 1987   60017788 64.15100   2189.6350\n1221              Philippines 1992   67185766 66.45800   2279.3240\n1222              Philippines 1997   75012988 68.56400   2536.5349\n1223              Philippines 2002   82995088 70.30300   2650.9211\n1224              Philippines 2007   91077287 71.68800   3190.4810\n1225                   Poland 1952   25730551 61.31000   4029.3297\n1226                   Poland 1957   28235346 65.77000   4734.2530\n1227                   Poland 1962   30329617 67.64000   5338.7521\n1228                   Poland 1967   31785378 69.61000   6557.1528\n1229                   Poland 1972   33039545 70.85000   8006.5070\n1230                   Poland 1977   34621254 70.67000   9508.1415\n1231                   Poland 1982   36227381 71.32000   8451.5310\n1232                   Poland 1987   37740710 70.98000   9082.3512\n1233                   Poland 1992   38370697 70.99000   7738.8812\n1234                   Poland 1997   38654957 72.75000  10159.5837\n1235                   Poland 2002   38625976 74.67000  12002.2391\n1236                   Poland 2007   38518241 75.56300  15389.9247\n1237                 Portugal 1952    8526050 59.82000   3068.3199\n1238                 Portugal 1957    8817650 61.51000   3774.5717\n1239                 Portugal 1962    9019800 64.39000   4727.9549\n1240                 Portugal 1967    9103000 66.60000   6361.5180\n1241                 Portugal 1972    8970450 69.26000   9022.2474\n1242                 Portugal 1977    9662600 70.41000  10172.4857\n1243                 Portugal 1982    9859650 72.77000  11753.8429\n1244                 Portugal 1987    9915289 74.06000  13039.3088\n1245                 Portugal 1992    9927680 74.86000  16207.2666\n1246                 Portugal 1997   10156415 75.97000  17641.0316\n1247                 Portugal 2002   10433867 77.29000  19970.9079\n1248                 Portugal 2007   10642836 78.09800  20509.6478\n1249              Puerto Rico 1952    2227000 64.28000   3081.9598\n1250              Puerto Rico 1957    2260000 68.54000   3907.1562\n1251              Puerto Rico 1962    2448046 69.62000   5108.3446\n1252              Puerto Rico 1967    2648961 71.10000   6929.2777\n1253              Puerto Rico 1972    2847132 72.16000   9123.0417\n1254              Puerto Rico 1977    3080828 73.44000   9770.5249\n1255              Puerto Rico 1982    3279001 73.75000  10330.9891\n1256              Puerto Rico 1987    3444468 74.63000  12281.3419\n1257              Puerto Rico 1992    3585176 73.91100  14641.5871\n1258              Puerto Rico 1997    3759430 74.91700  16999.4333\n1259              Puerto Rico 2002    3859606 77.77800  18855.6062\n1260              Puerto Rico 2007    3942491 78.74600  19328.7090\n1261                  Reunion 1952     257700 52.72400   2718.8853\n1262                  Reunion 1957     308700 55.09000   2769.4518\n1263                  Reunion 1962     358900 57.66600   3173.7233\n1264                  Reunion 1967     414024 60.54200   4021.1757\n1265                  Reunion 1972     461633 64.27400   5047.6586\n1266                  Reunion 1977     492095 67.06400   4319.8041\n1267                  Reunion 1982     517810 69.88500   5267.2194\n1268                  Reunion 1987     562035 71.91300   5303.3775\n1269                  Reunion 1992     622191 73.61500   6101.2558\n1270                  Reunion 1997     684810 74.77200   6071.9414\n1271                  Reunion 2002     743981 75.74400   6316.1652\n1272                  Reunion 2007     798094 76.44200   7670.1226\n1273                  Romania 1952   16630000 61.05000   3144.6132\n1274                  Romania 1957   17829327 64.10000   3943.3702\n1275                  Romania 1962   18680721 66.80000   4734.9976\n1276                  Romania 1967   19284814 66.80000   6470.8665\n1277                  Romania 1972   20662648 69.21000   8011.4144\n1278                  Romania 1977   21658597 69.46000   9356.3972\n1279                  Romania 1982   22356726 69.66000   9605.3141\n1280                  Romania 1987   22686371 69.53000   9696.2733\n1281                  Romania 1992   22797027 69.36000   6598.4099\n1282                  Romania 1997   22562458 69.72000   7346.5476\n1283                  Romania 2002   22404337 71.32200   7885.3601\n1284                  Romania 2007   22276056 72.47600  10808.4756\n1285                   Rwanda 1952    2534927 40.00000    493.3239\n1286                   Rwanda 1957    2822082 41.50000    540.2894\n1287                   Rwanda 1962    3051242 43.00000    597.4731\n1288                   Rwanda 1967    3451079 44.10000    510.9637\n1289                   Rwanda 1972    3992121 44.60000    590.5807\n1290                   Rwanda 1977    4657072 45.00000    670.0806\n1291                   Rwanda 1982    5507565 46.21800    881.5706\n1292                   Rwanda 1987    6349365 44.02000    847.9912\n1293                   Rwanda 1992    7290203 23.59900    737.0686\n1294                   Rwanda 1997    7212583 36.08700    589.9445\n1295                   Rwanda 2002    7852401 43.41300    785.6538\n1296                   Rwanda 2007    8860588 46.24200    863.0885\n1297    Sao Tome and Principe 1952      60011 46.47100    879.5836\n1298    Sao Tome and Principe 1957      61325 48.94500    860.7369\n1299    Sao Tome and Principe 1962      65345 51.89300   1071.5511\n1300    Sao Tome and Principe 1967      70787 54.42500   1384.8406\n1301    Sao Tome and Principe 1972      76595 56.48000   1532.9853\n1302    Sao Tome and Principe 1977      86796 58.55000   1737.5617\n1303    Sao Tome and Principe 1982      98593 60.35100   1890.2181\n1304    Sao Tome and Principe 1987     110812 61.72800   1516.5255\n1305    Sao Tome and Principe 1992     125911 62.74200   1428.7778\n1306    Sao Tome and Principe 1997     145608 63.30600   1339.0760\n1307    Sao Tome and Principe 2002     170372 64.33700   1353.0924\n1308    Sao Tome and Principe 2007     199579 65.52800   1598.4351\n1309             Saudi Arabia 1952    4005677 39.87500   6459.5548\n1310             Saudi Arabia 1957    4419650 42.86800   8157.5912\n1311             Saudi Arabia 1962    4943029 45.91400  11626.4197\n1312             Saudi Arabia 1967    5618198 49.90100  16903.0489\n1313             Saudi Arabia 1972    6472756 53.88600  24837.4287\n1314             Saudi Arabia 1977    8128505 58.69000  34167.7626\n1315             Saudi Arabia 1982   11254672 63.01200  33693.1753\n1316             Saudi Arabia 1987   14619745 66.29500  21198.2614\n1317             Saudi Arabia 1992   16945857 68.76800  24841.6178\n1318             Saudi Arabia 1997   21229759 70.53300  20586.6902\n1319             Saudi Arabia 2002   24501530 71.62600  19014.5412\n1320             Saudi Arabia 2007   27601038 72.77700  21654.8319\n1321                  Senegal 1952    2755589 37.27800   1450.3570\n1322                  Senegal 1957    3054547 39.32900   1567.6530\n1323                  Senegal 1962    3430243 41.45400   1654.9887\n1324                  Senegal 1967    3965841 43.56300   1612.4046\n1325                  Senegal 1972    4588696 45.81500   1597.7121\n1326                  Senegal 1977    5260855 48.87900   1561.7691\n1327                  Senegal 1982    6147783 52.37900   1518.4800\n1328                  Senegal 1987    7171347 55.76900   1441.7207\n1329                  Senegal 1992    8307920 58.19600   1367.8994\n1330                  Senegal 1997    9535314 60.18700   1392.3683\n1331                  Senegal 2002   10870037 61.60000   1519.6353\n1332                  Senegal 2007   12267493 63.06200   1712.4721\n1333                   Serbia 1952    6860147 57.99600   3581.4594\n1334                   Serbia 1957    7271135 61.68500   4981.0909\n1335                   Serbia 1962    7616060 64.53100   6289.6292\n1336                   Serbia 1967    7971222 66.91400   7991.7071\n1337                   Serbia 1972    8313288 68.70000  10522.0675\n1338                   Serbia 1977    8686367 70.30000  12980.6696\n1339                   Serbia 1982    9032824 70.16200  15181.0927\n1340                   Serbia 1987    9230783 71.21800  15870.8785\n1341                   Serbia 1992    9826397 71.65900   9325.0682\n1342                   Serbia 1997   10336594 72.23200   7914.3203\n1343                   Serbia 2002   10111559 73.21300   7236.0753\n1344                   Serbia 2007   10150265 74.00200   9786.5347\n1345             Sierra Leone 1952    2143249 30.33100    879.7877\n1346             Sierra Leone 1957    2295678 31.57000   1004.4844\n1347             Sierra Leone 1962    2467895 32.76700   1116.6399\n1348             Sierra Leone 1967    2662190 34.11300   1206.0435\n1349             Sierra Leone 1972    2879013 35.40000   1353.7598\n1350             Sierra Leone 1977    3140897 36.78800   1348.2852\n1351             Sierra Leone 1982    3464522 38.44500   1465.0108\n1352             Sierra Leone 1987    3868905 40.00600   1294.4478\n1353             Sierra Leone 1992    4260884 38.33300   1068.6963\n1354             Sierra Leone 1997    4578212 39.89700    574.6482\n1355             Sierra Leone 2002    5359092 41.01200    699.4897\n1356             Sierra Leone 2007    6144562 42.56800    862.5408\n1357                Singapore 1952    1127000 60.39600   2315.1382\n1358                Singapore 1957    1445929 63.17900   2843.1044\n1359                Singapore 1962    1750200 65.79800   3674.7356\n1360                Singapore 1967    1977600 67.94600   4977.4185\n1361                Singapore 1972    2152400 69.52100   8597.7562\n1362                Singapore 1977    2325300 70.79500  11210.0895\n1363                Singapore 1982    2651869 71.76000  15169.1611\n1364                Singapore 1987    2794552 73.56000  18861.5308\n1365                Singapore 1992    3235865 75.78800  24769.8912\n1366                Singapore 1997    3802309 77.15800  33519.4766\n1367                Singapore 2002    4197776 78.77000  36023.1054\n1368                Singapore 2007    4553009 79.97200  47143.1796\n1369          Slovak Republic 1952    3558137 64.36000   5074.6591\n1370          Slovak Republic 1957    3844277 67.45000   6093.2630\n1371          Slovak Republic 1962    4237384 70.33000   7481.1076\n1372          Slovak Republic 1967    4442238 70.98000   8412.9024\n1373          Slovak Republic 1972    4593433 70.35000   9674.1676\n1374          Slovak Republic 1977    4827803 70.45000  10922.6640\n1375          Slovak Republic 1982    5048043 70.80000  11348.5459\n1376          Slovak Republic 1987    5199318 71.08000  12037.2676\n1377          Slovak Republic 1992    5302888 71.38000   9498.4677\n1378          Slovak Republic 1997    5383010 72.71000  12126.2306\n1379          Slovak Republic 2002    5410052 73.80000  13638.7784\n1380          Slovak Republic 2007    5447502 74.66300  18678.3144\n1381                 Slovenia 1952    1489518 65.57000   4215.0417\n1382                 Slovenia 1957    1533070 67.85000   5862.2766\n1383                 Slovenia 1962    1582962 69.15000   7402.3034\n1384                 Slovenia 1967    1646912 69.18000   9405.4894\n1385                 Slovenia 1972    1694510 69.82000  12383.4862\n1386                 Slovenia 1977    1746919 70.97000  15277.0302\n1387                 Slovenia 1982    1861252 71.06300  17866.7218\n1388                 Slovenia 1987    1945870 72.25000  18678.5349\n1389                 Slovenia 1992    1999210 73.64000  14214.7168\n1390                 Slovenia 1997    2011612 75.13000  17161.1073\n1391                 Slovenia 2002    2011497 76.66000  20660.0194\n1392                 Slovenia 2007    2009245 77.92600  25768.2576\n1393                  Somalia 1952    2526994 32.97800   1135.7498\n1394                  Somalia 1957    2780415 34.97700   1258.1474\n1395                  Somalia 1962    3080153 36.98100   1369.4883\n1396                  Somalia 1967    3428839 38.97700   1284.7332\n1397                  Somalia 1972    3840161 40.97300   1254.5761\n1398                  Somalia 1977    4353666 41.97400   1450.9925\n1399                  Somalia 1982    5828892 42.95500   1176.8070\n1400                  Somalia 1987    6921858 44.50100   1093.2450\n1401                  Somalia 1992    6099799 39.65800    926.9603\n1402                  Somalia 1997    6633514 43.79500    930.5964\n1403                  Somalia 2002    7753310 45.93600    882.0818\n1404                  Somalia 2007    9118773 48.15900    926.1411\n1405             South Africa 1952   14264935 45.00900   4725.2955\n1406             South Africa 1957   16151549 47.98500   5487.1042\n1407             South Africa 1962   18356657 49.95100   5768.7297\n1408             South Africa 1967   20997321 51.92700   7114.4780\n1409             South Africa 1972   23935810 53.69600   7765.9626\n1410             South Africa 1977   27129932 55.52700   8028.6514\n1411             South Africa 1982   31140029 58.16100   8568.2662\n1412             South Africa 1987   35933379 60.83400   7825.8234\n1413             South Africa 1992   39964159 61.88800   7225.0693\n1414             South Africa 1997   42835005 60.23600   7479.1882\n1415             South Africa 2002   44433622 53.36500   7710.9464\n1416             South Africa 2007   43997828 49.33900   9269.6578\n1417                    Spain 1952   28549870 64.94000   3834.0347\n1418                    Spain 1957   29841614 66.66000   4564.8024\n1419                    Spain 1962   31158061 69.69000   5693.8439\n1420                    Spain 1967   32850275 71.44000   7993.5123\n1421                    Spain 1972   34513161 73.06000  10638.7513\n1422                    Spain 1977   36439000 74.39000  13236.9212\n1423                    Spain 1982   37983310 76.30000  13926.1700\n1424                    Spain 1987   38880702 76.90000  15764.9831\n1425                    Spain 1992   39549438 77.57000  18603.0645\n1426                    Spain 1997   39855442 78.77000  20445.2990\n1427                    Spain 2002   40152517 79.78000  24835.4717\n1428                    Spain 2007   40448191 80.94100  28821.0637\n1429                Sri Lanka 1952    7982342 57.59300   1083.5320\n1430                Sri Lanka 1957    9128546 61.45600   1072.5466\n1431                Sri Lanka 1962   10421936 62.19200   1074.4720\n1432                Sri Lanka 1967   11737396 64.26600   1135.5143\n1433                Sri Lanka 1972   13016733 65.04200   1213.3955\n1434                Sri Lanka 1977   14116836 65.94900   1348.7757\n1435                Sri Lanka 1982   15410151 68.75700   1648.0798\n1436                Sri Lanka 1987   16495304 69.01100   1876.7668\n1437                Sri Lanka 1992   17587060 70.37900   2153.7392\n1438                Sri Lanka 1997   18698655 70.45700   2664.4773\n1439                Sri Lanka 2002   19576783 70.81500   3015.3788\n1440                Sri Lanka 2007   20378239 72.39600   3970.0954\n1441                    Sudan 1952    8504667 38.63500   1615.9911\n1442                    Sudan 1957    9753392 39.62400   1770.3371\n1443                    Sudan 1962   11183227 40.87000   1959.5938\n1444                    Sudan 1967   12716129 42.85800   1687.9976\n1445                    Sudan 1972   14597019 45.08300   1659.6528\n1446                    Sudan 1977   17104986 47.80000   2202.9884\n1447                    Sudan 1982   20367053 50.33800   1895.5441\n1448                    Sudan 1987   24725960 51.74400   1507.8192\n1449                    Sudan 1992   28227588 53.55600   1492.1970\n1450                    Sudan 1997   32160729 55.37300   1632.2108\n1451                    Sudan 2002   37090298 56.36900   1993.3983\n1452                    Sudan 2007   42292929 58.55600   2602.3950\n1453                Swaziland 1952     290243 41.40700   1148.3766\n1454                Swaziland 1957     326741 43.42400   1244.7084\n1455                Swaziland 1962     370006 44.99200   1856.1821\n1456                Swaziland 1967     420690 46.63300   2613.1017\n1457                Swaziland 1972     480105 49.55200   3364.8366\n1458                Swaziland 1977     551425 52.53700   3781.4106\n1459                Swaziland 1982     649901 55.56100   3895.3840\n1460                Swaziland 1987     779348 57.67800   3984.8398\n1461                Swaziland 1992     962344 58.47400   3553.0224\n1462                Swaziland 1997    1054486 54.28900   3876.7685\n1463                Swaziland 2002    1130269 43.86900   4128.1169\n1464                Swaziland 2007    1133066 39.61300   4513.4806\n1465                   Sweden 1952    7124673 71.86000   8527.8447\n1466                   Sweden 1957    7363802 72.49000   9911.8782\n1467                   Sweden 1962    7561588 73.37000  12329.4419\n1468                   Sweden 1967    7867931 74.16000  15258.2970\n1469                   Sweden 1972    8122293 74.72000  17832.0246\n1470                   Sweden 1977    8251648 75.44000  18855.7252\n1471                   Sweden 1982    8325260 76.42000  20667.3812\n1472                   Sweden 1987    8421403 77.19000  23586.9293\n1473                   Sweden 1992    8718867 78.16000  23880.0168\n1474                   Sweden 1997    8897619 79.39000  25266.5950\n1475                   Sweden 2002    8954175 80.04000  29341.6309\n1476                   Sweden 2007    9031088 80.88400  33859.7484\n1477              Switzerland 1952    4815000 69.62000  14734.2327\n1478              Switzerland 1957    5126000 70.56000  17909.4897\n1479              Switzerland 1962    5666000 71.32000  20431.0927\n1480              Switzerland 1967    6063000 72.77000  22966.1443\n1481              Switzerland 1972    6401400 73.78000  27195.1130\n1482              Switzerland 1977    6316424 75.39000  26982.2905\n1483              Switzerland 1982    6468126 76.21000  28397.7151\n1484              Switzerland 1987    6649942 77.41000  30281.7046\n1485              Switzerland 1992    6995447 78.03000  31871.5303\n1486              Switzerland 1997    7193761 79.37000  32135.3230\n1487              Switzerland 2002    7361757 80.62000  34480.9577\n1488              Switzerland 2007    7554661 81.70100  37506.4191\n1489                    Syria 1952    3661549 45.88300   1643.4854\n1490                    Syria 1957    4149908 48.28400   2117.2349\n1491                    Syria 1962    4834621 50.30500   2193.0371\n1492                    Syria 1967    5680812 53.65500   1881.9236\n1493                    Syria 1972    6701172 57.29600   2571.4230\n1494                    Syria 1977    7932503 61.19500   3195.4846\n1495                    Syria 1982    9410494 64.59000   3761.8377\n1496                    Syria 1987   11242847 66.97400   3116.7743\n1497                    Syria 1992   13219062 69.24900   3340.5428\n1498                    Syria 1997   15081016 71.52700   4014.2390\n1499                    Syria 2002   17155814 73.05300   4090.9253\n1500                    Syria 2007   19314747 74.14300   4184.5481\n1501                   Taiwan 1952    8550362 58.50000   1206.9479\n1502                   Taiwan 1957   10164215 62.40000   1507.8613\n1503                   Taiwan 1962   11918938 65.20000   1822.8790\n1504                   Taiwan 1967   13648692 67.50000   2643.8587\n1505                   Taiwan 1972   15226039 69.39000   4062.5239\n1506                   Taiwan 1977   16785196 70.59000   5596.5198\n1507                   Taiwan 1982   18501390 72.16000   7426.3548\n1508                   Taiwan 1987   19757799 73.40000  11054.5618\n1509                   Taiwan 1992   20686918 74.26000  15215.6579\n1510                   Taiwan 1997   21628605 75.25000  20206.8210\n1511                   Taiwan 2002   22454239 76.99000  23235.4233\n1512                   Taiwan 2007   23174294 78.40000  28718.2768\n1513                 Tanzania 1952    8322925 41.21500    716.6501\n1514                 Tanzania 1957    9452826 42.97400    698.5356\n1515                 Tanzania 1962   10863958 44.24600    722.0038\n1516                 Tanzania 1967   12607312 45.75700    848.2187\n1517                 Tanzania 1972   14706593 47.62000    915.9851\n1518                 Tanzania 1977   17129565 49.91900    962.4923\n1519                 Tanzania 1982   19844382 50.60800    874.2426\n1520                 Tanzania 1987   23040630 51.53500    831.8221\n1521                 Tanzania 1992   26605473 50.44000    825.6825\n1522                 Tanzania 1997   30686889 48.46600    789.1862\n1523                 Tanzania 2002   34593779 49.65100    899.0742\n1524                 Tanzania 2007   38139640 52.51700   1107.4822\n1525                 Thailand 1952   21289402 50.84800    757.7974\n1526                 Thailand 1957   25041917 53.63000    793.5774\n1527                 Thailand 1962   29263397 56.06100   1002.1992\n1528                 Thailand 1967   34024249 58.28500   1295.4607\n1529                 Thailand 1972   39276153 60.40500   1524.3589\n1530                 Thailand 1977   44148285 62.49400   1961.2246\n1531                 Thailand 1982   48827160 64.59700   2393.2198\n1532                 Thailand 1987   52910342 66.08400   2982.6538\n1533                 Thailand 1992   56667095 67.29800   4616.8965\n1534                 Thailand 1997   60216677 67.52100   5852.6255\n1535                 Thailand 2002   62806748 68.56400   5913.1875\n1536                 Thailand 2007   65068149 70.61600   7458.3963\n1537                     Togo 1952    1219113 38.59600    859.8087\n1538                     Togo 1957    1357445 41.20800    925.9083\n1539                     Togo 1962    1528098 43.92200   1067.5348\n1540                     Togo 1967    1735550 46.76900   1477.5968\n1541                     Togo 1972    2056351 49.75900   1649.6602\n1542                     Togo 1977    2308582 52.88700   1532.7770\n1543                     Togo 1982    2644765 55.47100   1344.5780\n1544                     Togo 1987    3154264 56.94100   1202.2014\n1545                     Togo 1992    3747553 58.06100   1034.2989\n1546                     Togo 1997    4320890 58.39000    982.2869\n1547                     Togo 2002    4977378 57.56100    886.2206\n1548                     Togo 2007    5701579 58.42000    882.9699\n1549      Trinidad and Tobago 1952     662850 59.10000   3023.2719\n1550      Trinidad and Tobago 1957     764900 61.80000   4100.3934\n1551      Trinidad and Tobago 1962     887498 64.90000   4997.5240\n1552      Trinidad and Tobago 1967     960155 65.40000   5621.3685\n1553      Trinidad and Tobago 1972     975199 65.90000   6619.5514\n1554      Trinidad and Tobago 1977    1039009 68.30000   7899.5542\n1555      Trinidad and Tobago 1982    1116479 68.83200   9119.5286\n1556      Trinidad and Tobago 1987    1191336 69.58200   7388.5978\n1557      Trinidad and Tobago 1992    1183669 69.86200   7370.9909\n1558      Trinidad and Tobago 1997    1138101 69.46500   8792.5731\n1559      Trinidad and Tobago 2002    1101832 68.97600  11460.6002\n1560      Trinidad and Tobago 2007    1056608 69.81900  18008.5092\n1561                  Tunisia 1952    3647735 44.60000   1468.4756\n1562                  Tunisia 1957    3950849 47.10000   1395.2325\n1563                  Tunisia 1962    4286552 49.57900   1660.3032\n1564                  Tunisia 1967    4786986 52.05300   1932.3602\n1565                  Tunisia 1972    5303507 55.60200   2753.2860\n1566                  Tunisia 1977    6005061 59.83700   3120.8768\n1567                  Tunisia 1982    6734098 64.04800   3560.2332\n1568                  Tunisia 1987    7724976 66.89400   3810.4193\n1569                  Tunisia 1992    8523077 70.00100   4332.7202\n1570                  Tunisia 1997    9231669 71.97300   4876.7986\n1571                  Tunisia 2002    9770575 73.04200   5722.8957\n1572                  Tunisia 2007   10276158 73.92300   7092.9230\n1573                   Turkey 1952   22235677 43.58500   1969.1010\n1574                   Turkey 1957   25670939 48.07900   2218.7543\n1575                   Turkey 1962   29788695 52.09800   2322.8699\n1576                   Turkey 1967   33411317 54.33600   2826.3564\n1577                   Turkey 1972   37492953 57.00500   3450.6964\n1578                   Turkey 1977   42404033 59.50700   4269.1223\n1579                   Turkey 1982   47328791 61.03600   4241.3563\n1580                   Turkey 1987   52881328 63.10800   5089.0437\n1581                   Turkey 1992   58179144 66.14600   5678.3483\n1582                   Turkey 1997   63047647 68.83500   6601.4299\n1583                   Turkey 2002   67308928 70.84500   6508.0857\n1584                   Turkey 2007   71158647 71.77700   8458.2764\n1585                   Uganda 1952    5824797 39.97800    734.7535\n1586                   Uganda 1957    6675501 42.57100    774.3711\n1587                   Uganda 1962    7688797 45.34400    767.2717\n1588                   Uganda 1967    8900294 48.05100    908.9185\n1589                   Uganda 1972   10190285 51.01600    950.7359\n1590                   Uganda 1977   11457758 50.35000    843.7331\n1591                   Uganda 1982   12939400 49.84900    682.2662\n1592                   Uganda 1987   15283050 51.50900    617.7244\n1593                   Uganda 1992   18252190 48.82500    644.1708\n1594                   Uganda 1997   21210254 44.57800    816.5591\n1595                   Uganda 2002   24739869 47.81300    927.7210\n1596                   Uganda 2007   29170398 51.54200   1056.3801\n1597           United Kingdom 1952   50430000 69.18000   9979.5085\n1598           United Kingdom 1957   51430000 70.42000  11283.1779\n1599           United Kingdom 1962   53292000 70.76000  12477.1771\n1600           United Kingdom 1967   54959000 71.36000  14142.8509\n1601           United Kingdom 1972   56079000 72.01000  15895.1164\n1602           United Kingdom 1977   56179000 72.76000  17428.7485\n1603           United Kingdom 1982   56339704 74.04000  18232.4245\n1604           United Kingdom 1987   56981620 75.00700  21664.7877\n1605           United Kingdom 1992   57866349 76.42000  22705.0925\n1606           United Kingdom 1997   58808266 77.21800  26074.5314\n1607           United Kingdom 2002   59912431 78.47100  29478.9992\n1608           United Kingdom 2007   60776238 79.42500  33203.2613\n1609            United States 1952  157553000 68.44000  13990.4821\n1610            United States 1957  171984000 69.49000  14847.1271\n1611            United States 1962  186538000 70.21000  16173.1459\n1612            United States 1967  198712000 70.76000  19530.3656\n1613            United States 1972  209896000 71.34000  21806.0359\n1614            United States 1977  220239000 73.38000  24072.6321\n1615            United States 1982  232187835 74.65000  25009.5591\n1616            United States 1987  242803533 75.02000  29884.3504\n1617            United States 1992  256894189 76.09000  32003.9322\n1618            United States 1997  272911760 76.81000  35767.4330\n1619            United States 2002  287675526 77.31000  39097.0995\n1620            United States 2007  301139947 78.24200  42951.6531\n1621                  Uruguay 1952    2252965 66.07100   5716.7667\n1622                  Uruguay 1957    2424959 67.04400   6150.7730\n1623                  Uruguay 1962    2598466 68.25300   5603.3577\n1624                  Uruguay 1967    2748579 68.46800   5444.6196\n1625                  Uruguay 1972    2829526 68.67300   5703.4089\n1626                  Uruguay 1977    2873520 69.48100   6504.3397\n1627                  Uruguay 1982    2953997 70.80500   6920.2231\n1628                  Uruguay 1987    3045153 71.91800   7452.3990\n1629                  Uruguay 1992    3149262 72.75200   8137.0048\n1630                  Uruguay 1997    3262838 74.22300   9230.2407\n1631                  Uruguay 2002    3363085 75.30700   7727.0020\n1632                  Uruguay 2007    3447496 76.38400  10611.4630\n1633                Venezuela 1952    5439568 55.08800   7689.7998\n1634                Venezuela 1957    6702668 57.90700   9802.4665\n1635                Venezuela 1962    8143375 60.77000   8422.9742\n1636                Venezuela 1967    9709552 63.47900   9541.4742\n1637                Venezuela 1972   11515649 65.71200  10505.2597\n1638                Venezuela 1977   13503563 67.45600  13143.9510\n1639                Venezuela 1982   15620766 68.55700  11152.4101\n1640                Venezuela 1987   17910182 70.19000   9883.5846\n1641                Venezuela 1992   20265563 71.15000  10733.9263\n1642                Venezuela 1997   22374398 72.14600  10165.4952\n1643                Venezuela 2002   24287670 72.76600   8605.0478\n1644                Venezuela 2007   26084662 73.74700  11415.8057\n1645                  Vietnam 1952   26246839 40.41200    605.0665\n1646                  Vietnam 1957   28998543 42.88700    676.2854\n1647                  Vietnam 1962   33796140 45.36300    772.0492\n1648                  Vietnam 1967   39463910 47.83800    637.1233\n1649                  Vietnam 1972   44655014 50.25400    699.5016\n1650                  Vietnam 1977   50533506 55.76400    713.5371\n1651                  Vietnam 1982   56142181 58.81600    707.2358\n1652                  Vietnam 1987   62826491 62.82000    820.7994\n1653                  Vietnam 1992   69940728 67.66200    989.0231\n1654                  Vietnam 1997   76048996 70.67200   1385.8968\n1655                  Vietnam 2002   80908147 73.01700   1764.4567\n1656                  Vietnam 2007   85262356 74.24900   2441.5764\n1657       West Bank and Gaza 1952    1030585 43.16000   1515.5923\n1658       West Bank and Gaza 1957    1070439 45.67100   1827.0677\n1659       West Bank and Gaza 1962    1133134 48.12700   2198.9563\n1660       West Bank and Gaza 1967    1142636 51.63100   2649.7150\n1661       West Bank and Gaza 1972    1089572 56.53200   3133.4093\n1662       West Bank and Gaza 1977    1261091 60.76500   3682.8315\n1663       West Bank and Gaza 1982    1425876 64.40600   4336.0321\n1664       West Bank and Gaza 1987    1691210 67.04600   5107.1974\n1665       West Bank and Gaza 1992    2104779 69.71800   6017.6548\n1666       West Bank and Gaza 1997    2826046 71.09600   7110.6676\n1667       West Bank and Gaza 2002    3389578 72.37000   4515.4876\n1668       West Bank and Gaza 2007    4018332 73.42200   3025.3498\n1669               Yemen Rep. 1952    4963829 32.54800    781.7176\n1670               Yemen Rep. 1957    5498090 33.97000    804.8305\n1671               Yemen Rep. 1962    6120081 35.18000    825.6232\n1672               Yemen Rep. 1967    6740785 36.98400    862.4421\n1673               Yemen Rep. 1972    7407075 39.84800   1265.0470\n1674               Yemen Rep. 1977    8403990 44.17500   1829.7652\n1675               Yemen Rep. 1982    9657618 49.11300   1977.5570\n1676               Yemen Rep. 1987   11219340 52.92200   1971.7415\n1677               Yemen Rep. 1992   13367997 55.59900   1879.4967\n1678               Yemen Rep. 1997   15826497 58.02000   2117.4845\n1679               Yemen Rep. 2002   18701257 60.30800   2234.8208\n1680               Yemen Rep. 2007   22211743 62.69800   2280.7699\n1681                   Zambia 1952    2672000 42.03800   1147.3888\n1682                   Zambia 1957    3016000 44.07700   1311.9568\n1683                   Zambia 1962    3421000 46.02300   1452.7258\n1684                   Zambia 1967    3900000 47.76800   1777.0773\n1685                   Zambia 1972    4506497 50.10700   1773.4983\n1686                   Zambia 1977    5216550 51.38600   1588.6883\n1687                   Zambia 1982    6100407 51.82100   1408.6786\n1688                   Zambia 1987    7272406 50.82100   1213.3151\n1689                   Zambia 1992    8381163 46.10000   1210.8846\n1690                   Zambia 1997    9417789 40.23800   1071.3538\n1691                   Zambia 2002   10595811 39.19300   1071.6139\n1692                   Zambia 2007   11746035 42.38400   1271.2116\n1693                 Zimbabwe 1952    3080907 48.45100    406.8841\n1694                 Zimbabwe 1957    3646340 50.46900    518.7643\n1695                 Zimbabwe 1962    4277736 52.35800    527.2722\n1696                 Zimbabwe 1967    4995432 53.99500    569.7951\n1697                 Zimbabwe 1972    5861135 55.63500    799.3622\n1698                 Zimbabwe 1977    6642107 57.67400    685.5877\n1699                 Zimbabwe 1982    7636524 60.36300    788.8550\n1700                 Zimbabwe 1987    9216418 62.35100    706.1573\n1701                 Zimbabwe 1992   10704340 60.37700    693.4208\n1702                 Zimbabwe 1997   11404948 46.80900    792.4500\n1703                 Zimbabwe 2002   11926563 39.98900    672.0386\n1704                 Zimbabwe 2007   12311143 43.48700    469.7093\n\n\nNote that there are some other packages (e.g. the MASS package) that also have a function called select(). If you happen to load one of those packages after loading the tidyverse package in your session, you may end up with an error that says Error in select(., x) : unused argument (x). To fix this, you will either need to directly call the select() function from the dplyr package using dplyr::select() or ensure that you load such packages before the tidyverse package (which automatically loads the dplyr package)."
  },
  {
    "objectID": "blog/2019-08-05_base_r_to_tidyverse.html#filter-filter-to-rows-that-satisfy-certain-conditions",
    "href": "blog/2019-08-05_base_r_to_tidyverse.html#filter-filter-to-rows-that-satisfy-certain-conditions",
    "title": "Transitioning into the tidyverse (part 1)",
    "section": "filter: filter to rows that satisfy certain conditions",
    "text": "filter: filter to rows that satisfy certain conditions\nFiltering is a very simple way of only keeping rows that satisfy certain conditions. These conditions are always based on logical statements involving variables/columns of the data frame.\nFor instance, to keep only the rows that have a recorded population of at least 1 billion, you can use a filtering with a logical statement involving the pop variable (again unquoted).\n\ngapminder %&gt;% \n  filter(pop &gt; 1000000000) \n\n  country year        pop continent lifeExp gdpPercap\n1   China 1982 1000281000      Asia  65.525  962.4214\n2   China 1987 1084035000      Asia  67.274 1378.9040\n3   China 1992 1164970000      Asia  68.690 1655.7842\n4   China 1997 1230075000      Asia  70.426 2289.2341\n5   China 2002 1280400000      Asia  72.028 3119.2809\n6   China 2007 1318683096      Asia  72.961 4959.1149\n7   India 2002 1034172547      Asia  62.879 1746.7695\n8   India 2007 1110396331      Asia  64.698 2452.2104\n\n\nYou can specify multiple filter conditions using a comma (and in this case the filter function will return rows that satisfy all of the conditions specified). Below I filter to rows from 1992 that have a population of at least 100 million in that year.\n\ngapminder %&gt;% \n  filter(pop &gt; 100000000, year == 1992) \n\n        country year        pop continent lifeExp  gdpPercap\n1    Bangladesh 1992  113704579      Asia  56.018   837.8102\n2        Brazil 1992  155975974  Americas  67.057  6950.2830\n3         China 1992 1164970000      Asia  68.690  1655.7842\n4         India 1992  872000000      Asia  60.223  1164.4068\n5     Indonesia 1992  184816000      Asia  62.681  2383.1409\n6         Japan 1992  124329269      Asia  79.360 26824.8951\n7      Pakistan 1992  120065004      Asia  60.838  1971.8295\n8 United States 1992  256894189  Americas  76.090 32003.9322"
  },
  {
    "objectID": "blog/2019-08-05_base_r_to_tidyverse.html#mutate-add-a-new-variable",
    "href": "blog/2019-08-05_base_r_to_tidyverse.html#mutate-add-a-new-variable",
    "title": "Transitioning into the tidyverse (part 1)",
    "section": "mutate: add a new variable",
    "text": "mutate: add a new variable\nMutating the data frame involves adding a new variable. This new variable is usually a function of existing variables, but it can also be defined based on external objects.\nFor instance below I add a new variable, gdp, to the gapminder data frame. gdp is equal to gdpPercap multiplied by pop, and then look at the first 6 rows of the data frame using the classic head() function.\n\ngapminder %&gt;% \n  mutate(gdp = gdpPercap * pop) %&gt;%\n  head\n\n      country year      pop continent lifeExp gdpPercap         gdp\n1 Afghanistan 1952  8425333      Asia  28.801  779.4453  6567086330\n2 Afghanistan 1957  9240934      Asia  30.332  820.8530  7585448670\n3 Afghanistan 1962 10267083      Asia  31.997  853.1007  8758855797\n4 Afghanistan 1967 11537966      Asia  34.020  836.1971  9648014150\n5 Afghanistan 1972 13079460      Asia  36.088  739.9811  9678553274\n6 Afghanistan 1977 14880372      Asia  38.438  786.1134 11697659231\n\n\nNote that I haven’t re-defined the gapminder data frame, so all I have done here is print out the data frame with the additional gdp variable.\nIf you wanted to be able to use this gdp variable down the line, you would need to re-define the gapminder data frame so that gapminder now corresponds to the version with the gdp variable.\n\ngapminder &lt;- gapminder %&gt;% \n  mutate(gdp = gdpPercap * pop)"
  },
  {
    "objectID": "blog/2019-08-05_base_r_to_tidyverse.html#arrange-arrange-the-rows-of-the-data-frame-in-order-a-variable",
    "href": "blog/2019-08-05_base_r_to_tidyverse.html#arrange-arrange-the-rows-of-the-data-frame-in-order-a-variable",
    "title": "Transitioning into the tidyverse (part 1)",
    "section": "arrange: arrange the rows of the data frame in order a variable",
    "text": "arrange: arrange the rows of the data frame in order a variable\nThe arrange function allows you to easily reorder the rows of the data frame in increasing or decreasing order of one (or more) of the variables of the data frame.\nFor instance, you could arrange all rows in the data frame in order of increasing life expectancy.\n\ngapminder %&gt;% \n  arrange(lifeExp) %&gt;%\n  head\n\n       country year     pop continent lifeExp gdpPercap         gdp\n1       Rwanda 1992 7290203    Africa  23.599  737.0686  5373379682\n2  Afghanistan 1952 8425333      Asia  28.801  779.4453  6567086330\n3       Gambia 1952  284320    Africa  30.000  485.2307   137960781\n4       Angola 1952 4232095    Africa  30.015 3520.6103 14899557133\n5 Sierra Leone 1952 2143249    Africa  30.331  879.7877  1885604185\n6  Afghanistan 1957 9240934      Asia  30.332  820.8530  7585448670\n\n\nTo arrange in descending order, you need to wrap the variable name in the desc() function.\n\ngapminder %&gt;% \n  arrange(desc(gdpPercap)) %&gt;%\n  head\n\n  country year     pop continent lifeExp gdpPercap         gdp\n1  Kuwait 1957  212846      Asia  58.033 113523.13 24162944745\n2  Kuwait 1972  841934      Asia  67.712 109347.87 92063687055\n3  Kuwait 1952  160000      Asia  55.565 108382.35 17341176464\n4  Kuwait 1962  358266      Asia  60.470  95458.11 34199395868\n5  Kuwait 1967  575003      Asia  64.624  80894.88 46514800559\n6  Kuwait 1977 1140357      Asia  69.343  59265.48 67583801715\n\n\nAgain, if you wanted your data frame to actually be arranged as specified, you would need to re-define the gapminder data frame. But if you only need it for one quick analysis (e.g. creating a summary table), then you don’t need to redefine the data frame.\nBelow I re-define the gapminder dataset so that the rows are in order of increasing year, and the countries are in alphabetical order within each year (the secondary arrange variable).\n\ngapminder %&gt;% \n  arrange(year, country) %&gt;% \n  head\n\n      country year      pop continent lifeExp  gdpPercap          gdp\n1 Afghanistan 1952  8425333      Asia  28.801   779.4453   6567086330\n2     Albania 1952  1282697    Europe  55.230  1601.0561   2053669902\n3     Algeria 1952  9279525    Africa  43.077  2449.0082  22725632678\n4      Angola 1952  4232095    Africa  30.015  3520.6103  14899557133\n5   Argentina 1952 17876956  Americas  62.485  5911.3151 105676319105\n6   Australia 1952  8691212   Oceania  69.120 10039.5956  87256254102"
  },
  {
    "objectID": "blog/2019-08-05_base_r_to_tidyverse.html#group_by-apply-other-dplyr-functions-separately-within-within-a-group-defined-by-one-or-more-variables",
    "href": "blog/2019-08-05_base_r_to_tidyverse.html#group_by-apply-other-dplyr-functions-separately-within-within-a-group-defined-by-one-or-more-variables",
    "title": "Transitioning into the tidyverse (part 1)",
    "section": "group_by: apply other dplyr functions separately within within a group defined by one or more variables",
    "text": "group_by: apply other dplyr functions separately within within a group defined by one or more variables\nThe group_by() function can be really useful if you want to apply a function independently within groups of observations (where the groups are specified by a categorical variable in your data frame). Think of group_by() as splitting your data frame into several separate data frames based on the categorical variable you specify. All functions that you apply to the grouped data frame are applied separately to each group until you specify an ungroup() function.\nThe code below filters the data frame to only the country-years that have life expectancy above the average life expectancy for their continent.\n\ngapminder %&gt;%\n  group_by(continent) %&gt;%\n  filter(lifeExp &gt; mean(lifeExp)) %&gt;%\n  ungroup() \n\n# A tibble: 873 × 7\n   country  year      pop continent lifeExp gdpPercap           gdp\n   &lt;chr&gt;   &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;\n 1 Albania  1987  3075321 Europe       72       3739.  11498418358.\n 2 Albania  1997  3428038 Europe       73.0     3193.  10945912519.\n 3 Albania  2002  3508512 Europe       75.7     4604.  16153932130.\n 4 Albania  2007  3600523 Europe       76.4     5937.  21376411360.\n 5 Algeria  1967 12760499 Africa       51.4     3247.  41433235247.\n 6 Algeria  1972 14760787 Africa       54.5     4183.  61739408943.\n 7 Algeria  1977 17152804 Africa       58.0     4910.  84227416174.\n 8 Algeria  1982 20033753 Africa       61.4     5745. 115097120653.\n 9 Algeria  1987 23254956 Africa       65.8     5681. 132119742845.\n10 Algeria  1992 26298373 Africa       67.7     5023. 132102425043.\n# … with 863 more rows\n\n\nTo check that this does something different than it would without the group_by() (i.e. filtering to the country-years that have life expectancy above the average global life expectancy), compare the distribution of continents from each filter() command using the count() function (another handly dplyr function):\nThe number of countries from each continent included post-filtering when grouping by continent is:\n\ngapminder %&gt;%\n  group_by(continent) %&gt;%\n  filter(lifeExp &gt; mean(lifeExp)) %&gt;%\n  ungroup() %&gt;%\n  count(continent)\n\n# A tibble: 5 × 2\n  continent     n\n  &lt;chr&gt;     &lt;int&gt;\n1 Africa      282\n2 Americas    176\n3 Asia        216\n4 Europe      189\n5 Oceania      10\n\n\nThe number of countries from each continent included post-filtering when not grouping by continent is:\n\ngapminder %&gt;%\n  filter(lifeExp &gt; mean(lifeExp)) %&gt;%\n  count(continent)\n\n  continent   n\n1    Africa  80\n2  Americas 218\n3      Asia 224\n4    Europe 349\n5   Oceania  24\n\n\nNotice that when you don’t group by continent, substantially fewer African countries are included since they tend to have lower life expectencies than the global average.\nTo combine some of the things you’ve just learnt, the code below first filters to the year 2007, and then splits the data frame into groups by continent and adds a row to each group corresponding to the average life expectancy of all of the countries in that group/continent.\n\ngapminder %&gt;% \n  # first filter to 2007\n  filter(year == 2007) %&gt;%\n  # group by continent\n  group_by(continent) %&gt;%\n  # add a column within each continent corresponding to the average life expectancy\n  mutate(continent_lifeExp = mean(lifeExp)) %&gt;%\n  # ungroup the data frame\n  ungroup() %&gt;% \n  # only show a few variables\n  select(country, continent, lifeExp, continent_lifeExp) %&gt;%\n  head(10)\n\n# A tibble: 10 × 4\n   country     continent lifeExp continent_lifeExp\n   &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt;             &lt;dbl&gt;\n 1 Afghanistan Asia         43.8              70.7\n 2 Albania     Europe       76.4              77.6\n 3 Algeria     Africa       72.3              54.8\n 4 Angola      Africa       42.7              54.8\n 5 Argentina   Americas     75.3              73.6\n 6 Australia   Oceania      81.2              80.7\n 7 Austria     Europe       79.8              77.6\n 8 Bahrain     Asia         75.6              70.7\n 9 Bangladesh  Asia         64.1              70.7\n10 Belgium     Europe       79.4              77.6\n\n\nNotice that all rows from the same continent have the same value for continent_lifeExp. Note that even though this example defines a single value for each continent, this value is repeated for all rows within the continent."
  },
  {
    "objectID": "blog/2019-08-05_base_r_to_tidyverse.html#summarisesummarize-define-a-variable-that-is-a-summary-of-other-variables",
    "href": "blog/2019-08-05_base_r_to_tidyverse.html#summarisesummarize-define-a-variable-that-is-a-summary-of-other-variables",
    "title": "Transitioning into the tidyverse (part 1)",
    "section": "summarise/summarize: define a variable that is a summary of other variables",
    "text": "summarise/summarize: define a variable that is a summary of other variables\nThe summarise() (or summarize()) function aggregates across the rows of the data frame. For instance, you can calculate the average life expectancy, as well as the total GDP.\n\ngapminder %&gt;% \n  summarise(mean_lifeExp = mean(lifeExp),\n            total_gdp = sum(gdp))\n\n  mean_lifeExp    total_gdp\n1     59.47444 3.183235e+14\n\n\nThe summarise function plays very nicely with the group_by() function. For instance, by first grouping by year and then calculating the average life expectancy and total GDP for each year.\n\ngapminder %&gt;% \n  group_by(year) %&gt;%\n  summarise(mean_lifeExp = mean(lifeExp),\n            total_gdp = sum(gdp)) \n\n# A tibble: 12 × 3\n    year mean_lifeExp total_gdp\n   &lt;int&gt;        &lt;dbl&gt;     &lt;dbl&gt;\n 1  1952         49.1   7.04e12\n 2  1957         51.5   8.90e12\n 3  1962         53.6   1.10e13\n 4  1967         55.7   1.42e13\n 5  1972         57.6   1.84e13\n 6  1977         59.6   2.23e13\n 7  1982         61.5   2.54e13\n 8  1987         63.2   3.01e13\n 9  1992         64.2   3.45e13\n10  1997         65.0   4.10e13\n11  2002         65.7   4.73e13\n12  2007         67.0   5.81e13\n\n\nNote that since these are summaries of the data frame itself, I just want to print them out, rather than re-defining the gapminder data frame to be equal to these summaries. And since I won’t be using them for anything other than to look at, I don’t need to define them as a variable."
  },
  {
    "objectID": "blog/2019-08-05_base_r_to_tidyverse.html#more-dplyr-functions",
    "href": "blog/2019-08-05_base_r_to_tidyverse.html#more-dplyr-functions",
    "title": "Transitioning into the tidyverse (part 1)",
    "section": "More dplyr functions",
    "text": "More dplyr functions\nOther dplyr functions that are incredibly useful include:\nrename() for renaming variables of the data frame\n\ngapminder %&gt;%\n  rename(gdp_per_capita = gdpPercap,\n         life_exp = lifeExp) %&gt;%\n  head\n\n      country year      pop continent life_exp gdp_per_capita         gdp\n1 Afghanistan 1952  8425333      Asia   28.801       779.4453  6567086330\n2 Afghanistan 1957  9240934      Asia   30.332       820.8530  7585448670\n3 Afghanistan 1962 10267083      Asia   31.997       853.1007  8758855797\n4 Afghanistan 1967 11537966      Asia   34.020       836.1971  9648014150\n5 Afghanistan 1972 13079460      Asia   36.088       739.9811  9678553274\n6 Afghanistan 1977 14880372      Asia   38.438       786.1134 11697659231\n\n\ndistinct() for extracting the distinct values of a variable\n\ngapminder %&gt;% \n  distinct(continent)\n\n  continent\n1      Asia\n2    Europe\n3    Africa\n4  Americas\n5   Oceania\n\n\nsample_n() and sample_frac() for taking random samples of rows\n\ngapminder %&gt;% \n  sample_n(2)\n\n  country year     pop continent lifeExp gdpPercap        gdp\n1 Eritrea 1977 2512642    Africa  44.535  505.7538 1270778259\n2 Lesotho 1957  813338    Africa  45.047  335.9971  273279222\n\n\ncount() for counting the number of rows with each value of a categorical variable\n\ngapminder %&gt;% \n  count(continent)\n\n  continent   n\n1    Africa 624\n2  Americas 300\n3      Asia 396\n4    Europe 360\n5   Oceania  24\n\n\ntransmute() for doing a mutate and select at the same time: only the variables defined in the mutation are retained.\n\ngapminder %&gt;% \n  transmute(gdp = gdpPercap * pop) %&gt;%\n  head\n\n          gdp\n1  6567086330\n2  7585448670\n3  8758855797\n4  9648014150\n5  9678553274\n6 11697659231\n\n\nAdvanced dplyr practitioners will eventually want to learn about scoped verbs."
  },
  {
    "objectID": "blog/2019-08-05_base_r_to_tidyverse.html#adding-geom-layers",
    "href": "blog/2019-08-05_base_r_to_tidyverse.html#adding-geom-layers",
    "title": "Transitioning into the tidyverse (part 1)",
    "section": "Adding geom layers",
    "text": "Adding geom layers\nNext, I will add a “geom” layer to our ggplot object.\nLayers are added to ggplot objects using +, instead of %&gt;%, since you are not explicitly piping the LHS into each subsequent layer (we are actually adding a layer on top). The error messages have recently been improved to warn you if you are accidentally using a pipe %&gt;% to add layers to ggplot objects (which, once you start piping everything into everything, becomes an easy mistake to make).\nProbably the most common geom layer is geom_point. Inside geom_point(), you will specify the aesthetic mappings from the variables to the geometric objects that you want. For instance, if you want to plot a scatterplot with gdpPercap on the x-axis and lifeExp on the y-axis, then you would add a geom_point() geometric layer with relevant aesthetic function: geom_point(aes(x = gdpPercap, y = lifeExp)).\n\n# describe the base ggplot object and tell it what data we are interested in along with the aesthetic mapping\ngapminder %&gt;%\n  filter(year == 2007) %&gt;%\n  ggplot() +\n  # add a points layer on top\n  geom_point(aes(x = gdpPercap, y = lifeExp))\n\n\n\n\n\n\n\n\nWe could also add a smoothed trend line layer on top of the points using geom_smooth().\n\n# describe the base ggplot object and tell it what data we are interested in along with the aesthetic mapping\ngapminder %&gt;%\n  filter(year == 2007) %&gt;%\n  ggplot() +\n  # add a points layer on top\n  geom_point(aes(x = gdpPercap, y = lifeExp)) +\n  # add a smoothed LOESS layer\n  geom_smooth(aes(x = gdpPercap, y = lifeExp), method = \"loess\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nNote that since the aesthetics for geom_point() and geom_smooth() are the same, you might want to just specify global aesthetics in the ggplot() function, rather than layer-specific aesthetics.\n\n# describe the base ggplot object and tell it what data we are interested in along with the aesthetic mapping\ngapminder %&gt;%\n  filter(year == 2007) %&gt;%\n  # specify global aesthetic mappings\n  ggplot(aes(x = gdpPercap, y = lifeExp)) +\n  # add a points layer on top\n  geom_point() +\n  # add a smoothed LOESS layer\n  geom_smooth(method = \"loess\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nWe could also combine a points geom layer with a line geom layer, or any other type of geom layer. Line plots work well for plotting time series, so below we plot the average life expectancy over time using both point and line layers.\nHere you can do some clever combinations of dplyr manipulations with ggplot2 by summarising life expectancy by year and piping the results into a ggplot without having to define any intermediate variables.\n\ngapminder %&gt;%\n  # calcualte the average life expectency for each year\n  group_by(year) %&gt;%\n  summarise(avg_lifeExp = mean(lifeExp)) %&gt;%\n  ungroup() %&gt;%\n  # specify global aesthetic mappings\n  ggplot(aes(x = year, y = avg_lifeExp)) +\n  # add a points layer on top\n  geom_point() +\n  # add a line layer on top\n  geom_line()\n\n\n\n\n\n\n\n\nIf you wanted to have a separate line on our plot for each continent (rather than an aggregated line across all continents), you don’t need to add an individual layer for each continent to get the following plot:\n\n\n`summarise()` has grouped output by 'continent'. You can override using the\n`.groups` argument.\n\n\n\n\n\n\n\n\n\nInstead, start by also grouping by continent when you calculate the average life expectency by year.\n\ngapminder %&gt;%\n  group_by(continent, year) %&gt;%\n  summarise(avg_lifeExp = mean(lifeExp))\n\n`summarise()` has grouped output by 'continent'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 60 × 3\n# Groups:   continent [5]\n   continent  year avg_lifeExp\n   &lt;chr&gt;     &lt;int&gt;       &lt;dbl&gt;\n 1 Africa     1952        39.1\n 2 Africa     1957        41.3\n 3 Africa     1962        43.3\n 4 Africa     1967        45.3\n 5 Africa     1972        47.5\n 6 Africa     1977        49.6\n 7 Africa     1982        51.6\n 8 Africa     1987        53.3\n 9 Africa     1992        53.6\n10 Africa     1997        53.6\n# … with 50 more rows\n\n\nHowever if you try to use the same code as above to plot a line on the country-year grouped data frame, you get a weird zig-zag pattern.\n\ngapminder %&gt;%\n  group_by(continent, year) %&gt;%\n  summarise(avg_lifeExp = mean(lifeExp)) %&gt;%\n  ungroup() %&gt;%\n  ggplot() +\n  # add a points layer on top\n  geom_point(aes(x = year, y = avg_lifeExp)) +\n  # add a lines layer ontop\n  geom_line(aes(x = year, y = avg_lifeExp))\n\n`summarise()` has grouped output by 'continent'. You can override using the\n`.groups` argument.\n\n\n\n\n\n\n\n\n\nThis happens because you now have multiple average life expectancy values for each year, but you haven’t specified which ones go together. To fix this plot, you need to specify how the rows are grouped together (i.e. which variable defines the individual lines) by specifying the group = continent argument in the aes() function of the geom_line() layer.\n\ngapminder %&gt;%\n  group_by(continent, year) %&gt;%\n  summarise(avg_lifeExp = mean(lifeExp)) %&gt;%\n  ggplot() +\n  # add a points layer on top\n  geom_point(aes(x = year, y = avg_lifeExp)) +\n  # add a lines layer on top that is grouped by continent\n  geom_line(aes(x = year, y = avg_lifeExp, group = continent))\n\n`summarise()` has grouped output by 'continent'. You can override using the\n`.groups` argument."
  },
  {
    "objectID": "blog/2019-08-05_base_r_to_tidyverse.html#more-aesthetic-mappings-based-on-variables",
    "href": "blog/2019-08-05_base_r_to_tidyverse.html#more-aesthetic-mappings-based-on-variables",
    "title": "Transitioning into the tidyverse (part 1)",
    "section": "More aesthetic mappings based on variables",
    "text": "More aesthetic mappings based on variables\nSo far we have only specified the x- and y-position aesthetic mappings from the data to the geom objects. But you can also specify other types of aesthetic mappings, such as using a variable to specify the colour of the points.\nIf you want all of the points to be the same colour, you can specify a global point colour argument (that lies outside the aes() function).\n\ngapminder %&gt;%\n  ggplot() +\n  geom_point(aes(x = gdpPercap, y = lifeExp),\n             col = \"cornflowerblue\")\n\n\n\n\n\n\n\n\nHowever, if you wanted to use a variable from the data frame to define the colour (or any other aesthetic feature) of the geoms, you will need to include it inside the aes() function.\n\ngapminder %&gt;%\n  ggplot() +\n  geom_point(aes(x = gdpPercap, \n                 y = lifeExp, \n                 col  = continent))\n\n\n\n\n\n\n\n\nNote that the continent variable does not specify the colours themselves: this is done automatically. You can specify the colours you want yourself by adding a scale layer for colour.\n\ngapminder %&gt;%\n  ggplot() +\n  geom_point(aes(x = gdpPercap, \n                 y = lifeExp, \n                 col  = continent)) +\n  scale_colour_manual(values = c(\"orange\", \"red4\", \"purple\", \"darkgreen\", \"blue\"))\n\n\n\n\n\n\n\n\nThere are lots of types of scales that you can use for every type of aesthetic mapping (including x- and y-positions), and typically scales are specific to whether your variable using in the aesthetic mapping is discrete or continuous.\nWe could also add aesthetic mappings for other features such as shape, size, transparency (alpha) and more! For example, changing the size based on population:\n\ngapminder %&gt;%\n  ggplot() +\n  geom_point(aes(x = gdpPercap, y = lifeExp, \n                 col = continent, size = pop),\n             alpha = 0.5)\n\n\n\n\n\n\n\n\nFor the line plot example above where we plotted an average life expectancy time line for each continent, instead of specifying a group argument, you could instead specify a colour argument to be continent. This will will automatically group and colour by continent.\n\ngapminder %&gt;%\n  group_by(continent, year) %&gt;%\n  summarise(avg_lifeExp = mean(lifeExp)) %&gt;%\n  # specify global aesthetic mappings\n  ggplot() +\n  # add a points layer on top\n  geom_line(aes(x = year, y = avg_lifeExp, colour = continent))\n\n`summarise()` has grouped output by 'continent'. You can override using the\n`.groups` argument."
  },
  {
    "objectID": "blog/2019-08-05_base_r_to_tidyverse.html#other-types-of-layers",
    "href": "blog/2019-08-05_base_r_to_tidyverse.html#other-types-of-layers",
    "title": "Transitioning into the tidyverse (part 1)",
    "section": "Other types of layers",
    "text": "Other types of layers\nSo far, we have only seen scatterplots (points) and line plots, however, there are many other geoms you could add, including:\n\nHistograms\nHistograms only require an x-aesthetic (the y-aesthetic is a count by default, but you can force it to be a density by specifying y = ..density..).\n\ngapminder %&gt;%\n  ggplot() + \n  geom_histogram(aes(x = lifeExp), binwidth = 3)\n\n\n\n\n\n\n\n\n\n\nBoxplots\nBoxplots are automatically grouped by the x-aesthetic provided (e.g. continent in the plot below). To colour boxplots, use the fill argument instead of the col (or color/colour) argument.\n\ngapminder %&gt;%\n  ggplot() +\n  geom_boxplot(aes(x = continent, y = lifeExp, fill = continent))\n\n\n\n\n\n\n\n\n\n\nFaceting\nYou can create a grid (or “facet”) of plots separated by a categorical variable of your choosing (e.g. continent) by adding a facet layer.\n\ngapminder %&gt;%\n  ggplot() +\n  geom_point(aes(x = gdpPercap, y = lifeExp)) +\n  facet_wrap(~continent, ncol = 2)"
  },
  {
    "objectID": "blog/2019-08-05_base_r_to_tidyverse.html#customizing-ggplot2",
    "href": "blog/2019-08-05_base_r_to_tidyverse.html#customizing-ggplot2",
    "title": "Transitioning into the tidyverse (part 1)",
    "section": "Customizing ggplot2",
    "text": "Customizing ggplot2\nWhile we have stayed within the default ggplot2 functionalities here, there is a lot you can do with ggplot2. For instance, with practice, you will learn how to produce highly-customized plots by combining many layers together. As motivation, here is a much more beautiful plot that can be made with ggplot2:\n\ngapminder %&gt;% \n  filter(year == 2007) %&gt;%\n  ggplot() +\n  # add scatter points\n  geom_point(aes(x = gdpPercap, y = lifeExp, col = continent, size = pop),\n             alpha = 0.5) +\n  # add some text annotations for the very large countries\n  geom_text(aes(x = gdpPercap, y = lifeExp + 3, label = country),\n            col = \"grey50\",\n            data = filter(gapminder, year == 2007, pop &gt; 1000000000 | country %in% c(\"Nigeria\", \"United States\"))) +\n  # clean the axes names and breaks\n  scale_x_log10(limits = c(200, 60000)) +\n  # change labels\n  labs(title = \"GDP versus life expectancy in 2007\",\n       x = \"GDP per capita (log scale)\",\n       y = \"Life expectancy\",\n       size = \"Population\",\n       col = \"Continent\") +\n  # change the size scale\n  scale_size(range = c(0.1, 10),\n             # remove size legend\n             guide = \"none\") +\n  # add a nicer theme\n  theme_classic() +\n  # place legend at top and grey axis lines\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\nIn part two of this post on the tidyverse, you will see some ggplot2 code (under the guise of learning about factors) that makes this plot:\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\nIf you’d like to learn more about ggplot2, such as themes, scales and advanced geoms, check out my more detailed ggplot2 blog post.\nIf the tidyverse is new to you, I suggest that you stop here for now. Focus on incorporating piping, dplyr, and ggplot2 into every analysis that you do for the next few months (even if it would initially be quicker to use base R versions). When you feel comfortable with your new skills, move onto part two of this blog post and start to incorporate the remaining tidyverse packages (below) into your analytic workflow. Trying to learn everything at once is a sure-fire way to become discouraged. First get comfortable with the main ideas, then learn some more."
  },
  {
    "objectID": "blog/2017-11-17-caret_tutorial.html",
    "href": "blog/2017-11-17-caret_tutorial.html",
    "title": "A basic tutorial of caret: the machine learning package in R",
    "section": "",
    "text": "Note: If you’re new to caret, I suggest learning tidymodels instead http://www.rebeccabarter.com/blog/2020-03-25_machine_learning/. Tidymodels is essentially caret’s successor. Don’t worry though, your caret code will still work!\nOlder note: This tutorial was based on an older version of the abalone data that had a binary old varibale rather than a numeric age variable. It has been modified lightly so that it uses a manual old variable (is the abalone older than 10 or not) and ignores the numeric age variable.\nMaterials prepared by Rebecca Barter. Package developed by Max Kuhn.\nAn interactive Jupyter Notebook version of this tutorial can be found at https://github.com/rlbarter/STAT-215A-Fall-2017/tree/master/week11. Feel free to download it and use for your own learning or teaching adventures!\nR has a wide number of packages for machine learning (ML), which is great, but also quite frustrating since each package was designed independently and has very different syntax, inputs and outputs.\nThis means that if you want to do machine learning in R, you have to learn a large number of separate methods.\nRecognizing this, Max Kuhn (at the time working in drug discovery at Pfizer, now at RStudio) put together a single package for performing any machine learning method you like. This package is called caret. Caret stands for Classification And Regression Training. Apparently caret has little to do with our orange friend, the carrot.\nNot only does caret allow you to run a plethora of ML methods, it also provides tools for auxiliary techniques such as:\nAn extensive vignette for caret can be found here: https://topepo.github.io/caret/index.html"
  },
  {
    "objectID": "blog/2017-11-17-caret_tutorial.html#a-simple-view-of-caret-the-default-train-function",
    "href": "blog/2017-11-17-caret_tutorial.html#a-simple-view-of-caret-the-default-train-function",
    "title": "A basic tutorial of caret: the machine learning package in R",
    "section": "A simple view of caret: the default train function",
    "text": "A simple view of caret: the default train function\nTo implement your machine learning model of choice using caret you will use the train function. The types of modeling options available are many and are listed here: https://topepo.github.io/caret/available-models.html. In the example below, we will use the ranger implementation of random forest to predict whether abalone are “old” or not based on a bunch of physical properties of the abalone (sex, height, weight, diameter, etc). The abalone data came from the UCI Machine Learning repository (we split the data into a training and test set).\nFirst we load the data into R:\n\n# load in packages\nlibrary(caret)\nlibrary(ranger)\nlibrary(tidyverse)\nlibrary(e1071)\n# load in abalone dataset\nabalone_data &lt;- read.table(\"data/abalone.data\", sep = \",\")\n# load in column names\ncolnames(abalone_data) &lt;- c(\"sex\", \"length\", \"diameter\", \"height\", \n                            \"whole.weight\", \"shucked.weight\", \n                            \"viscera.weight\", \"shell.weight\", \"age\")\n# add a logical variable for \"old\" (age &gt; 10)\nabalone_data &lt;- abalone_data %&gt;%\n  mutate(old = age &gt; 10) %&gt;%\n  # remove the \"age\" variable\n  select(-age)\n# split into training and testing\nset.seed(23489)\ntrain_index &lt;- sample(1:nrow(abalone_data), 0.9 * nrow(abalone_data))\nabalone_train &lt;- abalone_data[train_index, ]\nabalone_test &lt;- abalone_data[-train_index, ]\n# remove the original dataset\nrm(abalone_data)\n# view the first 6 rows of the training data\nhead(abalone_train)\n\n     sex length diameter height whole.weight shucked.weight viscera.weight\n232    M  0.565    0.440  0.175       0.9025         0.3100         0.1930\n3906   M  0.380    0.270  0.095       0.2190         0.0835         0.0515\n1179   F  0.650    0.500  0.190       1.4640         0.6415         0.3390\n2296   F  0.520    0.415  0.145       0.8045         0.3325         0.1725\n1513   F  0.650    0.500  0.160       1.3825         0.7020         0.3040\n1023   F  0.640    0.500  0.170       1.5175         0.6930         0.3260\n     shell.weight   old\n232        0.3250  TRUE\n3906       0.0700 FALSE\n1179       0.4245 FALSE\n2296       0.2850 FALSE\n1513       0.3195 FALSE\n1023       0.4090  TRUE\n\n\nIt looks like we have 3,759 abalone:\n\ndim(abalone_train)\n\n[1] 3759    9\n\n\nTime to fit a random forest model using caret. Anytime we want to fit a model using train we tell it which model to fit by providing a formula for the first argument (as.factor(old) ~ . means that we want to model old as a function of all of the other variables). Then we need to provide a method (we specify \"ranger\" to implement randomForest).\n\n# fit a random forest model (using ranger)\nrf_fit &lt;- train(as.factor(old) ~ ., \n                data = abalone_train, \n                method = \"ranger\")\n\nBy default, the train function without any arguments re-runs the model over 25 bootstrap samples and across 3 options of the tuning parameter (the tuning parameter for ranger is mtry; the number of randomly selected predictors at each cut in the tree).\n\nrf_fit\n\nRandom Forest \n\n3759 samples\n   8 predictor\n   2 classes: 'FALSE', 'TRUE' \n\nNo pre-processing\nResampling: Bootstrapped (25 reps) \nSummary of sample sizes: 3759, 3759, 3759, 3759, 3759, 3759, ... \nResampling results across tuning parameters:\n\n  mtry  splitrule   Accuracy   Kappa    \n  2     gini        0.7794339  0.4982012\n  2     extratrees  0.7788261  0.4867672\n  5     gini        0.7722038  0.4853445\n  5     extratrees  0.7784925  0.4974177\n  9     gini        0.7665692  0.4738511\n  9     extratrees  0.7759596  0.4933252\n\nTuning parameter 'min.node.size' was held constant at a value of 1\nAccuracy was used to select the optimal model using the largest value.\nThe final values used for the model were mtry = 2, splitrule = gini\n and min.node.size = 1.\n\n\nTo test the data on an independent test set is equally as simple using the inbuilt predict function.\n\n# predict the outcome on a test set\nabalone_rf_pred &lt;- predict(rf_fit, abalone_test)\n# compare predicted outcome and true outcome\nconfusionMatrix(abalone_rf_pred, as.factor(abalone_test$old))\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction FALSE TRUE\n     FALSE   229   60\n     TRUE     33   96\n                                          \n               Accuracy : 0.7775          \n                 95% CI : (0.7346, 0.8165)\n    No Information Rate : 0.6268          \n    P-Value [Acc &gt; NIR] : 2.672e-11       \n                                          \n                  Kappa : 0.5072          \n                                          \n Mcnemar's Test P-Value : 0.007016        \n                                          \n            Sensitivity : 0.8740          \n            Specificity : 0.6154          \n         Pos Pred Value : 0.7924          \n         Neg Pred Value : 0.7442          \n             Prevalence : 0.6268          \n         Detection Rate : 0.5478          \n   Detection Prevalence : 0.6914          \n      Balanced Accuracy : 0.7447          \n                                          \n       'Positive' Class : FALSE"
  },
  {
    "objectID": "blog/2017-11-17-caret_tutorial.html#pre-processing-preprocess",
    "href": "blog/2017-11-17-caret_tutorial.html#pre-processing-preprocess",
    "title": "A basic tutorial of caret: the machine learning package in R",
    "section": "Pre-processing (preProcess)",
    "text": "Pre-processing (preProcess)\nThere are a number of pre-processing steps that are easily implemented by caret. Several stand-alone functions from caret target specific issues that might arise when setting up the model. These include\n\ndummyVars: creating dummy variables from categorical variables with multiple categories\nnearZeroVar: identifying zero- and near zero-variance predictors (these may cause issues when subsampling)\nfindCorrelation: identifying correlated predictors\nfindLinearCombos: identify linear dependencies between predictors\n\nIn addition to these individual functions, there also exists the preProcess function which can be used to perform more common tasks such as centering and scaling, imputation and transformation. preProcess takes in a data frame to be processed and a method which can be any of “BoxCox”, “YeoJohnson”, “expoTrans”, “center”, “scale”, “range”, “knnImpute”, “bagImpute”, “medianImpute”, “pca”, “ica”, “spatialSign”, “corr”, “zv”, “nzv”, and “conditionalX”.\n\n# center, scale and perform a YeoJohnson transformation\n# identify and remove variables with near zero variance\n# perform pca\nabalone_no_nzv_pca &lt;- preProcess(select(abalone_train, - old), \n                        method = c(\"center\", \"scale\", \"nzv\", \"pca\"))\nabalone_no_nzv_pca\n\nCreated from 3759 samples and 8 variables\n\nPre-processing:\n  - centered (7)\n  - ignored (1)\n  - principal component signal extraction (7)\n  - scaled (7)\n\nPCA needed 3 components to capture 95 percent of the variance\n\n\n\n# identify which variables were ignored, centered, scaled, etc\nabalone_no_nzv_pca$method\n\n$center\n[1] \"length\"         \"diameter\"       \"height\"         \"whole.weight\"  \n[5] \"shucked.weight\" \"viscera.weight\" \"shell.weight\"  \n\n$scale\n[1] \"length\"         \"diameter\"       \"height\"         \"whole.weight\"  \n[5] \"shucked.weight\" \"viscera.weight\" \"shell.weight\"  \n\n$pca\n[1] \"length\"         \"diameter\"       \"height\"         \"whole.weight\"  \n[5] \"shucked.weight\" \"viscera.weight\" \"shell.weight\"  \n\n$ignore\n[1] \"sex\"\n\n\n\n# identify the principal components\nabalone_no_nzv_pca$rotation\n\n                      PC1         PC2        PC3\nlength         -0.3835950  0.01308476 -0.5915192\ndiameter       -0.3838966  0.03978406 -0.5874657\nheight         -0.3458509  0.88289420  0.2793599\nwhole.weight   -0.3910710 -0.22191114  0.2394200\nshucked.weight -0.3784382 -0.33048177  0.2601988\nviscera.weight -0.3819522 -0.23798574  0.2841819\nshell.weight   -0.3792439 -0.06036456  0.1454731"
  },
  {
    "objectID": "blog/2017-11-17-caret_tutorial.html#data-splitting-createdatapartition-and-groupkfold",
    "href": "blog/2017-11-17-caret_tutorial.html#data-splitting-createdatapartition-and-groupkfold",
    "title": "A basic tutorial of caret: the machine learning package in R",
    "section": "Data splitting (createDataPartition and groupKFold)",
    "text": "Data splitting (createDataPartition and groupKFold)\nGenerating subsets of the data is easy with the createDataPartition function. While this function can be used to simply generate training and testing sets, it can also be used to subset the data while respecting important groupings that exist within the data.\nFirst, we show an example of performing general sample splitting to generate 10 different 80% subsamples.\n\n# identify the indices of 10 80% subsamples of the iris data\ntrain_index &lt;- createDataPartition(iris$Species,\n                                   p = 0.8,\n                                   list = FALSE,\n                                   times = 10)\n\n\n# look at the first 6 indices of each subsample\nhead(train_index)\n\n     Resample01 Resample02 Resample03 Resample04 Resample05 Resample06\n[1,]          3          3          1          1          1          2\n[2,]          4          4          2          2          2          3\n[3,]          5          5          3          3          3          4\n[4,]          6          6          5          4          4          5\n[5,]          7          9          6          5          6          6\n[6,]          8         10         10          6          7          7\n     Resample07 Resample08 Resample09 Resample10\n[1,]          2          2          1          2\n[2,]          4          3          3          5\n[3,]          5          4          4          6\n[4,]          6          5          5          7\n[5,]          8          6          8          9\n[6,]          9          7          9         11\n\n\nWhile the above is incredibly useful, it is also very easy to do using a for loop. Not so exciting.\nSomething that IS more exciting is the ability to do K-fold cross validation which respects groupings in the data. The groupKFold function does just that!\nAs an example, let’s consider the following made-up abalone groups so that each sequential set of 5 abalone that appear in the dataset together are in the same group. For simplicity we will only consider the first 50 abalone.\n\n# add a madeup grouping variable that groupes each subsequent 5 abalone together\n# filter to the first 50 abalone for simplicity\nabalone_grouped &lt;- cbind(abalone_train[1:50, ], group = rep(1:10, each = 5))\nhead(abalone_grouped, 10)\n\n     sex length diameter height whole.weight shucked.weight viscera.weight\n232    M  0.565    0.440  0.175       0.9025         0.3100         0.1930\n3906   M  0.380    0.270  0.095       0.2190         0.0835         0.0515\n1179   F  0.650    0.500  0.190       1.4640         0.6415         0.3390\n2296   F  0.520    0.415  0.145       0.8045         0.3325         0.1725\n1513   F  0.650    0.500  0.160       1.3825         0.7020         0.3040\n1023   F  0.640    0.500  0.170       1.5175         0.6930         0.3260\n2390   M  0.420    0.340  0.125       0.4495         0.1650         0.1125\n856    F  0.575    0.465  0.140       0.9580         0.4420         0.1815\n2462   F  0.500    0.385  0.130       0.7680         0.2625         0.0950\n2756   F  0.525    0.415  0.150       0.7055         0.3290         0.1470\n     shell.weight   old group\n232        0.3250  TRUE     1\n3906       0.0700 FALSE     1\n1179       0.4245 FALSE     1\n2296       0.2850 FALSE     1\n1513       0.3195 FALSE     1\n1023       0.4090  TRUE     2\n2390       0.1440  TRUE     2\n856        0.2705 FALSE     2\n2462       0.2700  TRUE     2\n2756       0.1990 FALSE     2\n\n\nThe following code performs 10-fold cross-validation while respecting the groups in the abalone data. That is, each group of abalone must always appear in the same group together.\n\n# perform grouped K means\ngroup_folds &lt;- groupKFold(abalone_grouped$group, k = 10)\ngroup_folds\n\n$Fold1\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 21 22 23 24 25 26 27 28 29 30\n[26] 31 32 33 34 35 41 42 43 44 45 46 47 48 49 50\n\n$Fold2\n [1]  1  2  3  4  5  6  7  8  9 10 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30\n[26] 36 37 38 39 40 41 42 43 44 45\n\n$Fold3\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n[26] 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 46 47 48 49 50\n\n$Fold4\n [1]  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 26 27 28 29 30 31 32 33 34 35\n[26] 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50\n\n$Fold5\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n[26] 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50\n\n$Fold6\n [1]  1  2  3  4  5 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30\n[26] 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50"
  },
  {
    "objectID": "blog/2017-11-17-caret_tutorial.html#resampling-options-traincontrol",
    "href": "blog/2017-11-17-caret_tutorial.html#resampling-options-traincontrol",
    "title": "A basic tutorial of caret: the machine learning package in R",
    "section": "Resampling options (trainControl)",
    "text": "Resampling options (trainControl)\nOne of the most important part of training ML models is tuning parameters. You can use the trainControl function to specify a number of parameters (including sampling parameters) in your model. The object that is outputted from trainControl will be provided as an argument for train.\n\nset.seed(998)\n# create a testing and training set\nin_training &lt;- createDataPartition(abalone_train$old, p = .75, list = FALSE)\ntraining &lt;- abalone_train[ in_training,]\ntesting  &lt;- abalone_train[-in_training,]\n\n\n# specify that the resampling method is \nfit_control &lt;- trainControl(## 10-fold CV\n                           method = \"cv\",\n                           number = 10)\n\n\n# run a random forest model\nset.seed(825)\nrf_fit &lt;- train(as.factor(old) ~ ., \n                data = abalone_train, \n                method = \"ranger\",\n                trControl = fit_control)\nrf_fit\n\nRandom Forest \n\n3759 samples\n   8 predictor\n   2 classes: 'FALSE', 'TRUE' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 3384, 3383, 3383, 3382, 3383, 3383, ... \nResampling results across tuning parameters:\n\n  mtry  splitrule   Accuracy   Kappa    \n  2     gini        0.7826656  0.5054371\n  2     extratrees  0.7853266  0.5032091\n  5     gini        0.7765528  0.4953944\n  5     extratrees  0.7850614  0.5120121\n  9     gini        0.7683032  0.4787823\n  9     extratrees  0.7810713  0.5057059\n\nTuning parameter 'min.node.size' was held constant at a value of 1\nAccuracy was used to select the optimal model using the largest value.\nThe final values used for the model were mtry = 2, splitrule = extratrees\n and min.node.size = 1.\n\n\nWe could instead use our grouped folds (rather than random CV folds) by assigning the index argument of trainControl to be grouped_folds.\n\n# specify that the resampling method is \ngroup_fit_control &lt;- trainControl(## use grouped CV folds\n                                  index = group_folds,\n                                  method = \"cv\")\nset.seed(825)\nrf_fit &lt;- train(as.factor(old) ~ ., \n                data = select(abalone_grouped, - group), \n                method = \"ranger\",\n                trControl = group_fit_control)\n\n\nrf_fit\n\nRandom Forest \n\n50 samples\n 8 predictor\n 2 classes: 'FALSE', 'TRUE' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 40, 35, 45, 40, 45, 45, ... \nResampling results across tuning parameters:\n\n  mtry  splitrule   Accuracy   Kappa     \n  2     gini        0.5222222  0.03968254\n  2     extratrees  0.5111111  0.03784970\n  5     gini        0.5444444  0.01758658\n  5     extratrees  0.5333333  0.08743687\n  9     gini        0.5777778  0.08071789\n  9     extratrees  0.5555556  0.13952020\n\nTuning parameter 'min.node.size' was held constant at a value of 1\nAccuracy was used to select the optimal model using the largest value.\nThe final values used for the model were mtry = 9, splitrule = gini\n and min.node.size = 1.\n\n\nYou can also pass functions to trainControl that would have otherwise been passed to preProcess."
  },
  {
    "objectID": "blog/2017-11-17-caret_tutorial.html#model-parameter-tuning-options-tunegrid",
    "href": "blog/2017-11-17-caret_tutorial.html#model-parameter-tuning-options-tunegrid",
    "title": "A basic tutorial of caret: the machine learning package in R",
    "section": "Model parameter tuning options (tuneGrid =)",
    "text": "Model parameter tuning options (tuneGrid =)\nYou could specify your own tuning grid for model parameters using the tuneGrid argument of the train function. For example, you can define a grid of parameter combinations.\n\n# define a grid of parameter options to try\nrf_grid &lt;- expand.grid(mtry = c(2, 3, 4, 5),\n                      splitrule = c(\"gini\", \"extratrees\"),\n                      min.node.size = c(1, 3, 5))\nrf_grid\n\n   mtry  splitrule min.node.size\n1     2       gini             1\n2     3       gini             1\n3     4       gini             1\n4     5       gini             1\n5     2 extratrees             1\n6     3 extratrees             1\n7     4 extratrees             1\n8     5 extratrees             1\n9     2       gini             3\n10    3       gini             3\n11    4       gini             3\n12    5       gini             3\n13    2 extratrees             3\n14    3 extratrees             3\n15    4 extratrees             3\n16    5 extratrees             3\n17    2       gini             5\n18    3       gini             5\n19    4       gini             5\n20    5       gini             5\n21    2 extratrees             5\n22    3 extratrees             5\n23    4 extratrees             5\n24    5 extratrees             5\n\n\n\n# re-fit the model with the parameter grid\nrf_fit &lt;- train(as.factor(old) ~ ., \n                data = select(abalone_grouped, -group), \n                method = \"ranger\",\n                trControl = group_fit_control,\n                # provide a grid of parameters\n                tuneGrid = rf_grid)\nrf_fit\n\nRandom Forest \n\n50 samples\n 8 predictor\n 2 classes: 'FALSE', 'TRUE' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 40, 35, 45, 40, 45, 45, ... \nResampling results across tuning parameters:\n\n  mtry  splitrule   min.node.size  Accuracy   Kappa       \n  2     gini        1              0.5722222   0.083698830\n  2     gini        3              0.4944444  -0.009825701\n  2     gini        5              0.5388889   0.012270259\n  2     extratrees  1              0.5111111   0.037849695\n  2     extratrees  3              0.5277778   0.085035842\n  2     extratrees  5              0.5277778   0.085035842\n  3     gini        1              0.5555556   0.111111111\n  3     gini        3              0.5888889   0.111111111\n  3     gini        5              0.5722222   0.066856453\n  3     extratrees  1              0.5444444   0.112636020\n  3     extratrees  3              0.5555556   0.139520202\n  3     extratrees  5              0.5277778   0.085035842\n  4     gini        1              0.5277778  -0.009825701\n  4     gini        3              0.5444444   0.080717893\n  4     gini        5              0.5111111   0.017586580\n  4     extratrees  1              0.5444444   0.109278267\n  4     extratrees  3              0.5444444   0.109278267\n  4     extratrees  5              0.5111111   0.037849695\n  5     gini        1              0.5777778   0.089015152\n  5     gini        3              0.5777778   0.080717893\n  5     gini        5              0.6111111   0.152146465\n  5     extratrees  1              0.5444444   0.109278267\n  5     extratrees  3              0.5444444   0.109278267\n  5     extratrees  5              0.5277778   0.085035842\n\nAccuracy was used to select the optimal model using the largest value.\nThe final values used for the model were mtry = 5, splitrule = gini\n and min.node.size = 5."
  },
  {
    "objectID": "blog/2020-03-25_machine_learning.html",
    "href": "blog/2020-03-25_machine_learning.html",
    "title": "Tidymodels: tidy machine learning in R",
    "section": "",
    "text": "There’s a new modeling pipeline in town: tidymodels. Over the past few years, tidymodels has been gradually emerging as the tidyverse’s machine learning toolkit.\nWhy tidymodels? Well, it turns out that R has a consistency problem. Since everything was made by different people and using different principles, everything has a slightly different interface, and trying to keep everything in line can be frustrating. Several years ago, Max Kuhn (formerly at Pfeizer, now at RStudio) developed the caret R package (see my caret tutorial) aimed at creating a uniform interface for the massive variety of machine learning models that exist in R. Caret was great in a lot of ways, but also limited in others. In my own use, I found it to be quite slow whenever I tried to use on problems of any kind of modest size.\nThat said, caret was a great starting point, so RStudio hired Max Kuhn to work on a tidy version of caret, and he and many other people have developed what has become tidymodels. Tidymodels has been in development for a few years, with snippets of it being released as they were developed (see my post on the recipes package). I’ve been holding off writing a post about tidymodels until it seemed as though the different pieces fit together sufficiently for it to all feel cohesive. I feel like they’re finally there - which means it is time for me to learn it! While caret isn’t going anywhere (you can continue to use caret, and your existing caret code isn’t going to stop working), tidymodels will eventually make it redundant.\nThe main resources I used to learn tidymodels were Alison Hill’s slides from Introduction to Machine Learning with the Tidyverse, which contains all the slides for the course she prepared with Garrett Grolemund for RStudio::conf(2020), and Edgar Ruiz’s Gentle introduction to tidymodels on the RStudio website.\nNote that throughout this post I’ll be assuming basic tidyverse knowledge, primarily of dplyr (e.g. piping %&gt;% and function such as mutate()). Fortunately, for all you purrr-phobes out there, purrr is not required. If you’d like to brush up on your tidyverse skills, check out my Introduction to the Tidyverse posts. If you’d like to learn purrr (purrr is very handy for working with tidymodels but is no longer a requirement), check out my purrr post.\n\nWhat is tidymodels\nMuch like the tidyverse consists of many core packages, such as ggplot2 and dplyr, tidymodels also consists of several core packages, including\n\nrsample: for sample splitting (e.g. train/test or cross-validation)\nrecipes: for pre-processing\nparsnip: for specifying the model\nyardstick: for evaluating the model\n\nSimilarly to how you can load the entire tidyverse suite of packages by typing library(tidyverse), you can load the entire tidymodels suite of packages by typing library(tidymodels).\nWe will also be using the tune package (for parameter tuning procedure) and the workflows package (for putting everything together) that I had thought were a part of CRAN’s tidymodels package bundle, but apparently they aren’t. These will need to be loaded separately for now.\nUnlike in my tidyverse post, I won’t base this post around the packages themselves, but I will mention the packages in passing.\n\n\nGetting set up\nFirst we need to load some libraries: tidymodels and tidyverse.\n\n# load the relevant tidymodels libraries\nlibrary(tidymodels)\nlibrary(tidyverse)\nlibrary(workflows)\nlibrary(tune)\n\nIf you don’t already have the tidymodels library (or any of the other libraries) installed, then you’ll need to install it (once only) using install.packages(\"tidymodels\").\nWe will use the Pima Indian Women’s diabetes dataset which contains information on 768 Pima Indian women’s diabetes status, as well as many predictive features such as the number of pregnancies (pregnant), plasma glucose concentration (glucose), diastolic blood pressure (pressure), triceps skin fold thickness (triceps), 2-hour serum insulin (insulin), BMI (mass), diabetes pedigree function (pedigree), and their age (age). In case you were wondering, the Pima Indians are a group of Native Americans living in an area consisting of what is now central and southern Arizona. The short name, “Pima” is believed to have come from a phrase meaning “I don’t know,” which they used repeatedly in their initial meetings with Spanish colonists. Thanks Wikipedia!\n\n# load the Pima Indians dataset from the mlbench dataset\nlibrary(mlbench)\ndata(PimaIndiansDiabetes)\n# rename dataset to have shorter name because lazy\ndiabetes_orig &lt;- PimaIndiansDiabetes\n\n\ndiabetes_orig\n\n    pregnant glucose pressure triceps insulin mass pedigree age diabetes\n1          6     148       72      35       0 33.6    0.627  50      pos\n2          1      85       66      29       0 26.6    0.351  31      neg\n3          8     183       64       0       0 23.3    0.672  32      pos\n4          1      89       66      23      94 28.1    0.167  21      neg\n5          0     137       40      35     168 43.1    2.288  33      pos\n6          5     116       74       0       0 25.6    0.201  30      neg\n7          3      78       50      32      88 31.0    0.248  26      pos\n8         10     115        0       0       0 35.3    0.134  29      neg\n9          2     197       70      45     543 30.5    0.158  53      pos\n10         8     125       96       0       0  0.0    0.232  54      pos\n11         4     110       92       0       0 37.6    0.191  30      neg\n12        10     168       74       0       0 38.0    0.537  34      pos\n13        10     139       80       0       0 27.1    1.441  57      neg\n14         1     189       60      23     846 30.1    0.398  59      pos\n15         5     166       72      19     175 25.8    0.587  51      pos\n16         7     100        0       0       0 30.0    0.484  32      pos\n17         0     118       84      47     230 45.8    0.551  31      pos\n18         7     107       74       0       0 29.6    0.254  31      pos\n19         1     103       30      38      83 43.3    0.183  33      neg\n20         1     115       70      30      96 34.6    0.529  32      pos\n21         3     126       88      41     235 39.3    0.704  27      neg\n22         8      99       84       0       0 35.4    0.388  50      neg\n23         7     196       90       0       0 39.8    0.451  41      pos\n24         9     119       80      35       0 29.0    0.263  29      pos\n25        11     143       94      33     146 36.6    0.254  51      pos\n26        10     125       70      26     115 31.1    0.205  41      pos\n27         7     147       76       0       0 39.4    0.257  43      pos\n28         1      97       66      15     140 23.2    0.487  22      neg\n29        13     145       82      19     110 22.2    0.245  57      neg\n30         5     117       92       0       0 34.1    0.337  38      neg\n31         5     109       75      26       0 36.0    0.546  60      neg\n32         3     158       76      36     245 31.6    0.851  28      pos\n33         3      88       58      11      54 24.8    0.267  22      neg\n34         6      92       92       0       0 19.9    0.188  28      neg\n35        10     122       78      31       0 27.6    0.512  45      neg\n36         4     103       60      33     192 24.0    0.966  33      neg\n37        11     138       76       0       0 33.2    0.420  35      neg\n38         9     102       76      37       0 32.9    0.665  46      pos\n39         2      90       68      42       0 38.2    0.503  27      pos\n40         4     111       72      47     207 37.1    1.390  56      pos\n41         3     180       64      25      70 34.0    0.271  26      neg\n42         7     133       84       0       0 40.2    0.696  37      neg\n43         7     106       92      18       0 22.7    0.235  48      neg\n44         9     171      110      24     240 45.4    0.721  54      pos\n45         7     159       64       0       0 27.4    0.294  40      neg\n46         0     180       66      39       0 42.0    1.893  25      pos\n47         1     146       56       0       0 29.7    0.564  29      neg\n48         2      71       70      27       0 28.0    0.586  22      neg\n49         7     103       66      32       0 39.1    0.344  31      pos\n50         7     105        0       0       0  0.0    0.305  24      neg\n51         1     103       80      11      82 19.4    0.491  22      neg\n52         1     101       50      15      36 24.2    0.526  26      neg\n53         5      88       66      21      23 24.4    0.342  30      neg\n54         8     176       90      34     300 33.7    0.467  58      pos\n55         7     150       66      42     342 34.7    0.718  42      neg\n56         1      73       50      10       0 23.0    0.248  21      neg\n57         7     187       68      39     304 37.7    0.254  41      pos\n58         0     100       88      60     110 46.8    0.962  31      neg\n59         0     146       82       0       0 40.5    1.781  44      neg\n60         0     105       64      41     142 41.5    0.173  22      neg\n61         2      84        0       0       0  0.0    0.304  21      neg\n62         8     133       72       0       0 32.9    0.270  39      pos\n63         5      44       62       0       0 25.0    0.587  36      neg\n64         2     141       58      34     128 25.4    0.699  24      neg\n65         7     114       66       0       0 32.8    0.258  42      pos\n66         5      99       74      27       0 29.0    0.203  32      neg\n67         0     109       88      30       0 32.5    0.855  38      pos\n68         2     109       92       0       0 42.7    0.845  54      neg\n69         1      95       66      13      38 19.6    0.334  25      neg\n70         4     146       85      27     100 28.9    0.189  27      neg\n71         2     100       66      20      90 32.9    0.867  28      pos\n72         5     139       64      35     140 28.6    0.411  26      neg\n73        13     126       90       0       0 43.4    0.583  42      pos\n74         4     129       86      20     270 35.1    0.231  23      neg\n75         1      79       75      30       0 32.0    0.396  22      neg\n76         1       0       48      20       0 24.7    0.140  22      neg\n77         7      62       78       0       0 32.6    0.391  41      neg\n78         5      95       72      33       0 37.7    0.370  27      neg\n79         0     131        0       0       0 43.2    0.270  26      pos\n80         2     112       66      22       0 25.0    0.307  24      neg\n81         3     113       44      13       0 22.4    0.140  22      neg\n82         2      74        0       0       0  0.0    0.102  22      neg\n83         7      83       78      26      71 29.3    0.767  36      neg\n84         0     101       65      28       0 24.6    0.237  22      neg\n85         5     137      108       0       0 48.8    0.227  37      pos\n86         2     110       74      29     125 32.4    0.698  27      neg\n87        13     106       72      54       0 36.6    0.178  45      neg\n88         2     100       68      25      71 38.5    0.324  26      neg\n89        15     136       70      32     110 37.1    0.153  43      pos\n90         1     107       68      19       0 26.5    0.165  24      neg\n91         1      80       55       0       0 19.1    0.258  21      neg\n92         4     123       80      15     176 32.0    0.443  34      neg\n93         7      81       78      40      48 46.7    0.261  42      neg\n94         4     134       72       0       0 23.8    0.277  60      pos\n95         2     142       82      18      64 24.7    0.761  21      neg\n96         6     144       72      27     228 33.9    0.255  40      neg\n97         2      92       62      28       0 31.6    0.130  24      neg\n98         1      71       48      18      76 20.4    0.323  22      neg\n99         6      93       50      30      64 28.7    0.356  23      neg\n100        1     122       90      51     220 49.7    0.325  31      pos\n101        1     163       72       0       0 39.0    1.222  33      pos\n102        1     151       60       0       0 26.1    0.179  22      neg\n103        0     125       96       0       0 22.5    0.262  21      neg\n104        1      81       72      18      40 26.6    0.283  24      neg\n105        2      85       65       0       0 39.6    0.930  27      neg\n106        1     126       56      29     152 28.7    0.801  21      neg\n107        1      96      122       0       0 22.4    0.207  27      neg\n108        4     144       58      28     140 29.5    0.287  37      neg\n109        3      83       58      31      18 34.3    0.336  25      neg\n110        0      95       85      25      36 37.4    0.247  24      pos\n111        3     171       72      33     135 33.3    0.199  24      pos\n112        8     155       62      26     495 34.0    0.543  46      pos\n113        1      89       76      34      37 31.2    0.192  23      neg\n114        4      76       62       0       0 34.0    0.391  25      neg\n115        7     160       54      32     175 30.5    0.588  39      pos\n116        4     146       92       0       0 31.2    0.539  61      pos\n117        5     124       74       0       0 34.0    0.220  38      pos\n118        5      78       48       0       0 33.7    0.654  25      neg\n119        4      97       60      23       0 28.2    0.443  22      neg\n120        4      99       76      15      51 23.2    0.223  21      neg\n121        0     162       76      56     100 53.2    0.759  25      pos\n122        6     111       64      39       0 34.2    0.260  24      neg\n123        2     107       74      30     100 33.6    0.404  23      neg\n124        5     132       80       0       0 26.8    0.186  69      neg\n125        0     113       76       0       0 33.3    0.278  23      pos\n126        1      88       30      42      99 55.0    0.496  26      pos\n127        3     120       70      30     135 42.9    0.452  30      neg\n128        1     118       58      36      94 33.3    0.261  23      neg\n129        1     117       88      24     145 34.5    0.403  40      pos\n130        0     105       84       0       0 27.9    0.741  62      pos\n131        4     173       70      14     168 29.7    0.361  33      pos\n132        9     122       56       0       0 33.3    1.114  33      pos\n133        3     170       64      37     225 34.5    0.356  30      pos\n134        8      84       74      31       0 38.3    0.457  39      neg\n135        2      96       68      13      49 21.1    0.647  26      neg\n136        2     125       60      20     140 33.8    0.088  31      neg\n137        0     100       70      26      50 30.8    0.597  21      neg\n138        0      93       60      25      92 28.7    0.532  22      neg\n139        0     129       80       0       0 31.2    0.703  29      neg\n140        5     105       72      29     325 36.9    0.159  28      neg\n141        3     128       78       0       0 21.1    0.268  55      neg\n142        5     106       82      30       0 39.5    0.286  38      neg\n143        2     108       52      26      63 32.5    0.318  22      neg\n144       10     108       66       0       0 32.4    0.272  42      pos\n145        4     154       62      31     284 32.8    0.237  23      neg\n146        0     102       75      23       0  0.0    0.572  21      neg\n147        9      57       80      37       0 32.8    0.096  41      neg\n148        2     106       64      35     119 30.5    1.400  34      neg\n149        5     147       78       0       0 33.7    0.218  65      neg\n150        2      90       70      17       0 27.3    0.085  22      neg\n151        1     136       74      50     204 37.4    0.399  24      neg\n152        4     114       65       0       0 21.9    0.432  37      neg\n153        9     156       86      28     155 34.3    1.189  42      pos\n154        1     153       82      42     485 40.6    0.687  23      neg\n155        8     188       78       0       0 47.9    0.137  43      pos\n156        7     152       88      44       0 50.0    0.337  36      pos\n157        2      99       52      15      94 24.6    0.637  21      neg\n158        1     109       56      21     135 25.2    0.833  23      neg\n159        2      88       74      19      53 29.0    0.229  22      neg\n160       17     163       72      41     114 40.9    0.817  47      pos\n161        4     151       90      38       0 29.7    0.294  36      neg\n162        7     102       74      40     105 37.2    0.204  45      neg\n163        0     114       80      34     285 44.2    0.167  27      neg\n164        2     100       64      23       0 29.7    0.368  21      neg\n165        0     131       88       0       0 31.6    0.743  32      pos\n166        6     104       74      18     156 29.9    0.722  41      pos\n167        3     148       66      25       0 32.5    0.256  22      neg\n168        4     120       68       0       0 29.6    0.709  34      neg\n169        4     110       66       0       0 31.9    0.471  29      neg\n170        3     111       90      12      78 28.4    0.495  29      neg\n171        6     102       82       0       0 30.8    0.180  36      pos\n172        6     134       70      23     130 35.4    0.542  29      pos\n173        2      87        0      23       0 28.9    0.773  25      neg\n174        1      79       60      42      48 43.5    0.678  23      neg\n175        2      75       64      24      55 29.7    0.370  33      neg\n176        8     179       72      42     130 32.7    0.719  36      pos\n177        6      85       78       0       0 31.2    0.382  42      neg\n178        0     129      110      46     130 67.1    0.319  26      pos\n179        5     143       78       0       0 45.0    0.190  47      neg\n180        5     130       82       0       0 39.1    0.956  37      pos\n181        6      87       80       0       0 23.2    0.084  32      neg\n182        0     119       64      18      92 34.9    0.725  23      neg\n183        1       0       74      20      23 27.7    0.299  21      neg\n184        5      73       60       0       0 26.8    0.268  27      neg\n185        4     141       74       0       0 27.6    0.244  40      neg\n186        7     194       68      28       0 35.9    0.745  41      pos\n187        8     181       68      36     495 30.1    0.615  60      pos\n188        1     128       98      41      58 32.0    1.321  33      pos\n189        8     109       76      39     114 27.9    0.640  31      pos\n190        5     139       80      35     160 31.6    0.361  25      pos\n191        3     111       62       0       0 22.6    0.142  21      neg\n192        9     123       70      44      94 33.1    0.374  40      neg\n193        7     159       66       0       0 30.4    0.383  36      pos\n194       11     135        0       0       0 52.3    0.578  40      pos\n195        8      85       55      20       0 24.4    0.136  42      neg\n196        5     158       84      41     210 39.4    0.395  29      pos\n197        1     105       58       0       0 24.3    0.187  21      neg\n198        3     107       62      13      48 22.9    0.678  23      pos\n199        4     109       64      44      99 34.8    0.905  26      pos\n200        4     148       60      27     318 30.9    0.150  29      pos\n201        0     113       80      16       0 31.0    0.874  21      neg\n202        1     138       82       0       0 40.1    0.236  28      neg\n203        0     108       68      20       0 27.3    0.787  32      neg\n204        2      99       70      16      44 20.4    0.235  27      neg\n205        6     103       72      32     190 37.7    0.324  55      neg\n206        5     111       72      28       0 23.9    0.407  27      neg\n207        8     196       76      29     280 37.5    0.605  57      pos\n208        5     162      104       0       0 37.7    0.151  52      pos\n209        1      96       64      27      87 33.2    0.289  21      neg\n210        7     184       84      33       0 35.5    0.355  41      pos\n211        2      81       60      22       0 27.7    0.290  25      neg\n212        0     147       85      54       0 42.8    0.375  24      neg\n213        7     179       95      31       0 34.2    0.164  60      neg\n214        0     140       65      26     130 42.6    0.431  24      pos\n215        9     112       82      32     175 34.2    0.260  36      pos\n216       12     151       70      40     271 41.8    0.742  38      pos\n217        5     109       62      41     129 35.8    0.514  25      pos\n218        6     125       68      30     120 30.0    0.464  32      neg\n219        5      85       74      22       0 29.0    1.224  32      pos\n220        5     112       66       0       0 37.8    0.261  41      pos\n221        0     177       60      29     478 34.6    1.072  21      pos\n222        2     158       90       0       0 31.6    0.805  66      pos\n223        7     119        0       0       0 25.2    0.209  37      neg\n224        7     142       60      33     190 28.8    0.687  61      neg\n225        1     100       66      15      56 23.6    0.666  26      neg\n226        1      87       78      27      32 34.6    0.101  22      neg\n227        0     101       76       0       0 35.7    0.198  26      neg\n228        3     162       52      38       0 37.2    0.652  24      pos\n229        4     197       70      39     744 36.7    2.329  31      neg\n230        0     117       80      31      53 45.2    0.089  24      neg\n231        4     142       86       0       0 44.0    0.645  22      pos\n232        6     134       80      37     370 46.2    0.238  46      pos\n233        1      79       80      25      37 25.4    0.583  22      neg\n234        4     122       68       0       0 35.0    0.394  29      neg\n235        3      74       68      28      45 29.7    0.293  23      neg\n236        4     171       72       0       0 43.6    0.479  26      pos\n237        7     181       84      21     192 35.9    0.586  51      pos\n238        0     179       90      27       0 44.1    0.686  23      pos\n239        9     164       84      21       0 30.8    0.831  32      pos\n240        0     104       76       0       0 18.4    0.582  27      neg\n241        1      91       64      24       0 29.2    0.192  21      neg\n242        4      91       70      32      88 33.1    0.446  22      neg\n243        3     139       54       0       0 25.6    0.402  22      pos\n244        6     119       50      22     176 27.1    1.318  33      pos\n245        2     146       76      35     194 38.2    0.329  29      neg\n246        9     184       85      15       0 30.0    1.213  49      pos\n247       10     122       68       0       0 31.2    0.258  41      neg\n248        0     165       90      33     680 52.3    0.427  23      neg\n249        9     124       70      33     402 35.4    0.282  34      neg\n250        1     111       86      19       0 30.1    0.143  23      neg\n251        9     106       52       0       0 31.2    0.380  42      neg\n252        2     129       84       0       0 28.0    0.284  27      neg\n253        2      90       80      14      55 24.4    0.249  24      neg\n254        0      86       68      32       0 35.8    0.238  25      neg\n255       12      92       62       7     258 27.6    0.926  44      pos\n256        1     113       64      35       0 33.6    0.543  21      pos\n257        3     111       56      39       0 30.1    0.557  30      neg\n258        2     114       68      22       0 28.7    0.092  25      neg\n259        1     193       50      16     375 25.9    0.655  24      neg\n260       11     155       76      28     150 33.3    1.353  51      pos\n261        3     191       68      15     130 30.9    0.299  34      neg\n262        3     141        0       0       0 30.0    0.761  27      pos\n263        4      95       70      32       0 32.1    0.612  24      neg\n264        3     142       80      15       0 32.4    0.200  63      neg\n265        4     123       62       0       0 32.0    0.226  35      pos\n266        5      96       74      18      67 33.6    0.997  43      neg\n267        0     138        0       0       0 36.3    0.933  25      pos\n268        2     128       64      42       0 40.0    1.101  24      neg\n269        0     102       52       0       0 25.1    0.078  21      neg\n270        2     146        0       0       0 27.5    0.240  28      pos\n271       10     101       86      37       0 45.6    1.136  38      pos\n272        2     108       62      32      56 25.2    0.128  21      neg\n273        3     122       78       0       0 23.0    0.254  40      neg\n274        1      71       78      50      45 33.2    0.422  21      neg\n275       13     106       70       0       0 34.2    0.251  52      neg\n276        2     100       70      52      57 40.5    0.677  25      neg\n277        7     106       60      24       0 26.5    0.296  29      pos\n278        0     104       64      23     116 27.8    0.454  23      neg\n279        5     114       74       0       0 24.9    0.744  57      neg\n280        2     108       62      10     278 25.3    0.881  22      neg\n281        0     146       70       0       0 37.9    0.334  28      pos\n282       10     129       76      28     122 35.9    0.280  39      neg\n283        7     133       88      15     155 32.4    0.262  37      neg\n284        7     161       86       0       0 30.4    0.165  47      pos\n285        2     108       80       0       0 27.0    0.259  52      pos\n286        7     136       74      26     135 26.0    0.647  51      neg\n287        5     155       84      44     545 38.7    0.619  34      neg\n288        1     119       86      39     220 45.6    0.808  29      pos\n289        4      96       56      17      49 20.8    0.340  26      neg\n290        5     108       72      43      75 36.1    0.263  33      neg\n291        0      78       88      29      40 36.9    0.434  21      neg\n292        0     107       62      30      74 36.6    0.757  25      pos\n293        2     128       78      37     182 43.3    1.224  31      pos\n294        1     128       48      45     194 40.5    0.613  24      pos\n295        0     161       50       0       0 21.9    0.254  65      neg\n296        6     151       62      31     120 35.5    0.692  28      neg\n297        2     146       70      38     360 28.0    0.337  29      pos\n298        0     126       84      29     215 30.7    0.520  24      neg\n299       14     100       78      25     184 36.6    0.412  46      pos\n300        8     112       72       0       0 23.6    0.840  58      neg\n301        0     167        0       0       0 32.3    0.839  30      pos\n302        2     144       58      33     135 31.6    0.422  25      pos\n303        5      77       82      41      42 35.8    0.156  35      neg\n304        5     115       98       0       0 52.9    0.209  28      pos\n305        3     150       76       0       0 21.0    0.207  37      neg\n306        2     120       76      37     105 39.7    0.215  29      neg\n307       10     161       68      23     132 25.5    0.326  47      pos\n308        0     137       68      14     148 24.8    0.143  21      neg\n309        0     128       68      19     180 30.5    1.391  25      pos\n310        2     124       68      28     205 32.9    0.875  30      pos\n311        6      80       66      30       0 26.2    0.313  41      neg\n312        0     106       70      37     148 39.4    0.605  22      neg\n313        2     155       74      17      96 26.6    0.433  27      pos\n314        3     113       50      10      85 29.5    0.626  25      neg\n315        7     109       80      31       0 35.9    1.127  43      pos\n316        2     112       68      22      94 34.1    0.315  26      neg\n317        3      99       80      11      64 19.3    0.284  30      neg\n318        3     182       74       0       0 30.5    0.345  29      pos\n319        3     115       66      39     140 38.1    0.150  28      neg\n320        6     194       78       0       0 23.5    0.129  59      pos\n321        4     129       60      12     231 27.5    0.527  31      neg\n322        3     112       74      30       0 31.6    0.197  25      pos\n323        0     124       70      20       0 27.4    0.254  36      pos\n324       13     152       90      33      29 26.8    0.731  43      pos\n325        2     112       75      32       0 35.7    0.148  21      neg\n326        1     157       72      21     168 25.6    0.123  24      neg\n327        1     122       64      32     156 35.1    0.692  30      pos\n328       10     179       70       0       0 35.1    0.200  37      neg\n329        2     102       86      36     120 45.5    0.127  23      pos\n330        6     105       70      32      68 30.8    0.122  37      neg\n331        8     118       72      19       0 23.1    1.476  46      neg\n332        2      87       58      16      52 32.7    0.166  25      neg\n333        1     180        0       0       0 43.3    0.282  41      pos\n334       12     106       80       0       0 23.6    0.137  44      neg\n335        1      95       60      18      58 23.9    0.260  22      neg\n336        0     165       76      43     255 47.9    0.259  26      neg\n337        0     117        0       0       0 33.8    0.932  44      neg\n338        5     115       76       0       0 31.2    0.343  44      pos\n339        9     152       78      34     171 34.2    0.893  33      pos\n340        7     178       84       0       0 39.9    0.331  41      pos\n341        1     130       70      13     105 25.9    0.472  22      neg\n342        1      95       74      21      73 25.9    0.673  36      neg\n343        1       0       68      35       0 32.0    0.389  22      neg\n344        5     122       86       0       0 34.7    0.290  33      neg\n345        8      95       72       0       0 36.8    0.485  57      neg\n346        8     126       88      36     108 38.5    0.349  49      neg\n347        1     139       46      19      83 28.7    0.654  22      neg\n348        3     116        0       0       0 23.5    0.187  23      neg\n349        3      99       62      19      74 21.8    0.279  26      neg\n350        5       0       80      32       0 41.0    0.346  37      pos\n351        4      92       80       0       0 42.2    0.237  29      neg\n352        4     137       84       0       0 31.2    0.252  30      neg\n353        3      61       82      28       0 34.4    0.243  46      neg\n354        1      90       62      12      43 27.2    0.580  24      neg\n355        3      90       78       0       0 42.7    0.559  21      neg\n356        9     165       88       0       0 30.4    0.302  49      pos\n357        1     125       50      40     167 33.3    0.962  28      pos\n358       13     129        0      30       0 39.9    0.569  44      pos\n359       12      88       74      40      54 35.3    0.378  48      neg\n360        1     196       76      36     249 36.5    0.875  29      pos\n361        5     189       64      33     325 31.2    0.583  29      pos\n362        5     158       70       0       0 29.8    0.207  63      neg\n363        5     103      108      37       0 39.2    0.305  65      neg\n364        4     146       78       0       0 38.5    0.520  67      pos\n365        4     147       74      25     293 34.9    0.385  30      neg\n366        5      99       54      28      83 34.0    0.499  30      neg\n367        6     124       72       0       0 27.6    0.368  29      pos\n368        0     101       64      17       0 21.0    0.252  21      neg\n369        3      81       86      16      66 27.5    0.306  22      neg\n370        1     133      102      28     140 32.8    0.234  45      pos\n371        3     173       82      48     465 38.4    2.137  25      pos\n372        0     118       64      23      89  0.0    1.731  21      neg\n373        0      84       64      22      66 35.8    0.545  21      neg\n374        2     105       58      40      94 34.9    0.225  25      neg\n375        2     122       52      43     158 36.2    0.816  28      neg\n376       12     140       82      43     325 39.2    0.528  58      pos\n377        0      98       82      15      84 25.2    0.299  22      neg\n378        1      87       60      37      75 37.2    0.509  22      neg\n379        4     156       75       0       0 48.3    0.238  32      pos\n380        0      93      100      39      72 43.4    1.021  35      neg\n381        1     107       72      30      82 30.8    0.821  24      neg\n382        0     105       68      22       0 20.0    0.236  22      neg\n383        1     109       60       8     182 25.4    0.947  21      neg\n384        1      90       62      18      59 25.1    1.268  25      neg\n385        1     125       70      24     110 24.3    0.221  25      neg\n386        1     119       54      13      50 22.3    0.205  24      neg\n387        5     116       74      29       0 32.3    0.660  35      pos\n388        8     105      100      36       0 43.3    0.239  45      pos\n389        5     144       82      26     285 32.0    0.452  58      pos\n390        3     100       68      23      81 31.6    0.949  28      neg\n391        1     100       66      29     196 32.0    0.444  42      neg\n392        5     166       76       0       0 45.7    0.340  27      pos\n393        1     131       64      14     415 23.7    0.389  21      neg\n394        4     116       72      12      87 22.1    0.463  37      neg\n395        4     158       78       0       0 32.9    0.803  31      pos\n396        2     127       58      24     275 27.7    1.600  25      neg\n397        3      96       56      34     115 24.7    0.944  39      neg\n398        0     131       66      40       0 34.3    0.196  22      pos\n399        3      82       70       0       0 21.1    0.389  25      neg\n400        3     193       70      31       0 34.9    0.241  25      pos\n401        4      95       64       0       0 32.0    0.161  31      pos\n402        6     137       61       0       0 24.2    0.151  55      neg\n403        5     136       84      41      88 35.0    0.286  35      pos\n404        9      72       78      25       0 31.6    0.280  38      neg\n405        5     168       64       0       0 32.9    0.135  41      pos\n406        2     123       48      32     165 42.1    0.520  26      neg\n407        4     115       72       0       0 28.9    0.376  46      pos\n408        0     101       62       0       0 21.9    0.336  25      neg\n409        8     197       74       0       0 25.9    1.191  39      pos\n410        1     172       68      49     579 42.4    0.702  28      pos\n411        6     102       90      39       0 35.7    0.674  28      neg\n412        1     112       72      30     176 34.4    0.528  25      neg\n413        1     143       84      23     310 42.4    1.076  22      neg\n414        1     143       74      22      61 26.2    0.256  21      neg\n415        0     138       60      35     167 34.6    0.534  21      pos\n416        3     173       84      33     474 35.7    0.258  22      pos\n417        1      97       68      21       0 27.2    1.095  22      neg\n418        4     144       82      32       0 38.5    0.554  37      pos\n419        1      83       68       0       0 18.2    0.624  27      neg\n420        3     129       64      29     115 26.4    0.219  28      pos\n421        1     119       88      41     170 45.3    0.507  26      neg\n422        2      94       68      18      76 26.0    0.561  21      neg\n423        0     102       64      46      78 40.6    0.496  21      neg\n424        2     115       64      22       0 30.8    0.421  21      neg\n425        8     151       78      32     210 42.9    0.516  36      pos\n426        4     184       78      39     277 37.0    0.264  31      pos\n427        0      94        0       0       0  0.0    0.256  25      neg\n428        1     181       64      30     180 34.1    0.328  38      pos\n429        0     135       94      46     145 40.6    0.284  26      neg\n430        1      95       82      25     180 35.0    0.233  43      pos\n431        2      99        0       0       0 22.2    0.108  23      neg\n432        3      89       74      16      85 30.4    0.551  38      neg\n433        1      80       74      11      60 30.0    0.527  22      neg\n434        2     139       75       0       0 25.6    0.167  29      neg\n435        1      90       68       8       0 24.5    1.138  36      neg\n436        0     141        0       0       0 42.4    0.205  29      pos\n437       12     140       85      33       0 37.4    0.244  41      neg\n438        5     147       75       0       0 29.9    0.434  28      neg\n439        1      97       70      15       0 18.2    0.147  21      neg\n440        6     107       88       0       0 36.8    0.727  31      neg\n441        0     189      104      25       0 34.3    0.435  41      pos\n442        2      83       66      23      50 32.2    0.497  22      neg\n443        4     117       64      27     120 33.2    0.230  24      neg\n444        8     108       70       0       0 30.5    0.955  33      pos\n445        4     117       62      12       0 29.7    0.380  30      pos\n446        0     180       78      63      14 59.4    2.420  25      pos\n447        1     100       72      12      70 25.3    0.658  28      neg\n448        0      95       80      45      92 36.5    0.330  26      neg\n449        0     104       64      37      64 33.6    0.510  22      pos\n450        0     120       74      18      63 30.5    0.285  26      neg\n451        1      82       64      13      95 21.2    0.415  23      neg\n452        2     134       70       0       0 28.9    0.542  23      pos\n453        0      91       68      32     210 39.9    0.381  25      neg\n454        2     119        0       0       0 19.6    0.832  72      neg\n455        2     100       54      28     105 37.8    0.498  24      neg\n456       14     175       62      30       0 33.6    0.212  38      pos\n457        1     135       54       0       0 26.7    0.687  62      neg\n458        5      86       68      28      71 30.2    0.364  24      neg\n459       10     148       84      48     237 37.6    1.001  51      pos\n460        9     134       74      33      60 25.9    0.460  81      neg\n461        9     120       72      22      56 20.8    0.733  48      neg\n462        1      71       62       0       0 21.8    0.416  26      neg\n463        8      74       70      40      49 35.3    0.705  39      neg\n464        5      88       78      30       0 27.6    0.258  37      neg\n465       10     115       98       0       0 24.0    1.022  34      neg\n466        0     124       56      13     105 21.8    0.452  21      neg\n467        0      74       52      10      36 27.8    0.269  22      neg\n468        0      97       64      36     100 36.8    0.600  25      neg\n469        8     120        0       0       0 30.0    0.183  38      pos\n470        6     154       78      41     140 46.1    0.571  27      neg\n471        1     144       82      40       0 41.3    0.607  28      neg\n472        0     137       70      38       0 33.2    0.170  22      neg\n473        0     119       66      27       0 38.8    0.259  22      neg\n474        7     136       90       0       0 29.9    0.210  50      neg\n475        4     114       64       0       0 28.9    0.126  24      neg\n476        0     137       84      27       0 27.3    0.231  59      neg\n477        2     105       80      45     191 33.7    0.711  29      pos\n478        7     114       76      17     110 23.8    0.466  31      neg\n479        8     126       74      38      75 25.9    0.162  39      neg\n480        4     132       86      31       0 28.0    0.419  63      neg\n481        3     158       70      30     328 35.5    0.344  35      pos\n482        0     123       88      37       0 35.2    0.197  29      neg\n483        4      85       58      22      49 27.8    0.306  28      neg\n484        0      84       82      31     125 38.2    0.233  23      neg\n485        0     145        0       0       0 44.2    0.630  31      pos\n486        0     135       68      42     250 42.3    0.365  24      pos\n487        1     139       62      41     480 40.7    0.536  21      neg\n488        0     173       78      32     265 46.5    1.159  58      neg\n489        4      99       72      17       0 25.6    0.294  28      neg\n490        8     194       80       0       0 26.1    0.551  67      neg\n491        2      83       65      28      66 36.8    0.629  24      neg\n492        2      89       90      30       0 33.5    0.292  42      neg\n493        4      99       68      38       0 32.8    0.145  33      neg\n494        4     125       70      18     122 28.9    1.144  45      pos\n495        3      80        0       0       0  0.0    0.174  22      neg\n496        6     166       74       0       0 26.6    0.304  66      neg\n497        5     110       68       0       0 26.0    0.292  30      neg\n498        2      81       72      15      76 30.1    0.547  25      neg\n499        7     195       70      33     145 25.1    0.163  55      pos\n500        6     154       74      32     193 29.3    0.839  39      neg\n501        2     117       90      19      71 25.2    0.313  21      neg\n502        3      84       72      32       0 37.2    0.267  28      neg\n503        6       0       68      41       0 39.0    0.727  41      pos\n504        7      94       64      25      79 33.3    0.738  41      neg\n505        3      96       78      39       0 37.3    0.238  40      neg\n506       10      75       82       0       0 33.3    0.263  38      neg\n507        0     180       90      26      90 36.5    0.314  35      pos\n508        1     130       60      23     170 28.6    0.692  21      neg\n509        2      84       50      23      76 30.4    0.968  21      neg\n510        8     120       78       0       0 25.0    0.409  64      neg\n511       12      84       72      31       0 29.7    0.297  46      pos\n512        0     139       62      17     210 22.1    0.207  21      neg\n513        9      91       68       0       0 24.2    0.200  58      neg\n514        2      91       62       0       0 27.3    0.525  22      neg\n515        3      99       54      19      86 25.6    0.154  24      neg\n516        3     163       70      18     105 31.6    0.268  28      pos\n517        9     145       88      34     165 30.3    0.771  53      pos\n518        7     125       86       0       0 37.6    0.304  51      neg\n519       13      76       60       0       0 32.8    0.180  41      neg\n520        6     129       90       7     326 19.6    0.582  60      neg\n521        2      68       70      32      66 25.0    0.187  25      neg\n522        3     124       80      33     130 33.2    0.305  26      neg\n523        6     114        0       0       0  0.0    0.189  26      neg\n524        9     130       70       0       0 34.2    0.652  45      pos\n525        3     125       58       0       0 31.6    0.151  24      neg\n526        3      87       60      18       0 21.8    0.444  21      neg\n527        1      97       64      19      82 18.2    0.299  21      neg\n528        3     116       74      15     105 26.3    0.107  24      neg\n529        0     117       66      31     188 30.8    0.493  22      neg\n530        0     111       65       0       0 24.6    0.660  31      neg\n531        2     122       60      18     106 29.8    0.717  22      neg\n532        0     107       76       0       0 45.3    0.686  24      neg\n533        1      86       66      52      65 41.3    0.917  29      neg\n534        6      91        0       0       0 29.8    0.501  31      neg\n535        1      77       56      30      56 33.3    1.251  24      neg\n536        4     132        0       0       0 32.9    0.302  23      pos\n537        0     105       90       0       0 29.6    0.197  46      neg\n538        0      57       60       0       0 21.7    0.735  67      neg\n539        0     127       80      37     210 36.3    0.804  23      neg\n540        3     129       92      49     155 36.4    0.968  32      pos\n541        8     100       74      40     215 39.4    0.661  43      pos\n542        3     128       72      25     190 32.4    0.549  27      pos\n543       10      90       85      32       0 34.9    0.825  56      pos\n544        4      84       90      23      56 39.5    0.159  25      neg\n545        1      88       78      29      76 32.0    0.365  29      neg\n546        8     186       90      35     225 34.5    0.423  37      pos\n547        5     187       76      27     207 43.6    1.034  53      pos\n548        4     131       68      21     166 33.1    0.160  28      neg\n549        1     164       82      43      67 32.8    0.341  50      neg\n550        4     189      110      31       0 28.5    0.680  37      neg\n551        1     116       70      28       0 27.4    0.204  21      neg\n552        3      84       68      30     106 31.9    0.591  25      neg\n553        6     114       88       0       0 27.8    0.247  66      neg\n554        1      88       62      24      44 29.9    0.422  23      neg\n555        1      84       64      23     115 36.9    0.471  28      neg\n556        7     124       70      33     215 25.5    0.161  37      neg\n557        1      97       70      40       0 38.1    0.218  30      neg\n558        8     110       76       0       0 27.8    0.237  58      neg\n559       11     103       68      40       0 46.2    0.126  42      neg\n560       11      85       74       0       0 30.1    0.300  35      neg\n561        6     125       76       0       0 33.8    0.121  54      pos\n562        0     198       66      32     274 41.3    0.502  28      pos\n563        1      87       68      34      77 37.6    0.401  24      neg\n564        6      99       60      19      54 26.9    0.497  32      neg\n565        0      91       80       0       0 32.4    0.601  27      neg\n566        2      95       54      14      88 26.1    0.748  22      neg\n567        1      99       72      30      18 38.6    0.412  21      neg\n568        6      92       62      32     126 32.0    0.085  46      neg\n569        4     154       72      29     126 31.3    0.338  37      neg\n570        0     121       66      30     165 34.3    0.203  33      pos\n571        3      78       70       0       0 32.5    0.270  39      neg\n572        2     130       96       0       0 22.6    0.268  21      neg\n573        3     111       58      31      44 29.5    0.430  22      neg\n574        2      98       60      17     120 34.7    0.198  22      neg\n575        1     143       86      30     330 30.1    0.892  23      neg\n576        1     119       44      47      63 35.5    0.280  25      neg\n577        6     108       44      20     130 24.0    0.813  35      neg\n578        2     118       80       0       0 42.9    0.693  21      pos\n579       10     133       68       0       0 27.0    0.245  36      neg\n580        2     197       70      99       0 34.7    0.575  62      pos\n581        0     151       90      46       0 42.1    0.371  21      pos\n582        6     109       60      27       0 25.0    0.206  27      neg\n583       12     121       78      17       0 26.5    0.259  62      neg\n584        8     100       76       0       0 38.7    0.190  42      neg\n585        8     124       76      24     600 28.7    0.687  52      pos\n586        1      93       56      11       0 22.5    0.417  22      neg\n587        8     143       66       0       0 34.9    0.129  41      pos\n588        6     103       66       0       0 24.3    0.249  29      neg\n589        3     176       86      27     156 33.3    1.154  52      pos\n590        0      73        0       0       0 21.1    0.342  25      neg\n591       11     111       84      40       0 46.8    0.925  45      pos\n592        2     112       78      50     140 39.4    0.175  24      neg\n593        3     132       80       0       0 34.4    0.402  44      pos\n594        2      82       52      22     115 28.5    1.699  25      neg\n595        6     123       72      45     230 33.6    0.733  34      neg\n596        0     188       82      14     185 32.0    0.682  22      pos\n597        0      67       76       0       0 45.3    0.194  46      neg\n598        1      89       24      19      25 27.8    0.559  21      neg\n599        1     173       74       0       0 36.8    0.088  38      pos\n600        1     109       38      18     120 23.1    0.407  26      neg\n601        1     108       88      19       0 27.1    0.400  24      neg\n602        6      96        0       0       0 23.7    0.190  28      neg\n603        1     124       74      36       0 27.8    0.100  30      neg\n604        7     150       78      29     126 35.2    0.692  54      pos\n605        4     183        0       0       0 28.4    0.212  36      pos\n606        1     124       60      32       0 35.8    0.514  21      neg\n607        1     181       78      42     293 40.0    1.258  22      pos\n608        1      92       62      25      41 19.5    0.482  25      neg\n609        0     152       82      39     272 41.5    0.270  27      neg\n610        1     111       62      13     182 24.0    0.138  23      neg\n611        3     106       54      21     158 30.9    0.292  24      neg\n612        3     174       58      22     194 32.9    0.593  36      pos\n613        7     168       88      42     321 38.2    0.787  40      pos\n614        6     105       80      28       0 32.5    0.878  26      neg\n615       11     138       74      26     144 36.1    0.557  50      pos\n616        3     106       72       0       0 25.8    0.207  27      neg\n617        6     117       96       0       0 28.7    0.157  30      neg\n618        2      68       62      13      15 20.1    0.257  23      neg\n619        9     112       82      24       0 28.2    1.282  50      pos\n620        0     119        0       0       0 32.4    0.141  24      pos\n621        2     112       86      42     160 38.4    0.246  28      neg\n622        2      92       76      20       0 24.2    1.698  28      neg\n623        6     183       94       0       0 40.8    1.461  45      neg\n624        0      94       70      27     115 43.5    0.347  21      neg\n625        2     108       64       0       0 30.8    0.158  21      neg\n626        4      90       88      47      54 37.7    0.362  29      neg\n627        0     125       68       0       0 24.7    0.206  21      neg\n628        0     132       78       0       0 32.4    0.393  21      neg\n629        5     128       80       0       0 34.6    0.144  45      neg\n630        4      94       65      22       0 24.7    0.148  21      neg\n631        7     114       64       0       0 27.4    0.732  34      pos\n632        0     102       78      40      90 34.5    0.238  24      neg\n633        2     111       60       0       0 26.2    0.343  23      neg\n634        1     128       82      17     183 27.5    0.115  22      neg\n635       10      92       62       0       0 25.9    0.167  31      neg\n636       13     104       72       0       0 31.2    0.465  38      pos\n637        5     104       74       0       0 28.8    0.153  48      neg\n638        2      94       76      18      66 31.6    0.649  23      neg\n639        7      97       76      32      91 40.9    0.871  32      pos\n640        1     100       74      12      46 19.5    0.149  28      neg\n641        0     102       86      17     105 29.3    0.695  27      neg\n642        4     128       70       0       0 34.3    0.303  24      neg\n643        6     147       80       0       0 29.5    0.178  50      pos\n644        4      90        0       0       0 28.0    0.610  31      neg\n645        3     103       72      30     152 27.6    0.730  27      neg\n646        2     157       74      35     440 39.4    0.134  30      neg\n647        1     167       74      17     144 23.4    0.447  33      pos\n648        0     179       50      36     159 37.8    0.455  22      pos\n649       11     136       84      35     130 28.3    0.260  42      pos\n650        0     107       60      25       0 26.4    0.133  23      neg\n651        1      91       54      25     100 25.2    0.234  23      neg\n652        1     117       60      23     106 33.8    0.466  27      neg\n653        5     123       74      40      77 34.1    0.269  28      neg\n654        2     120       54       0       0 26.8    0.455  27      neg\n655        1     106       70      28     135 34.2    0.142  22      neg\n656        2     155       52      27     540 38.7    0.240  25      pos\n657        2     101       58      35      90 21.8    0.155  22      neg\n658        1     120       80      48     200 38.9    1.162  41      neg\n659       11     127      106       0       0 39.0    0.190  51      neg\n660        3      80       82      31      70 34.2    1.292  27      pos\n661       10     162       84       0       0 27.7    0.182  54      neg\n662        1     199       76      43       0 42.9    1.394  22      pos\n663        8     167      106      46     231 37.6    0.165  43      pos\n664        9     145       80      46     130 37.9    0.637  40      pos\n665        6     115       60      39       0 33.7    0.245  40      pos\n666        1     112       80      45     132 34.8    0.217  24      neg\n667        4     145       82      18       0 32.5    0.235  70      pos\n668       10     111       70      27       0 27.5    0.141  40      pos\n669        6      98       58      33     190 34.0    0.430  43      neg\n670        9     154       78      30     100 30.9    0.164  45      neg\n671        6     165       68      26     168 33.6    0.631  49      neg\n672        1      99       58      10       0 25.4    0.551  21      neg\n673       10      68      106      23      49 35.5    0.285  47      neg\n674        3     123      100      35     240 57.3    0.880  22      neg\n675        8      91       82       0       0 35.6    0.587  68      neg\n676        6     195       70       0       0 30.9    0.328  31      pos\n677        9     156       86       0       0 24.8    0.230  53      pos\n678        0      93       60       0       0 35.3    0.263  25      neg\n679        3     121       52       0       0 36.0    0.127  25      pos\n680        2     101       58      17     265 24.2    0.614  23      neg\n681        2      56       56      28      45 24.2    0.332  22      neg\n682        0     162       76      36       0 49.6    0.364  26      pos\n683        0      95       64      39     105 44.6    0.366  22      neg\n684        4     125       80       0       0 32.3    0.536  27      pos\n685        5     136       82       0       0  0.0    0.640  69      neg\n686        2     129       74      26     205 33.2    0.591  25      neg\n687        3     130       64       0       0 23.1    0.314  22      neg\n688        1     107       50      19       0 28.3    0.181  29      neg\n689        1     140       74      26     180 24.1    0.828  23      neg\n690        1     144       82      46     180 46.1    0.335  46      pos\n691        8     107       80       0       0 24.6    0.856  34      neg\n692       13     158      114       0       0 42.3    0.257  44      pos\n693        2     121       70      32      95 39.1    0.886  23      neg\n694        7     129       68      49     125 38.5    0.439  43      pos\n695        2      90       60       0       0 23.5    0.191  25      neg\n696        7     142       90      24     480 30.4    0.128  43      pos\n697        3     169       74      19     125 29.9    0.268  31      pos\n698        0      99        0       0       0 25.0    0.253  22      neg\n699        4     127       88      11     155 34.5    0.598  28      neg\n700        4     118       70       0       0 44.5    0.904  26      neg\n701        2     122       76      27     200 35.9    0.483  26      neg\n702        6     125       78      31       0 27.6    0.565  49      pos\n703        1     168       88      29       0 35.0    0.905  52      pos\n704        2     129        0       0       0 38.5    0.304  41      neg\n705        4     110       76      20     100 28.4    0.118  27      neg\n706        6      80       80      36       0 39.8    0.177  28      neg\n707       10     115        0       0       0  0.0    0.261  30      pos\n708        2     127       46      21     335 34.4    0.176  22      neg\n709        9     164       78       0       0 32.8    0.148  45      pos\n710        2      93       64      32     160 38.0    0.674  23      pos\n711        3     158       64      13     387 31.2    0.295  24      neg\n712        5     126       78      27      22 29.6    0.439  40      neg\n713       10     129       62      36       0 41.2    0.441  38      pos\n714        0     134       58      20     291 26.4    0.352  21      neg\n715        3     102       74       0       0 29.5    0.121  32      neg\n716        7     187       50      33     392 33.9    0.826  34      pos\n717        3     173       78      39     185 33.8    0.970  31      pos\n718       10      94       72      18       0 23.1    0.595  56      neg\n719        1     108       60      46     178 35.5    0.415  24      neg\n720        5      97       76      27       0 35.6    0.378  52      pos\n721        4      83       86      19       0 29.3    0.317  34      neg\n722        1     114       66      36     200 38.1    0.289  21      neg\n723        1     149       68      29     127 29.3    0.349  42      pos\n724        5     117       86      30     105 39.1    0.251  42      neg\n725        1     111       94       0       0 32.8    0.265  45      neg\n726        4     112       78      40       0 39.4    0.236  38      neg\n727        1     116       78      29     180 36.1    0.496  25      neg\n728        0     141       84      26       0 32.4    0.433  22      neg\n729        2     175       88       0       0 22.9    0.326  22      neg\n730        2      92       52       0       0 30.1    0.141  22      neg\n731        3     130       78      23      79 28.4    0.323  34      pos\n732        8     120       86       0       0 28.4    0.259  22      pos\n733        2     174       88      37     120 44.5    0.646  24      pos\n734        2     106       56      27     165 29.0    0.426  22      neg\n735        2     105       75       0       0 23.3    0.560  53      neg\n736        4      95       60      32       0 35.4    0.284  28      neg\n737        0     126       86      27     120 27.4    0.515  21      neg\n738        8      65       72      23       0 32.0    0.600  42      neg\n739        2      99       60      17     160 36.6    0.453  21      neg\n740        1     102       74       0       0 39.5    0.293  42      pos\n741       11     120       80      37     150 42.3    0.785  48      pos\n742        3     102       44      20      94 30.8    0.400  26      neg\n743        1     109       58      18     116 28.5    0.219  22      neg\n744        9     140       94       0       0 32.7    0.734  45      pos\n745       13     153       88      37     140 40.6    1.174  39      neg\n746       12     100       84      33     105 30.0    0.488  46      neg\n747        1     147       94      41       0 49.3    0.358  27      pos\n748        1      81       74      41      57 46.3    1.096  32      neg\n749        3     187       70      22     200 36.4    0.408  36      pos\n750        6     162       62       0       0 24.3    0.178  50      pos\n751        4     136       70       0       0 31.2    1.182  22      pos\n752        1     121       78      39      74 39.0    0.261  28      neg\n753        3     108       62      24       0 26.0    0.223  25      neg\n754        0     181       88      44     510 43.3    0.222  26      pos\n755        8     154       78      32       0 32.4    0.443  45      pos\n756        1     128       88      39     110 36.5    1.057  37      pos\n757        7     137       90      41       0 32.0    0.391  39      neg\n758        0     123       72       0       0 36.3    0.258  52      pos\n759        1     106       76       0       0 37.5    0.197  26      neg\n760        6     190       92       0       0 35.5    0.278  66      pos\n761        2      88       58      26      16 28.4    0.766  22      neg\n762        9     170       74      31       0 44.0    0.403  43      pos\n763        9      89       62       0       0 22.5    0.142  33      neg\n764       10     101       76      48     180 32.9    0.171  63      neg\n765        2     122       70      27       0 36.8    0.340  27      neg\n766        5     121       72      23     112 26.2    0.245  30      neg\n767        1     126       60       0       0 30.1    0.349  47      pos\n768        1      93       70      31       0 30.4    0.315  23      neg\n\n\nA quick exploration reveals that there are more zeros in the data than expected (especially since a BMI or tricep skin fold thickness of 0 is impossible), implying that missing values are recorded as zeros. See for instance the histogram of the tricep skin fold thickness, which has a number of 0 entries that are set apart from the other entries.\n\nggplot(diabetes_orig) +\n  geom_histogram(aes(x = triceps))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nThis phenomena can also seen in the glucose, pressure, insulin and mass variables. Thus, we convert the 0 entries in all variables (other than “pregnant”) to NA. To do that, we use the mutate_at() function (which will soon be superseded by mutate() with across()) to specify which variables we want to apply our mutating function to, and we use the if_else() function to specify what to replace the value with if the condition is true or false.\n\ndiabetes_clean &lt;- diabetes_orig %&gt;%\n  mutate_at(vars(triceps, glucose, pressure, insulin, mass), \n            function(.var) { \n              if_else(condition = (.var == 0), # if true (i.e. the entry is 0)\n                      true = as.numeric(NA),  # replace the value with NA\n                      false = .var # otherwise leave it as it is\n                      )\n            })\n\nOur data is ready. Hopefully you’ve replenished your cup of tea (or coffee if you’re into that for some reason). Let’s start making some tidy models!\n\n\nSplit into train/test\nFirst, let’s split our dataset into training and testing data. The training data will be used to fit our model and tune its parameters, where the testing data will be used to evaluate our final model’s performance.\nThis split can be done automatically using the inital_split() function (from rsample) which creates a special “split” object.\n\nset.seed(234589)\n# split the data into trainng (75%) and testing (25%)\ndiabetes_split &lt;- initial_split(diabetes_clean, \n                                prop = 3/4)\ndiabetes_split\n\n&lt;Training/Testing/Total&gt;\n&lt;576/192/768&gt;\n\n\nThe printed output of diabetes_split, our split object, tells us how many observations we have in the training set, the testing set, and overall: &lt;train/test/total&gt;.\nThe training and testing sets can be extracted from the “split” object using the training() and testing() functions. Although, we won’t actually use these objects in the pipeline (we will be using the diabetes_split object itself).\n\n# extract training and testing sets\ndiabetes_train &lt;- training(diabetes_split)\ndiabetes_test &lt;- testing(diabetes_split)\n\nAt some point we’re going to want to do some parameter tuning, and to do that we’re going to want to use cross-validation. So we can create a cross-validated version of the training set in preparation for that moment using vfold_cv().\n\n# create CV object from training data\ndiabetes_cv &lt;- vfold_cv(diabetes_train)\n\n\n\nDefine a recipe\nRecipes allow you to specify the role of each variable as an outcome or predictor variable (using a “formula”), and any pre-processing steps you want to conduct (such as normalization, imputation, PCA, etc).\nCreating a recipe has two parts (layered on top of one another using pipes %&gt;%):\n\nSpecify the formula (recipe()): specify the outcome variable and predictor variables\nSpecify pre-processing steps (step_zzz()): define the pre-processing steps, such as imputation, creating dummy variables, scaling, and more\n\nFor instance, we can define the following recipe\n\n# define the recipe\ndiabetes_recipe &lt;- \n  # which consists of the formula (outcome ~ predictors)\n  recipe(diabetes ~ pregnant + glucose + pressure + triceps + \n           insulin + mass + pedigree + age, \n         data = diabetes_clean) %&gt;%\n  # and some pre-processing steps\n  step_normalize(all_numeric()) %&gt;%\n  step_impute_knn(all_predictors())\n\nIf you’ve ever seen formulas before (e.g. using the lm() function in R), you might have noticed that we could have written our formula much more efficiently using the formula short-hand where . represents all of the variables in the data: outcome ~ . will fit a model that predicts the outcome using all other columns.\nThe full list of pre-processing steps available can be found here. In the recipe steps above we used the functions all_numeric() and all_predictors() as arguments to the pre-processing steps. These are called “role selections”, and they specify that we want to apply the step to “all numeric” variables or “all predictor variables”. The list of all potential role selectors can be found by typing ?selections into your console.\nNote that we used the original diabetes_clean data object (we set recipe(..., data = diabetes_clean)), rather than the diabetes_train object or the diabetes_split object. It turns out we could have used any of these. All recipes takes from the data object at this point is the names and roles of the outcome and predictor variables. We will apply this recipe to specific datasets later. This means that for large data sets, the head of the data could be used to pass the recipe a smaller data set to save time and memory.\nIndeed, if we print a summary of the diabetes_recipe object, it just shows us how many predictor variables we’ve specified and the steps we’ve specified (but it doesn’t actually implement them yet!).\n\ndiabetes_recipe\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 8\n\n\n\n\n\n── Operations \n\n\n• Centering and scaling for: all_numeric()\n\n\n• K-nearest neighbor imputation for: all_predictors()\n\n\nIf you want to extract the pre-processed dataset itself, you can first prep() the recipe for a specific dataset and juice() the prepped recipe to extract the pre-processed data. It turns out that extracting the pre-processed data isn’t actually necessary for the pipeline, since this will be done under the hood when the model is fit, but sometimes it’s useful anyway.\n\ndiabetes_train_preprocessed &lt;- diabetes_recipe %&gt;%\n  # apply the recipe to the training data\n  prep(diabetes_train) %&gt;%\n  # extract the pre-processed training dataset\n  juice()\ndiabetes_train_preprocessed\n\n# A tibble: 576 × 9\n   pregnant glucose pressure triceps insulin     mass pedigree     age diabetes\n      &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;fct&gt;   \n 1   1.23   -0.390    0.262   0.892   -0.317 -0.656      0.502 -0.201  pos     \n 2   0.0447  1.09    -0.0616 -0.0348  -0.219 -0.168     -0.400  0.301  neg     \n 3   1.82    1.91    -0.224   0.595    0.146  0.379     -0.813  0.301  neg     \n 4  -1.14   -0.0948  -0.710  -0.591   -0.522 -0.311      3.76  -1.04   neg     \n 5  -0.548   0.233    0.424   0.706    0.239  1.56       2.25  -0.201  pos     \n 6   0.637  -0.0620  -1.84   -0.683    0.190 -0.771      2.53  -0.0335 pos     \n 7  -0.844  -1.64    -2.01   -1.05    -0.629 -1.73      -0.445 -0.953  neg     \n 8  -0.548  -0.423   -1.68   -0.313   -0.735  0.00505   -0.460 -0.953  neg     \n 9  -1.14    0.463   -0.386   1.17     0.796  1.41      -0.320 -0.786  pos     \n10   1.53    1.41     0.424   0.150    0.877  0.0482    -0.968  0.969  pos     \n# … with 566 more rows\n\n\nI wrote a much longer post on recipes if you’d like to check out more details. However, note that the preparation and bake steps described in that post are no longer necessary in the tidymodels pipeline, since they’re now implemented under the hood by the later model fitting functions in this pipeline.\n\n\nSpecify the model\nSo far we’ve split our data into training/testing, and we’ve specified our pre-processing steps using a recipe. The next thing we want to specify is our model (using the parsnip package).\nParsnip offers a unified interface for the massive variety of models that exist in R. This means that you only have to learn one way of specifying a model, and you can use this specification and have it generate a linear model, a random forest model, a support vector machine model, and more with a single line of code.\nThere are a few primary components that you need to provide for the model specification\n\nThe model type: what kind of model you want to fit, set using a different function depending on the model, such as rand_forest() for random forest, logistic_reg() for logistic regression, svm_poly() for a polynomial SVM model etc. The full list of models available via parsnip can be found here.\nThe arguments: the model parameter values (now consistently named across different models), set using set_args().\nThe engine: the underlying package the model should come from (e.g. “ranger” for the ranger implementation of Random Forest), set using set_engine().\nThe mode: the type of prediction - since several packages can do both classification (binary/categorical prediction) and regression (continuous prediction), set using set_mode().\n\nFor instance, if we want to fit a random forest model as implemented by the ranger package for the purpose of classification and we want to tune the mtry parameter (the number of randomly selected variables to be considered at each split in the trees), then we would define the following model specification:\n\nrf_model &lt;- \n  # specify that the model is a random forest\n  rand_forest() %&gt;%\n  # specify that the `mtry` parameter needs to be tuned\n  set_args(mtry = tune()) %&gt;%\n  # select the engine/package that underlies the model\n  set_engine(\"ranger\", importance = \"impurity\") %&gt;%\n  # choose either the continuous regression or binary classification mode\n  set_mode(\"classification\") \n\nIf you want to be able to examine the variable importance of your final model later, you will need to set importance argument when setting the engine. For ranger, the importance options are \"impurity\" or \"permutation\".\nAs another example, the following code would instead specify a logistic regression model from the glm package.\n\nlr_model &lt;- \n  # specify that the model is a random forest\n  logistic_reg() %&gt;%\n  # select the engine/package that underlies the model\n  set_engine(\"glm\") %&gt;%\n  # choose either the continuous regression or binary classification mode\n  set_mode(\"classification\") \n\nNote that this code doesn’t actually fit the model. Like the recipe, it just outlines a description of the model. Moreover, setting a parameter to tune() means that it will be tuned later in the tune stage of the pipeline (i.e. the value of the parameter that yields the best performance will be chosen). You could also just specify a particular value of the parameter if you don’t want to tune it e.g. using set_args(mtry = 4).\nAnother thing to note is that nothing about this model specification is specific to the diabetes dataset.\n\n\nPut it all together in a workflow\nWe’re now ready to put the model and recipes together into a workflow. You initiate a workflow using workflow() (from the workflows package) and then you can add a recipe and add a model to it.\n\n# set the workflow\nrf_workflow &lt;- workflow() %&gt;%\n  # add the recipe\n  add_recipe(diabetes_recipe) %&gt;%\n  # add the model\n  add_model(rf_model)\n\nNote that we still haven’t yet implemented the pre-processing steps in the recipe nor have we fit the model. We’ve just written the framework. It is only when we tune the parameters or fit the model that the recipe and model frameworks are actually implemented.\n\n\nTune the parameters\nSince we had a parameter that we designated to be tuned (mtry), we need to tune it (i.e. choose the value that leads to the best performance) before fitting our model. If you don’t have any parameters to tune, you can skip this step.\nNote that we will do our tuning using the cross-validation object (diabetes_cv). To do this, we specify the range of mtry values we want to try, and then we add a tuning layer to our workflow using tune_grid() (from the tune package). Note that we focus on two metrics: accuracy and roc_auc (from the yardstick package).\n\n# specify which values eant to try\nrf_grid &lt;- expand.grid(mtry = c(3, 4, 5))\n# extract results\nrf_tune_results &lt;- rf_workflow %&gt;%\n  tune_grid(resamples = diabetes_cv, #CV object\n            grid = rf_grid, # grid of values to try\n            metrics = metric_set(accuracy, roc_auc) # metrics we care about\n            )\n\nYou can tune multiple parameters at once by providing multiple parameters to the expand.grid() function, e.g. expand.grid(mtry = c(3, 4, 5), trees = c(100, 500)).\nIt’s always a good idea to explore the results of the cross-validation. collect_metrics() is a really handy function that can be used in a variety of circumstances to extract any metrics that have been calculated within the object it’s being used on. In this case, the metrics come from the cross-validation performance across the different values of the parameters.\n\n# print results\nrf_tune_results %&gt;%\n  collect_metrics()\n\n# A tibble: 6 × 7\n   mtry .metric  .estimator  mean     n std_err .config             \n  &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1     3 accuracy binary     0.762    10  0.0108 Preprocessor1_Model1\n2     3 roc_auc  binary     0.840    10  0.0116 Preprocessor1_Model1\n3     4 accuracy binary     0.767    10  0.0130 Preprocessor1_Model2\n4     4 roc_auc  binary     0.840    10  0.0128 Preprocessor1_Model2\n5     5 accuracy binary     0.766    10  0.0108 Preprocessor1_Model3\n6     5 roc_auc  binary     0.837    10  0.0118 Preprocessor1_Model3\n\n\nAcross both accuracy and AUC, mtry = 4 yields the best performance (just).\n\n\nFinalize the workflow\nWe want to add a layer to our workflow that corresponds to the tuned parameter, i.e. sets mtry to be the value that yielded the best results. If you didn’t tune any parameters, you can skip this step.\nWe can extract the best value for the accuracy metric by applying the select_best() function to the tune object.\n\nparam_final &lt;- rf_tune_results %&gt;%\n  select_best(metric = \"accuracy\")\nparam_final\n\n# A tibble: 1 × 2\n   mtry .config             \n  &lt;dbl&gt; &lt;chr&gt;               \n1     4 Preprocessor1_Model2\n\n\nThen we can add this parameter to the workflow using the finalize_workflow() function.\n\nrf_workflow &lt;- rf_workflow %&gt;%\n  finalize_workflow(param_final)\n\n\n\nEvaluate the model on the test set\nNow we’ve defined our recipe, our model, and tuned the model’s parameters, we’re ready to actually fit the final model. Since all of this information is contained within the workflow object, we will apply the last_fit() function to our workflow and our train/test split object. This will automatically train the model specified by the workflow using the training data, and produce evaluations based on the test set.\n\nrf_fit &lt;- rf_workflow %&gt;%\n  # fit on the training set and evaluate on test set\n  last_fit(diabetes_split)\n\nNote that the fit object that is created is a data-frame-like object; specifically, it is a tibble with list columns.\n\nrf_fit\n\n# Resampling results\n# Manual resampling \n# A tibble: 1 × 6\n  splits            id               .metrics .notes   .predictions .workflow \n  &lt;list&gt;            &lt;chr&gt;            &lt;list&gt;   &lt;list&gt;   &lt;list&gt;       &lt;list&gt;    \n1 &lt;split [576/192]&gt; train/test split &lt;tibble&gt; &lt;tibble&gt; &lt;tibble&gt;     &lt;workflow&gt;\n\n\nThis is a really nice feature of tidymodels (and is what makes it work so nicely with the tidyverse) since you can do all of your tidyverse operations to the model object. While truly taking advantage of this flexibility requires proficiency with purrr, if you don’t want to deal with purrr and list-columns, there are functions that can extract the relevant information from the fit object that remove the need for purrr as we will see below.\nSince we supplied the train/test object when we fit the workflow, the metrics are evaluated on the test set. Now when we use the collect_metrics() function (recall we used this when tuning our parameters), it extracts the performance of the final model (since rf_fit now consists of a single final model) applied to the test set.\n\ntest_performance &lt;- rf_fit %&gt;% collect_metrics()\ntest_performance\n\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary         0.792 Preprocessor1_Model1\n2 roc_auc  binary         0.853 Preprocessor1_Model1\n\n\nOverall the performance is very good, with an accuracy of 0.74 and an AUC of 0.82.\nYou can also extract the test set predictions themselves using the collect_predictions() function. Note that there are 192 rows in the predictions object below which matches the number of test set observations (just to give you some evidence that these are based on the test set rather than the training set).\n\n# generate predictions from the test set\ntest_predictions &lt;- rf_fit %&gt;% collect_predictions()\ntest_predictions\n\n# A tibble: 192 × 7\n   id               .pred_neg .pred_pos  .row .pred_class diabetes .config      \n   &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt; &lt;int&gt; &lt;fct&gt;       &lt;fct&gt;    &lt;chr&gt;        \n 1 train/test split     0.365    0.635      3 pos         pos      Preprocessor…\n 2 train/test split     0.213    0.787      9 pos         pos      Preprocessor…\n 3 train/test split     0.765    0.235     11 neg         neg      Preprocessor…\n 4 train/test split     0.511    0.489     13 neg         neg      Preprocessor…\n 5 train/test split     0.594    0.406     18 neg         pos      Preprocessor…\n 6 train/test split     0.356    0.644     26 pos         pos      Preprocessor…\n 7 train/test split     0.209    0.791     32 pos         pos      Preprocessor…\n 8 train/test split     0.751    0.249     50 neg         neg      Preprocessor…\n 9 train/test split     0.961    0.0385    53 neg         neg      Preprocessor…\n10 train/test split     0.189    0.811     55 pos         neg      Preprocessor…\n# … with 182 more rows\n\n\nSince this is just a normal data frame/tibble object, we can generate summaries and plots such as a confusion matrix.\n\n# generate a confusion matrix\ntest_predictions %&gt;% \n  conf_mat(truth = diabetes, estimate = .pred_class)\n\n          Truth\nPrediction neg pos\n       neg 107  17\n       pos  23  45\n\n\nWe could also plot distributions of the predicted probability distributions for each class.\n\ntest_predictions %&gt;%\n  ggplot() +\n  geom_density(aes(x = .pred_pos, fill = diabetes), \n               alpha = 0.5)\n\n\n\n\n\n\n\n\nIf you’re familiar with purrr, you could use purrr functions to extract the predictions column using pull(). The following code does almost the same thing as collect_predictions(). You could similarly have done this with the .metrics column.\n\ntest_predictions &lt;- rf_fit %&gt;% pull(.predictions)\ntest_predictions\n\n[[1]]\n# A tibble: 192 × 6\n   .pred_neg .pred_pos  .row .pred_class diabetes .config             \n       &lt;dbl&gt;     &lt;dbl&gt; &lt;int&gt; &lt;fct&gt;       &lt;fct&gt;    &lt;chr&gt;               \n 1     0.365    0.635      3 pos         pos      Preprocessor1_Model1\n 2     0.213    0.787      9 pos         pos      Preprocessor1_Model1\n 3     0.765    0.235     11 neg         neg      Preprocessor1_Model1\n 4     0.511    0.489     13 neg         neg      Preprocessor1_Model1\n 5     0.594    0.406     18 neg         pos      Preprocessor1_Model1\n 6     0.356    0.644     26 pos         pos      Preprocessor1_Model1\n 7     0.209    0.791     32 pos         pos      Preprocessor1_Model1\n 8     0.751    0.249     50 neg         neg      Preprocessor1_Model1\n 9     0.961    0.0385    53 neg         neg      Preprocessor1_Model1\n10     0.189    0.811     55 pos         neg      Preprocessor1_Model1\n# … with 182 more rows\n\n\n\n\nFitting and using your final model\nThe previous section evaluated the model trained on the training data using the testing data. But once you’ve determined your final model, you often want to train it on your full dataset and then use it to predict the response for new data.\nIf you want to use your model to predict the response for new observations, you need to use the fit() function on your workflow and the dataset that you want to fit the final model on (e.g. the complete training + testing dataset).\n\nfinal_model &lt;- fit(rf_workflow, diabetes_clean)\n\nThe final_model object contains a few things including the ranger object trained with the parameters established through the workflow contained in rf_workflow based on the data in diabetes_clean (the combined training and testing data).\n\nfinal_model\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_normalize()\n• step_impute_knn()\n\n── Model ───────────────────────────────────────────────────────────────────────\nRanger result\n\nCall:\n ranger::ranger(x = maybe_data_frame(x), y = y, mtry = min_cols(~4,      x), importance = ~\"impurity\", num.threads = 1, verbose = FALSE,      seed = sample.int(10^5, 1), probability = TRUE) \n\nType:                             Probability estimation \nNumber of trees:                  500 \nSample size:                      768 \nNumber of independent variables:  8 \nMtry:                             4 \nTarget node size:                 10 \nVariable importance mode:         impurity \nSplitrule:                        gini \nOOB prediction error (Brier s.):  0.1583874 \n\n\nIf we wanted to predict the diabetes status of a new woman, we could use the normal predict() function.\nFor instance, below we define the data for a new woman.\n\nnew_woman &lt;- tribble(~pregnant, ~glucose, ~pressure, ~triceps, ~insulin, ~mass, ~pedigree, ~age,\n                     2, 95, 70, 31, 102, 28.2, 0.67, 47)\nnew_woman\n\n# A tibble: 1 × 8\n  pregnant glucose pressure triceps insulin  mass pedigree   age\n     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;\n1        2      95       70      31     102  28.2     0.67    47\n\n\nThe predicted diabetes status of this new woman is “negative”.\n\npredict(final_model, new_data = new_woman)\n\n# A tibble: 1 × 1\n  .pred_class\n  &lt;fct&gt;      \n1 neg        \n\n\n\n\nVariable importance\nIf you want to extract the variable importance scores from your model, as far as I can tell, for now you need to extract the model object from the fit() object (which for us is called final_model). The function that extracts the model is pull_workflow_fit() and then you need to grab the fit object that the output contains.\n\nranger_obj &lt;- pull_workflow_fit(final_model)$fit\n\nWarning: `pull_workflow_fit()` was deprecated in workflows 0.2.3.\nℹ Please use `extract_fit_parsnip()` instead.\n\nranger_obj\n\nRanger result\n\nCall:\n ranger::ranger(x = maybe_data_frame(x), y = y, mtry = min_cols(~4,      x), importance = ~\"impurity\", num.threads = 1, verbose = FALSE,      seed = sample.int(10^5, 1), probability = TRUE) \n\nType:                             Probability estimation \nNumber of trees:                  500 \nSample size:                      768 \nNumber of independent variables:  8 \nMtry:                             4 \nTarget node size:                 10 \nVariable importance mode:         impurity \nSplitrule:                        gini \nOOB prediction error (Brier s.):  0.1583874 \n\n\nThen you can extract the variable importance from the ranger object itself (variable.importance is a specific object contained within ranger output - this will need to be adapted for the specific object type of other models).\n\nranger_obj$variable.importance\n\npregnant  glucose pressure  triceps  insulin     mass pedigree      age \n16.40687 79.68408 17.08361 22.10685 52.27195 42.60717 30.12246 33.19040"
  },
  {
    "objectID": "blog/2023-03-21-quarto-website.html",
    "href": "blog/2023-03-21-quarto-website.html",
    "title": "Thanks, Quarto, for saving my blog!",
    "section": "",
    "text": "Hello reader, it’s been a minute!\nI’m sure you didn’t notice, but I’ve been kind of silent on here for the past three years. So silent, it’s almost deafening (to my ears, at least).\nFear not, there are good reasons for my absence in this space. I’ve actually spent the past three years writing a book called “Veridical Data Science: the practice of responsible data analysis and decision making”, together with my PhD and postdoc advisor, the Veridical Data Science pioneer, Bin Yu. And I was doing this more-or-less full-time (more on veridical data science to come).\nMuch to my dismay, it turns out that after a full day of book-writing1, it’s hard to summon the energy to do even more writing in the form of blog posts. Book-burnout is real. So what was I doing with my spare time2 if I wasn’t blogging, you ask? Instead of writing blog posts, I was doing a lot of non-blogging things like weathering a pandemic, climbing dumb rocks, learning to draw, moving all around the country, and other non-blogging things like that.\nBut now that the book is “done” (as in, it’s been sent to the publishers, which, if you know anything about publishing, means that it’s actually very far from done), and I’ve settled into a new job as an Assistant Research Professor at the University of Utah, I finally feel ready to start blogging again! Hooray!\nExcept… not actually hooray. When I finally went to update my original blogdown website on my new work computer, R yelled at me. Loudly. In red. And I didn’t like it one bit. The day that I had long feared had finally arrived: I didn’t know how to update my own website without breaking it. Blogdown is fantastic until it’s not.\nFortunately, this happened at the best of possible times. Not only have substantially fewer people been following my website lately (that’ll happen when you don’t add anything new to it in three years…), so I’m not so afraid to make some breaking changes, but we are now living in the magical era of Quarto!\nPerhaps you’re like me, and it’s taken you a while to realize why this new era of Quarto is so magic. My first impression of Quarto was that it’s pretty much R Markdown, but with a few extra features and slightly different syntax. This is true, and also not true.\nAfter hearing so much about how much easier so many things (like making a blog, for instance) are with Quarto, I figured that I might as well give it a go and try to port my broken blogdown website over to Quarto. The alternative seems to be spending my life debugging blogdown, so what have I got to lose?\nNow that I’m on the other side, I have to say. Wow. Just wow. I managed to re-create my entire website using quarto without running into a single issue in just 4 hours. For reference, when I originally created my blog using blogdown 5ish years ago, it took around 5 days and I had to try several different themes before I found one that was stable and didn’t break easily.\nIf you’re looking to make the transition yourself, fortunately there are already several useful resources out there, like Albert Rapp’s blog post and Nick Tierney’s blog post. The Quarto documentation itself is also really helpful.\nThe end result of my 4-hours of effort is what you see here. My website looks a bit different (and I got rid of some of the less-visited sections that no one cares about), but I think it’s much cleaner and easier to navigate. I might do some fine-tuning down the line, but I’m in no rush.\nAnyway, all this is to say: Welcome to my new website! I hope to be creating a lot more content for you in the near future. I’m excited to oil to my blogging gears, and get back on the road!"
  },
  {
    "objectID": "blog/2020-07-09-across.html",
    "href": "blog/2020-07-09-across.html",
    "title": "Across (dplyr 1.0.0): applying dplyr functions simultaneously across multiple columns",
    "section": "",
    "text": "I often find that I want to use a dplyr function on multiple columns at once. For instance, perhaps I want to scale all of the numeric variables at once using a mutate function, or I want to provide the same summary for three of my variables.\nWhile it’s been possible to do such tasks for a while using scoped verbs, it’s now even easier - and more consistent - using dplyr’s new across() function.\nTo demonstrate across(), I’m going to use Palmer’s Penguin dataset, which was originally collected and made available by Dr. Kristen Gorman and the Palmer Station, Antarctica LTER, but has recently been made popular in the R community by Allison Horst as an alternative to the over-used Iris dataset.\nTo start with, let’s load the penguins dataset (via the palmerpenguins package) and the tidyverse package. If you’re new to the tidyverse (primarily to dplyr and piping, %&gt;%), I suggest taking a look at my post on the tidyverse before reading this post.\n\n# remotes::install_github(\"allisonhorst/palmerpenguins\")\nlibrary(palmerpenguins)\nlibrary(tidyverse)\n\n\npenguins\n\n# A tibble: 344 × 8\n   species island    bill_length_mm bill_depth_mm flipper_…¹ body_…² sex    year\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;      &lt;int&gt;   &lt;int&gt; &lt;fct&gt; &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7        181    3750 male   2007\n 2 Adelie  Torgersen           39.5          17.4        186    3800 fema…  2007\n 3 Adelie  Torgersen           40.3          18          195    3250 fema…  2007\n 4 Adelie  Torgersen           NA            NA           NA      NA &lt;NA&gt;   2007\n 5 Adelie  Torgersen           36.7          19.3        193    3450 fema…  2007\n 6 Adelie  Torgersen           39.3          20.6        190    3650 male   2007\n 7 Adelie  Torgersen           38.9          17.8        181    3625 fema…  2007\n 8 Adelie  Torgersen           39.2          19.6        195    4675 male   2007\n 9 Adelie  Torgersen           34.1          18.1        193    3475 &lt;NA&gt;   2007\n10 Adelie  Torgersen           42            20.2        190    4250 &lt;NA&gt;   2007\n# … with 334 more rows, and abbreviated variable names ¹​flipper_length_mm,\n#   ²​body_mass_g\n\n\nThere are 344 rows in the penguins dataset, one for each penguin, and 7 columns. The first two columns, species and island, specify the species and island of the penguin, the next four specify numeric traits about the penguin, including the bill and flipper length, the bill depth and the body mass.\nThe new across() function turns all dplyr functions into “scoped” versions of themselves, which means you can specify multiple columns that your dplyr function will apply to.\nOrdinarily, if we want to summarise a single column, such as species, by calculating the number of distinct entries (using n_distinct()) it contains, we would typically write\n\npenguins %&gt;%\n  summarise(distinct_species = n_distinct(species))\n\n# A tibble: 1 × 1\n  distinct_species\n             &lt;int&gt;\n1                3\n\n\nIf we wanted to calculate n_distinct() not only across species, but also across island and sex, we would need to write out the n_distinct function three separate times:\n\npenguins %&gt;%\n  summarise(distinct_species = n_distinct(species),\n            distinct_island = n_distinct(island),\n            distinct_sex = n_distinct(sex))\n\n# A tibble: 1 × 3\n  distinct_species distinct_island distinct_sex\n             &lt;int&gt;           &lt;int&gt;        &lt;int&gt;\n1                3               3            3\n\n\nWouldn’t it be nice if we could just write which columns we want to apply n_distinct() to, and then specify n_distinct() once, rather than having to apply n_distinct to each column separately?\nThis is where across() comes in. It is used inside your favourite dplyr function and the syntax is across(.cols, .fnd), where .cols specifies the columns that you want the dplyr function to act on. When dplyr functions involve external functions that you’re applying to columns e.g. n_distinct() in the example above, this external function is placed in the .fnd argument. For example, we would to apply n_distinct() to species, island, and sex, we would write across(c(species, island, sex), n_distinct) in the summarise parentheses.\nNote that we are specifying which variables we want to involve in the summarise using c(), as if we’re listing the variable names in a vector, but because we’re in dplyr-land, we don’t need to put them in quotes:\n\npenguins %&gt;%\n  summarise(across(c(species, island, sex), \n                   n_distinct))\n\n# A tibble: 1 × 3\n  species island   sex\n    &lt;int&gt;  &lt;int&gt; &lt;int&gt;\n1       3      3     3\n\n\nSomething else that’s really neat is that you can also use !c() to negate a set of variables (i.e. to apply the function to all variables except those that you specified in c()):\n\npenguins %&gt;%\n  summarise(across(!c(species, island, sex), \n                   n_distinct))\n\n# A tibble: 1 × 5\n  bill_length_mm bill_depth_mm flipper_length_mm body_mass_g  year\n           &lt;int&gt;         &lt;int&gt;             &lt;int&gt;       &lt;int&gt; &lt;int&gt;\n1            165            81                56          95     3\n\n\nI want to emphasize here that the function n_distinct() is an argument of across(), rather than being an argument of the dplyr function (summarise).\n\nSelect helpers: selecting columns to apply the function to\nSo far we’ve seen how to apply a dplyr function to a set of columns using a vector notation c(col1, col2, col3, ...). However, there are many other ways to specify the columns that you want to apply the dplyr function to.\n\neverything(): apply the function to all of the columns\n\n\npenguins %&gt;%\n  summarise(across(everything(), n_distinct))\n\n# A tibble: 1 × 8\n  species island bill_length_mm bill_depth_mm flipper_leng…¹ body_…²   sex  year\n    &lt;int&gt;  &lt;int&gt;          &lt;int&gt;         &lt;int&gt;          &lt;int&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1       3      3            165            81             56      95     3     3\n# … with abbreviated variable names ¹​flipper_length_mm, ²​body_mass_g\n\n\n\nstarts_with(): apply the function to all columns whose name starts with a specific string\n\n\npenguins %&gt;%\n  summarise(across(starts_with(\"bill\"), n_distinct))\n\n# A tibble: 1 × 2\n  bill_length_mm bill_depth_mm\n           &lt;int&gt;         &lt;int&gt;\n1            165            81\n\n\n\ncontains(): apply the function to all columns whose name contains a specific string\n\n\npenguins %&gt;%\n  summarise(across(contains(\"length\"), n_distinct))\n\n# A tibble: 1 × 2\n  bill_length_mm flipper_length_mm\n           &lt;int&gt;             &lt;int&gt;\n1            165                56\n\n\n\nwhere() apply the function to all columns that satisfy a logical condition, such as is.numeric()\n\n\npenguins %&gt;%\n  summarise(across(where(is.numeric), n_distinct))\n\n# A tibble: 1 × 5\n  bill_length_mm bill_depth_mm flipper_length_mm body_mass_g  year\n           &lt;int&gt;         &lt;int&gt;             &lt;int&gt;       &lt;int&gt; &lt;int&gt;\n1            165            81                56          95     3\n\n\nThe full list of select helpers can be found here.\n\n\nUsing in-line functions with across\nLet’s look at an example of summarizing the columns using a custom function (rather than n_distinct()). I usually do this using the tilde-dot shorthand for inline functions. The notation works by replacing\n\nfunction(x) {\n  x + 10\n}\n\nwith\n\n~{.x + 10}\n\n~ indicates that you have started an anonymous function, and the argument of the anonymous function can be referred to using .x (or simply .). Unlike normal function arguments that can be anything that you like, the tilde-dot function argument is always .x.\nFor instance, to identify how many missing values there are in every column, we could specify the inline function ~sum(is.na(.)), which calculates how many NA values are in each column (where the column is represented by .) and adds them up:\n\npenguins %&gt;%\n  summarise(across(everything(), \n                   ~sum(is.na(.))))\n\n# A tibble: 1 × 8\n  species island bill_length_mm bill_depth_mm flipper_leng…¹ body_…²   sex  year\n    &lt;int&gt;  &lt;int&gt;          &lt;int&gt;         &lt;int&gt;          &lt;int&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1       0      0              2             2              2       2    11     0\n# … with abbreviated variable names ¹​flipper_length_mm, ²​body_mass_g\n\n\nThis shows that there are missing values in every column except for the first two (species and island).\n\n\nA mutate example\nWhat if we want to replace the missing values in the numeric columns with 0 (clearly a terrible choice)? Without the across() function, we would apply an if_else() function separately to each numeric column, which will replace all NA values with 0 and leave all non-NA values as they are:\n\nreplace0 &lt;- function(x) {\n  if_else(condition = is.na(x), \n          true = 0, \n          false = as.numeric(x))\n}\npenguins %&gt;%\n  mutate(bill_length_mm = replace0(bill_length_mm),\n         bill_depth_mm = replace0(bill_depth_mm),\n         flipper_length_mm = replace0(flipper_length_mm),\n         body_mass_g = replace0(body_mass_g))\n\n# A tibble: 344 × 8\n   species island    bill_length_mm bill_depth_mm flipper_…¹ body_…² sex    year\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt; &lt;fct&gt; &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7        181    3750 male   2007\n 2 Adelie  Torgersen           39.5          17.4        186    3800 fema…  2007\n 3 Adelie  Torgersen           40.3          18          195    3250 fema…  2007\n 4 Adelie  Torgersen            0             0            0       0 &lt;NA&gt;   2007\n 5 Adelie  Torgersen           36.7          19.3        193    3450 fema…  2007\n 6 Adelie  Torgersen           39.3          20.6        190    3650 male   2007\n 7 Adelie  Torgersen           38.9          17.8        181    3625 fema…  2007\n 8 Adelie  Torgersen           39.2          19.6        195    4675 male   2007\n 9 Adelie  Torgersen           34.1          18.1        193    3475 &lt;NA&gt;   2007\n10 Adelie  Torgersen           42            20.2        190    4250 &lt;NA&gt;   2007\n# … with 334 more rows, and abbreviated variable names ¹​flipper_length_mm,\n#   ²​body_mass_g\n\n\nBut fortunately, we can do this a lot more efficiently with across().\n\n# define a function to replace NA with 0\n\npenguins %&gt;%\n  mutate(across(where(is.numeric), replace0))\n\n# A tibble: 344 × 8\n   species island    bill_length_mm bill_depth_mm flipper_…¹ body_…² sex    year\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt;\n 1 Adelie  Torgersen           39.1          18.7        181    3750 male   2007\n 2 Adelie  Torgersen           39.5          17.4        186    3800 fema…  2007\n 3 Adelie  Torgersen           40.3          18          195    3250 fema…  2007\n 4 Adelie  Torgersen            0             0            0       0 &lt;NA&gt;   2007\n 5 Adelie  Torgersen           36.7          19.3        193    3450 fema…  2007\n 6 Adelie  Torgersen           39.3          20.6        190    3650 male   2007\n 7 Adelie  Torgersen           38.9          17.8        181    3625 fema…  2007\n 8 Adelie  Torgersen           39.2          19.6        195    4675 male   2007\n 9 Adelie  Torgersen           34.1          18.1        193    3475 &lt;NA&gt;   2007\n10 Adelie  Torgersen           42            20.2        190    4250 &lt;NA&gt;   2007\n# … with 334 more rows, and abbreviated variable names ¹​flipper_length_mm,\n#   ²​body_mass_g\n\n\nAlthough obviously 0 isn’t a great choice, so perhaps we can replace the missing values with the mean value of the column. This time, rather than define a new function (in place of replace0), we’ll be a bit more concise and use the tilde-dot notation to specify the function we want to apply.\n\npenguins %&gt;%\n  mutate(across(where(is.numeric), ~if_else(is.na(.), mean(., na.rm = T), as.numeric(.))))\n\n# A tibble: 344 × 8\n   species island    bill_length_mm bill_depth_mm flipper_…¹ body_…² sex    year\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt;\n 1 Adelie  Torgersen           39.1          18.7       181    3750  male   2007\n 2 Adelie  Torgersen           39.5          17.4       186    3800  fema…  2007\n 3 Adelie  Torgersen           40.3          18         195    3250  fema…  2007\n 4 Adelie  Torgersen           43.9          17.2       201.   4202. &lt;NA&gt;   2007\n 5 Adelie  Torgersen           36.7          19.3       193    3450  fema…  2007\n 6 Adelie  Torgersen           39.3          20.6       190    3650  male   2007\n 7 Adelie  Torgersen           38.9          17.8       181    3625  fema…  2007\n 8 Adelie  Torgersen           39.2          19.6       195    4675  male   2007\n 9 Adelie  Torgersen           34.1          18.1       193    3475  &lt;NA&gt;   2007\n10 Adelie  Torgersen           42            20.2       190    4250  &lt;NA&gt;   2007\n# … with 334 more rows, and abbreviated variable names ¹​flipper_length_mm,\n#   ²​body_mass_g\n\n\nOr better yet, perhaps we can replace the missing values with the average value within the relevant species and island.\n\npenguins %&gt;%\n  group_by(species, island) %&gt;%\n  mutate(across(where(is.numeric), \n                ~if_else(condition = is.na(.), \n                         true = mean(., na.rm = T), \n                         false = as.numeric(.)))) %&gt;%\n  ungroup()\n\n# A tibble: 344 × 8\n   species island    bill_length_mm bill_depth_mm flipper_…¹ body_…² sex    year\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt;\n 1 Adelie  Torgersen           39.1          18.7       181    3750  male   2007\n 2 Adelie  Torgersen           39.5          17.4       186    3800  fema…  2007\n 3 Adelie  Torgersen           40.3          18         195    3250  fema…  2007\n 4 Adelie  Torgersen           39.0          18.4       191.   3706. &lt;NA&gt;   2007\n 5 Adelie  Torgersen           36.7          19.3       193    3450  fema…  2007\n 6 Adelie  Torgersen           39.3          20.6       190    3650  male   2007\n 7 Adelie  Torgersen           38.9          17.8       181    3625  fema…  2007\n 8 Adelie  Torgersen           39.2          19.6       195    4675  male   2007\n 9 Adelie  Torgersen           34.1          18.1       193    3475  &lt;NA&gt;   2007\n10 Adelie  Torgersen           42            20.2       190    4250  &lt;NA&gt;   2007\n# … with 334 more rows, and abbreviated variable names ¹​flipper_length_mm,\n#   ²​body_mass_g\n\n\n\n\nA select example\nWhen you’re using select, you don’t have to include the across() function, because the select helpers have always worked with select(). This means that you can just write\n\npenguins %&gt;%\n  select(where(is.numeric))\n\n# A tibble: 344 × 5\n   bill_length_mm bill_depth_mm flipper_length_mm body_mass_g  year\n            &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;int&gt;\n 1           39.1          18.7               181        3750  2007\n 2           39.5          17.4               186        3800  2007\n 3           40.3          18                 195        3250  2007\n 4           NA            NA                  NA          NA  2007\n 5           36.7          19.3               193        3450  2007\n 6           39.3          20.6               190        3650  2007\n 7           38.9          17.8               181        3625  2007\n 8           39.2          19.6               195        4675  2007\n 9           34.1          18.1               193        3475  2007\n10           42            20.2               190        4250  2007\n# … with 334 more rows\n\n\nrather than\n\npenguins %&gt;%\n  select(across(where(is.numeric)))\n\nwhich will throw an error.\nHopefully across() will make your life easier, as it has mine!"
  },
  {
    "objectID": "blog/2020-02-03_blogger.html",
    "href": "blog/2020-02-03_blogger.html",
    "title": "Becoming an R blogger",
    "section": "",
    "text": "This year I was given the opportunity to talk at rstudio::conf(2020), which, if you’ve never been, is one of those rare conferences where every person you meet is both extremely friendly and excited about R, and you learn a million fun and useful things that you can actually use. 10/10, would go again.\nMy talk was about blogging about R. Specifically, about why I blog, and why you should too, while also giving some tips and tricks for getting started. My slides can are here, if you just want to flick through the slides and get the tl;dr.\n\n\nRead on below if you’re interested in learning more!\n\nWhy start a blog?\nMany years ago, I began to realize that I love to teach and explain things. As it turned out, it doesn’t really matter what the things are, but since I happened to be getting a PhD in statistics, I figured I might as well apply my love of explaining things to my work. Since I decided that I might want to do explain things as a career one day, I decided to start a portfolio in the form of a blog, which turned out to be a great way to practice my explaining skills (which have undoubtedly improved over the past few years). To be honest, I never really expected anyone to pay attention to the things I wrote (and for many years, they didn’t), but I’m pretty excited that you’re here reading this now!\nOne of the main benefits I’ve found since I started my blog several years ago is that I’ve learnt so many things. For starters, having to come up with topics to write about is an excellent way to keep up with what’s current. If you hear about a new R package or statistical concept floating around, but you can’t think of an excuse to learn about it in your every-day work, write a blog post about it! Having to explain something is an excellent way to learn it. Writing a blog post also turns out to be a great way to procrastinate during those afternoons when you can’t force yourself to do your actual work, but you still want to give the illusion (both to your boss and yourself) that you’re being productive.\nSo my top 4 reasons to start a blog:\n\nTo learn!\nPortfolio & gain exposure (people will start to know who you are)\nTo practice your communication skills\nProductive procrastination\n\nBonus reason: someone else might find what you wrote useful and send you a nice email, or even give you a job!\n\n\nHow to choose a topic\nSo you’ve decided you want to start a blog. That’s so awesome! Now how in the world are you supposed to decide what to write about?\nI tend to choose topics in one of two ways:\nWrite about something that you just learned. Especially if you had a hard time learning it, because there weren’t any good resources out there. This is a definite indication that there is a need for a good post about a topic. It happens surprisingly often, even with widely used R packages.\nAlternatively, write about something you want to learn. As I mentioned above, having to explain something to someone else is a great way to learn something thoroughly in the first place.\n\n\nUse interesting and easily accessible data examples\nI always try to use interesting data examples (such as the gapminder dataset), or examples that are humorous in some way (such as the baking data example for my recipes blogpost).\nMaking it very easy for your reader to access the data in their own console is highly recommended. For instance, instead of proving a link where the user can download the data locally to their computer and then load it in to their environment (since who knows where it will end up on their computer relative to their working directory), try to use data examples that can be downloaded directly from a URL. For instance:\n\nmuffin_cupcake_data_orig &lt;- read_csv(\"https://raw.githubusercontent.com/adashofdata/muffin-cupcake/master/recipes_muffins_cupcakes.csv\")\n\n\n\nKeep it simple\nWhen explaining technical concepts, try to stay away from complex terminology, jargon, notation, and if possible, stay away from math altogether. Opt instead for metaphors and images that explain a concept. You’re writing a blog post, not a text book. My advice is always: explain it to your grandmother. Always keep your audience in mind. Err on the side of being too detailed, and assume that your audience is a total newbie. Then the people who already know the basics can skip over your intro-level descriptions and take away the high level info, but this way you’re still making your post accessible to absolute newbies.\nIn addition, avoid saying that things are “obvious”, “easy”, or “trivial”. Just because they are for you, doesn’t mean that they are for everyone, and you’re likely to scare away people who don’t find it obvious, easy, or trivial.\n\n\nCreating and hosting your blog\nI write my blog in RStudio using the blogdown R package, which magically combines many .Rmd files into a webpage. It’s amazing. There are many pre-existing themes that you can choose from that you can modify with a little bit of CSS and html code if you want to.\nI also have a GitHub repository that contains my website, which is where I push any updates I make to my website to. Then I have a Netlify site that mirrors whatever is in the GitHub repository, so that any changes I push are automatically updated.\nA summary of the technical workflow to get the website set up is as follows: make a GitHub repository containing your website, tell Netlify what the repository is called and push. Bam! Your website is now ready!\nIf you’re looking for some more explicit details, here they are:\n\nStart a new github repository for your website and call it anything you like. By that I mean that it doesn’t have to be username.github.io. I called mine personal-webpage. So creative, I know.\nUse the R package blogdown to design your page with Hugo. There are lots of cool themes available. Choose a good one.\nPush your website, content and all, to your new repo.\nHead on over to netlify, create and account/log in and hit “New site from Git”.\n\n\nStep 1: Set the Continuous Deployment Git provider to GitHub (or whichever provider you use).\nStep 2: Choose the repository containing your website.\nStep 3: Set the Build command to hugo_0.19 (or whichever version you want), and the Publish directory to “public” (this is the folder in which Hugo by default puts the actual webpage files when it’s built).\n\n\nHit “Deploy site”.\nIf you like you can choose to “Change site name”.\nYour site can now be found at sitename.netlify.com!\nEvery time you push new content to the GitHub repo, Netlify will automatically rebuild and deploy your site. You can just sit back and relax :).\n\nIt’s probably a good idea to buy a domain name from somewhere like Google Domains for like $10/year and point that domain name towards your netlify site, but otherwise you’re good to go!\n\n\nSpreading the word\nTitle your blogs as something people might google, such as “A caret tutorial” (try googling this, and see what comes up ;)). Most of the people who come to my website find it on Google.\nA great way to gain a reader following is Twitter (I’m @rlbarter). Twitter is also a surprisingly effective way to keep up to date with what’s current with the R community. Even though I initially had very few followers, whenever I shared one of my blogs on twitter, it would get a surprisingly large number of re-tweets and likes. It gave me warm fuzzies.\nFinally, another great way to get experience explaining things, and to get your blog (and name) out there is to run tutorials on your favorite blog posts. Reach out to your local R Ladies or UseR! groups, or run a tutorial in your department or company.\n\n\nGo forth and blog!"
  },
  {
    "objectID": "blog/2018-05-29_getting_fancy_ggplot2.html",
    "href": "blog/2018-05-29_getting_fancy_ggplot2.html",
    "title": "Getting fancy with ggplot2: code for alternatives to grouped bar charts",
    "section": "",
    "text": "Here I provide the code I used to create the figures from my previous post on alternatives to grouped bar charts. You are encouraged to play with them yourself!\nThe key to creating unique and creative visualizations using libraries such as ggplot (or even just straight SVG) is (1) to move away from thinking of data visualization only as the default plot types (bar plots, boxplots, scatterplots, etc), and (2) to realise that most visualizations are essentially lines and circles that you can arrange however you desire in space. Drawing a picture on paper before beginning to code your data viz is a great way to create customized visualizations for each dataset.\n\nBars\nFirst, I load in the libraries (the data comes from the dslabs library), and convert the data to long-form for creating the grouped bar charts.\n\n# load in libraries\nlibrary(tidyverse)\nlibrary(dslabs)\n# combine male and female success rates into single data frame\nsuccess_rates &lt;- rbind(\n  # male success rates\n  transmute(research_funding_rates, \n            discipline, \n            success = success_rates_men, \n            gender = \"Male\"),\n  # female success rates\n  transmute(research_funding_rates, \n            discipline, \n            success = success_rates_women, \n            gender = \"Female\")) \n\n\n# print the data\nsuccess_rates\n\n            discipline success gender\n1    Chemical sciences    26.5   Male\n2    Physical sciences    19.3   Male\n3              Physics    26.9   Male\n4           Humanities    14.3   Male\n5   Technical sciences    15.9   Male\n6    Interdisciplinary    11.4   Male\n7  Earth/life sciences    24.4   Male\n8      Social sciences    15.3   Male\n9     Medical sciences    18.8   Male\n10   Chemical sciences    25.6 Female\n11   Physical sciences    23.1 Female\n12             Physics    22.2 Female\n13          Humanities    19.3 Female\n14  Technical sciences    21.0 Female\n15   Interdisciplinary    21.8 Female\n16 Earth/life sciences    14.3 Female\n17     Social sciences    11.5 Female\n18    Medical sciences    11.2 Female\n\n\nI used geom_bar() to create a grouped bar chart containing the data. I grouped by gender by setting fill = gender within the aes() function.\n\n# make grouped bar plot\nggplot(success_rates) +\n  # add bar for each discipline colored by gender\n  geom_bar(aes(x = discipline, y = success, fill = gender),\n           stat = \"identity\", position = \"dodge\") +\n  # name axes and remove gap between bars and y-axis\n  scale_y_continuous(\"Success Rate\", expand = c(0, 0)) +\n  scale_x_discrete(\"Discipline\") +\n  scale_fill_manual(values = c(\"#468189\", \"#9DBEBB\")) +\n  # remove grey theme\n  theme_classic(base_size = 18) +\n  # rotate x-axis and remove superfluous axis elements\n  theme(axis.text.x = element_text(angle = 90, \n                                   hjust = 1, vjust = 0),\n        axis.line = element_blank(),\n        axis.ticks.x = element_blank()) \n\n\n\n\n\n\n\n\n\n\nSlope plot\nTo create the sloped chart, it ends up being easier to keep the data in its original wide form.\nThe first element I add is the slopes from men to women using geom_segment(). Each slope is colored based on whether men or women are more successful.\n\ngg_slope &lt;- research_funding_rates %&gt;%\n  # add a variable for when men are more successful than women (for colours)\n  mutate(men_more_successful = success_rates_men &gt; success_rates_women) %&gt;%\n  ggplot() +\n  # add a line segment that goes from men to women for each discipline\n  geom_segment(aes(x = 1, xend = 2, \n                   y = success_rates_men, \n                   yend = success_rates_women,\n                   group = discipline,\n                   col = men_more_successful), \n               size = 1.2) +\n  # set the colors\n  scale_color_manual(values = c(\"#468189\", \"#9DBEBB\"), guide = \"none\")  +\n  # remove all axis stuff\n  theme_classic() + \n  theme(axis.line = element_blank(),\n        axis.text = element_blank(),\n        axis.title = element_blank(),\n        axis.ticks = element_blank()) \ngg_slope\n\n\n\n\n\n\n\n\nNext, I add a left and right axis at the start and end of the slopes.\n\ngg_slope &lt;- gg_slope +\n  # add vertical lines that act as axis for men\n  geom_segment(x = 1, xend = 1, \n               y = min(research_funding_rates$success_rates_men) - 2,\n               yend = max(research_funding_rates$success_rates_men) + 1,\n               col = \"grey70\", size = 0.5) +\n  # add vertical lines that act as axis for women\n  geom_segment(x = 2, xend = 2, \n               y = min(research_funding_rates$success_rates_men) - 2,\n               yend = max(research_funding_rates$success_rates_men) + 1,\n               col = \"grey70\", size = 0.5) +\n  # add the words \"men\" and \"women\" above their axes\n  geom_text(aes(x = x, y = y, label = label),\n            data = data.frame(x = 1:2, \n                              y = 2 + max(research_funding_rates$success_rates_men),\n                              label = c(\"men\", \"women\")),\n            col = \"grey30\",\n            size = 6) \ngg_slope  \n\n\n\n\n\n\n\n\nI also need to add an identifier for each discipline’s slope. I do this by annotating the plot with text.\n\ngg_slope &lt;- gg_slope +\n  # add the label and success rate for each discipline next the men axis\n  geom_text(aes(x = 1 - 0.03, \n                y = success_rates_men, \n                label = paste0(discipline, \", \", \n                               round(success_rates_men, 1), \"%\")),\n             col = \"grey30\", hjust = \"right\") +\n  # add the success rate next to each point on the women axis\n  geom_text(aes(x = 2 + 0.08, \n                y = success_rates_women, \n                label = paste0(round(success_rates_women, 1), \"%\")),\n            col = \"grey30\") +\n  # set the limits of the x-axis so that the labels are not cut off\n  scale_x_continuous(limits = c(0.5, 2.1)) \ngg_slope\n\n\n\n\n\n\n\n\nFinally, I decide to add points (for aesthetic purposes). Behind each circle, I add a slightly larger white circle to act as a border and to give a slight “gap” look.\n\ngg_slope &lt;- gg_slope + \n  \n  # add the white outline for the points at each rate for men\n  geom_point(aes(x = 1, \n                 y = success_rates_men), size = 4.5,\n             col = \"white\") +\n  # add the white outline for the points at each rate for women\n  geom_point(aes(x = 2, \n                 y = success_rates_women), size = 4.5,\n             col = \"white\") +\n  \n  # add the actual points at each rate for men\n  geom_point(aes(x = 1, \n                 y = success_rates_men), size = 4,\n             col = \"grey60\") +\n  # add the actual points at each rate for men\n  geom_point(aes(x = 2, \n                 y = success_rates_women), size = 4,\n             col = \"grey60\") \ngg_slope\n\n\n\n\n\n\n\n\n\n\nHorizontal dots\nTo create the horizontal dot plot, I again keep the data in its original wide-form but I arrange the disciplines in order of women’s success rate (this will make the plot easier to read).\nThe first things I do to create the dot plot is add the horizontal discipline lines and the points for the success rates of men and women.\n\ngg_dot &lt;- research_funding_rates %&gt;%\n  # rearrange the factor levels for discipline by rates for women\n  arrange(success_rates_women) %&gt;%\n  mutate(discipline = fct_inorder(discipline)) %&gt;%\n  \n  ggplot() +\n  # remove axes and superfluous grids\n  theme_classic() +\n  theme(axis.title = element_blank(),\n        axis.ticks.y = element_blank(),\n        axis.line = element_blank()) +\n  \n  \n  # add a dummy point for scaling purposes\n  geom_point(aes(x = 12, y = discipline), \n             size = 0, col = \"white\") + \n  \n  # add the horizontal discipline lines\n  geom_hline(yintercept = 1:9, col = \"grey80\") +\n  \n  # add a point for each male success rate\n  geom_point(aes(x = success_rates_men, y = discipline), \n             size = 11, col = \"#9DBEBB\") +\n  # add a point for each female success rate\n  geom_point(aes(x = success_rates_women, y = discipline),\n             size = 11, col = \"#468189\") \ngg_dot\n\n\n\n\n\n\n\n\nSince I really like to annotate my figures with text so that the audience can work less hard, I add the success rate on top of each dot. I also prefer to annotate features my plot directly instead of using legends, so I also add a label for men and women.\n\ngg_dot &lt;- gg_dot + \n  # add the text (%) for each male success rate\n  geom_text(aes(x = success_rates_men, y = discipline, \n                label = paste0(round(success_rates_men, 1))),\n            col = \"black\") +\n  # add the text (%) for each female success rate\n  geom_text(aes(x = success_rates_women, y = discipline, \n                label = paste0(round(success_rates_women, 1))),\n            col = \"white\") +\n  # add a label above the first two points\n  geom_text(aes(x = x, y = y, label = label, col = label),\n            data.frame(x = c(25.6 - 1.1, 26.5 + 0.6), y = 10, \n                       label = c(\"women\", \"men\")), size = 6) +\n  scale_color_manual(values = c(\"#9DBEBB\", \"#468189\"), guide = \"none\") +\n  \n  # manually specify the x-axis\n  scale_x_continuous(breaks = c(15, 20, 25), \n                     labels = c(\"15%\", \"20%\", \"25%\")) +\n  # manually set the spacing above and below the plot\n  scale_y_discrete(expand = c(0.2, 0)) \ngg_dot\n\n\n\n\n\n\n\n\n\n\nReferences\n\nThe research funding data comes from Rafael Irizarry’s dslabs R package\nThe slope and dot plot ideas for this post come from Ann K. Emery’s post on the same topic."
  },
  {
    "objectID": "blog/2019-06-06_pre_processing.html",
    "href": "blog/2019-06-06_pre_processing.html",
    "title": "Using the recipes package for easy pre-processing",
    "section": "",
    "text": "Pre-processing data in R used to be the bane of my existence. For something that should be fairly straightforward, it often really wasn’t. Often my frustrations stemmed from simple things such as factor variables having different levels in the training data and test data, or a variable having missing values in the test data but not in the training data. I’d write a function that would pre-process the training data, and when I’d try to apply it to the test data, R would cry and yell and just be generally unpleasant.\nThankfully most of the pain of pre-processing is now in the past thanks to the recipes R package that is a part of the new “tidymodels” package ecosystem (which, I guess is supposed to be equivalent to the data-focused “tidyverse” package ecosystem that includes dplyr, tidyr, and other super awesome packages like that). Recipes was developed by Max Kuhn and Hadley Wickham.\nSo let’s get baking!"
  },
  {
    "objectID": "blog/2019-06-06_pre_processing.html#the-fundamentals-of-pre-processing-your-data-using-recipes",
    "href": "blog/2019-06-06_pre_processing.html#the-fundamentals-of-pre-processing-your-data-using-recipes",
    "title": "Using the recipes package for easy pre-processing",
    "section": "The fundamentals of pre-processing your data using recipes",
    "text": "The fundamentals of pre-processing your data using recipes\nCreating a recipe has four steps:\n\nGet the ingredients (recipe()): specify the response variable and predictor variables\nWrite the recipe (step_zzz()): define the pre-processing steps, such as imputation, creating dummy variables, scaling, and more\nPrepare the recipe (prep()): provide a dataset to base each step on (e.g. if one of the steps is to remove variables that only have one unique value, then you need to give it a dataset so it can decide which variables satisfy this criteria to ensure that it is doing the same thing to every dataset you apply it to)\nBake the recipe (bake()): apply the pre-processing steps to your datasets\n\nIn this blog post I’ll walk you through these three steps, touching on the wide range of things that recipes can do, while hopefully convincing you that recipes makes life really easy and that you should use it next time you need to do some pre-processing."
  },
  {
    "objectID": "blog/2019-06-06_pre_processing.html#a-simple-example-cupcakes-or-muffins",
    "href": "blog/2019-06-06_pre_processing.html#a-simple-example-cupcakes-or-muffins",
    "title": "Using the recipes package for easy pre-processing",
    "section": "A simple example: cupcakes or muffins?",
    "text": "A simple example: cupcakes or muffins?\nTo keep things in the theme, I’m going to use a dataset from Alice Zhao’s git repo that I found when I typed “cupcake dataset” into Google. Our goal will be to classify recipes as either cupcakes or muffins based on the quantities used for each of the ingredients. So perhaps we will learn two things today: (1) how to use the recipes package, and (2) the difference between cupcakes and muffins.\n\n# set up so that all variables of tibbles are printed\noptions(dplyr.width = Inf)\n# load useful libraries\nlibrary(tidyverse)\nlibrary(recipes) # could also load the tidymodels package\n# load in the data\nmuffin_cupcake_data_orig &lt;- read_csv(\"https://raw.githubusercontent.com/adashofdata/muffin-cupcake/master/recipes_muffins_cupcakes.csv\")\n# look at data\nmuffin_cupcake_data_orig\n\n# A tibble: 20 × 9\n   Type    Flour  Milk Sugar Butter   Egg `Baking Powder` Vanilla  Salt\n   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;           &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n 1 Muffin     55    28     3      7     5               2       0     0\n 2 Muffin     47    24    12      6     9               1       0     0\n 3 Muffin     47    23    18      6     4               1       0     0\n 4 Muffin     45    11    17     17     8               1       0     0\n 5 Muffin     50    25    12      6     5               2       1     0\n 6 Muffin     55    27     3      7     5               2       1     0\n 7 Muffin     54    27     7      5     5               2       0     0\n 8 Muffin     47    26    10     10     4               1       0     0\n 9 Muffin     50    17    17      8     6               1       0     0\n10 Muffin     50    17    17     11     4               1       0     0\n11 Cupcake    39     0    26     19    14               1       1     0\n12 Cupcake    42    21    16     10     8               3       0     0\n13 Cupcake    34    17    20     20     5               2       1     0\n14 Cupcake    39    13    17     19    10               1       1     0\n15 Cupcake    38    15    23     15     8               0       1     0\n16 Cupcake    42    18    25      9     5               1       0     0\n17 Cupcake    36    14    21     14    11               2       1     0\n18 Cupcake    38    15    31      8     6               1       1     0\n19 Cupcake    36    16    24     12     9               1       1     0\n20 Cupcake    34    17    23     11    13               0       1     0\n\n\nSince the space in the column name Baking Powder is going to really annoy me, I’m going to do a quick clean where I convert all of the column names to lower case and replace the space with an underscore.\nAs a side note, I’ve started naming all of my temporary function arguments (lambda functions?) with a period preceding the name. I find it makes it a lot easier to read. As another side note, if you’ve never seen the rename_all() function before, check out my blog post on scoped verbs!\n\nmuffin_cupcake_data &lt;- muffin_cupcake_data_orig %&gt;%\n  # rename all columns \n  rename_all(function(.name) {\n    .name %&gt;% \n      # replace all names with the lowercase versions\n      tolower %&gt;%\n      # replace all spaces with underscores\n      str_replace(\" \", \"_\")\n    })\n# check that this did what I wanted\nmuffin_cupcake_data\n\n# A tibble: 20 × 9\n   type    flour  milk sugar butter   egg baking_powder vanilla  salt\n   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;         &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n 1 Muffin     55    28     3      7     5             2       0     0\n 2 Muffin     47    24    12      6     9             1       0     0\n 3 Muffin     47    23    18      6     4             1       0     0\n 4 Muffin     45    11    17     17     8             1       0     0\n 5 Muffin     50    25    12      6     5             2       1     0\n 6 Muffin     55    27     3      7     5             2       1     0\n 7 Muffin     54    27     7      5     5             2       0     0\n 8 Muffin     47    26    10     10     4             1       0     0\n 9 Muffin     50    17    17      8     6             1       0     0\n10 Muffin     50    17    17     11     4             1       0     0\n11 Cupcake    39     0    26     19    14             1       1     0\n12 Cupcake    42    21    16     10     8             3       0     0\n13 Cupcake    34    17    20     20     5             2       1     0\n14 Cupcake    39    13    17     19    10             1       1     0\n15 Cupcake    38    15    23     15     8             0       1     0\n16 Cupcake    42    18    25      9     5             1       0     0\n17 Cupcake    36    14    21     14    11             2       1     0\n18 Cupcake    38    15    31      8     6             1       1     0\n19 Cupcake    36    16    24     12     9             1       1     0\n20 Cupcake    34    17    23     11    13             0       1     0\n\n\nSince recipes does a lot of useful stuff for categorical variables as well as with missing values, I’m going to modify the data a little bit so that it’s a bit more interesting (for educational purposes only - don’t ever actually modify your data so it’s more interesting, in science that’s called “fraud”, and fraud is bad).\n\n# add an additional ingredients column that is categorical\nmuffin_cupcake_data &lt;- muffin_cupcake_data %&gt;%\n  mutate(additional_ingredients = c(\"fruit\", \n                                    \"fruit\", \n                                    \"none\", \n                                    \"nuts\", \n                                    \"fruit\", \n                                    \"fruit\", \n                                    \"nuts\", \n                                    \"none\", \n                                    \"none\", \n                                    \"nuts\",\n                                    \"icing\",\n                                    \"icing\",\n                                    \"fruit\",\n                                    \"none\",\n                                    \"fruit\",\n                                    \"icing\",\n                                    \"none\",\n                                    \"fruit\",\n                                    \"icing\",\n                                    \"icing\"))\n# add some random missing values here and there just for fun\nset.seed(26738)\nmuffin_cupcake_data &lt;- muffin_cupcake_data %&gt;%\n  # only add missing values to numeric columns\n  mutate_if(is.numeric,\n            function(x) {\n              # randomly decide if 0, 2, or 3 values will be missing from each column\n              n_missing &lt;- sample(0:3, 1)\n              # replace n_missing randomly selected values from each column with NA\n              x[sample(1:20, n_missing)] &lt;- NA\n              return(x)\n              })\nmuffin_cupcake_data\n\n# A tibble: 20 × 10\n   type    flour  milk sugar butter   egg baking_powder vanilla  salt\n   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;         &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n 1 Muffin     55    28     3      7     5             2       0     0\n 2 Muffin     47    24    12      6     9             1      NA     0\n 3 Muffin     47    23    18      6     4             1       0     0\n 4 Muffin     NA    NA    17     17     8            NA       0     0\n 5 Muffin     50    25    12      6     5             2       1     0\n 6 Muffin     55    27     3      7     5             2       1     0\n 7 Muffin     54    27     7      5     5             2       0     0\n 8 Muffin     47    26    10     10     4            NA      NA     0\n 9 Muffin     50    17    17      8     6             1       0     0\n10 Muffin     50    NA    17     11     4             1       0     0\n11 Cupcake    39     0    26     19    14             1       1     0\n12 Cupcake    42    21    16     10     8             3       0     0\n13 Cupcake    NA    17    20     20     5             2       1     0\n14 Cupcake    39    13    17     19    10             1       1     0\n15 Cupcake    38    15    23     NA     8             0       1     0\n16 Cupcake    42    18    25     NA     5             1       0     0\n17 Cupcake    36    14    21     14    11             2       1     0\n18 Cupcake    38    15    31      8     6             1       1     0\n19 Cupcake    36    16    24     12     9             1      NA     0\n20 Cupcake    34    17    23     11    13             0       1     0\n   additional_ingredients\n   &lt;chr&gt;                 \n 1 fruit                 \n 2 fruit                 \n 3 none                  \n 4 nuts                  \n 5 fruit                 \n 6 fruit                 \n 7 nuts                  \n 8 none                  \n 9 none                  \n10 nuts                  \n11 icing                 \n12 icing                 \n13 fruit                 \n14 none                  \n15 fruit                 \n16 icing                 \n17 none                  \n18 fruit                 \n19 icing                 \n20 icing                 \n\n\nFinally, I’m going to split my data into training and test sets, so that you can see how nicely our recipe can be applied to multiple data frames.\n\nlibrary(rsample)\nmuffin_cupcake_split &lt;- initial_split(muffin_cupcake_data)\nmuffin_cupcake_train &lt;- training(muffin_cupcake_split)\nmuffin_cupcake_test &lt;- testing(muffin_cupcake_split)\nrm(muffin_cupcake_data)\n\nOur training data is\n\nmuffin_cupcake_train\n\n# A tibble: 15 × 10\n   type    flour  milk sugar butter   egg baking_powder vanilla  salt\n   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;         &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n 1 Cupcake    42    18    25     NA     5             1       0     0\n 2 Muffin     50    17    17      8     6             1       0     0\n 3 Cupcake    38    15    23     NA     8             0       1     0\n 4 Cupcake    39    13    17     19    10             1       1     0\n 5 Muffin     47    26    10     10     4            NA      NA     0\n 6 Muffin     55    27     3      7     5             2       1     0\n 7 Cupcake    42    21    16     10     8             3       0     0\n 8 Muffin     50    NA    17     11     4             1       0     0\n 9 Muffin     47    23    18      6     4             1       0     0\n10 Cupcake    34    17    23     11    13             0       1     0\n11 Cupcake    39     0    26     19    14             1       1     0\n12 Cupcake    36    14    21     14    11             2       1     0\n13 Muffin     NA    NA    17     17     8            NA       0     0\n14 Muffin     50    25    12      6     5             2       1     0\n15 Cupcake    36    16    24     12     9             1      NA     0\n   additional_ingredients\n   &lt;chr&gt;                 \n 1 icing                 \n 2 none                  \n 3 fruit                 \n 4 none                  \n 5 none                  \n 6 fruit                 \n 7 icing                 \n 8 nuts                  \n 9 none                  \n10 icing                 \n11 icing                 \n12 none                  \n13 nuts                  \n14 fruit                 \n15 icing                 \n\n\nand our testing data is\n\nmuffin_cupcake_test\n\n# A tibble: 5 × 10\n  type    flour  milk sugar butter   egg baking_powder vanilla  salt\n  &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;         &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1 Muffin     55    28     3      7     5             2       0     0\n2 Muffin     47    24    12      6     9             1      NA     0\n3 Muffin     54    27     7      5     5             2       0     0\n4 Cupcake    NA    17    20     20     5             2       1     0\n5 Cupcake    38    15    31      8     6             1       1     0\n  additional_ingredients\n  &lt;chr&gt;                 \n1 fruit                 \n2 fruit                 \n3 nuts                  \n4 fruit                 \n5 fruit"
  },
  {
    "objectID": "blog/2019-06-06_pre_processing.html#writing-and-applying-the-recipe",
    "href": "blog/2019-06-06_pre_processing.html#writing-and-applying-the-recipe",
    "title": "Using the recipes package for easy pre-processing",
    "section": "Writing and applying the recipe",
    "text": "Writing and applying the recipe\nNow that we’ve set up our data, we’re ready to write some recipes and do some baking! The first thing we need to do is get the ingredients. We can use formula notation within the recipe() function to do this: the thing we’re trying to predict is the variable to the left of the ~, and the predictor variables are the things to the right of it (Since I’m including all of my variables, I could have written type ~ .).\n\n# define the recipe (it looks a lot like applying the lm function)\nmodel_recipe &lt;- recipe(type ~ flour + milk + sugar + butter + egg + \n                         baking_powder + vanilla + salt + additional_ingredients, \n                       data = muffin_cupcake_train)\n\nIf we print a summary of the model_recipe object, it just shows us the variables we’ve specified, their type, and whether they’re a predictor or an outcome.\n\nsummary(model_recipe)\n\n# A tibble: 10 × 4\n   variable               type      role      source  \n   &lt;chr&gt;                  &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n 1 flour                  &lt;chr [2]&gt; predictor original\n 2 milk                   &lt;chr [2]&gt; predictor original\n 3 sugar                  &lt;chr [2]&gt; predictor original\n 4 butter                 &lt;chr [2]&gt; predictor original\n 5 egg                    &lt;chr [2]&gt; predictor original\n 6 baking_powder          &lt;chr [2]&gt; predictor original\n 7 vanilla                &lt;chr [2]&gt; predictor original\n 8 salt                   &lt;chr [2]&gt; predictor original\n 9 additional_ingredients &lt;chr [3]&gt; predictor original\n10 type                   &lt;chr [3]&gt; outcome   original\n\n\n\nWriting the recipe steps\nSo now we have our ingredients, we are ready to write the recipe (i.e. describe our pre-processing steps). We write the recipe one step at a time. We have many steps to choose from, including:\n\nstep_dummy(): creating dummy variables from categorical variables.\nstep_zzzimpute(): where instead of “zzz” it is the name of a method, such as step_knnimpute(), step_meanimpute(), step_modeimpute(). I find that the fancier imputation methods are reeeeally slow for decently large datasets, so I would probably do this step outside of the recipes package unless you just want to do a quick mean or mode impute (which, to be honest, I often do).\nstep_scale(): normalize to have a standard deviation of 1.\nstep_center(): center to have a mean of 0.\nstep_range(): normalize numeric data to be within a pre-defined range of values.\nstep_pca(): create principal component variables from your data.\nstep_nzv(): remove variables that have (or almost have) the same value for every data point.\n\nYou can also create your own step (which I’ve never felt the need to do, but the details of which can be found here https://tidymodels.github.io/recipes/articles/Custom_Steps.html).\nIn each step, you need to specify which variables you want to apply it to. There are many ways to do this:\n\nSpecifying the variable name(s) as the first argument\nStandard dplyr selectors:\n\neverything() applies the step to all columns,\ncontains() allows you to specify column names that contain a specific string,\nstarts_with() allows you to specify column names that start with a sepcific string,\netc\n\nFunctions that specify the role of the variables:\n\nall_predictors() applies the step to the predictor variables only\nall_outcomes() applies the step to the outcome variable(s) only\n\nFunctions that specify the type of the variables:\n\nall_nominal() applies the step to all variables that are nominal (categorical)\nall_numeric() applies the step to all variables that are numeric\n\n\nTo ignore a specific column, you can specify it’s name with a negative sign as a variable (just like you would in select())\n\n# define the steps we want to apply\nmodel_recipe_steps &lt;- model_recipe %&gt;% \n  # mean impute numeric variables\n  step_impute_mean(all_numeric()) %&gt;%\n  # convert the additional ingredients variable to dummy variables\n  step_dummy(additional_ingredients) %&gt;%\n  # rescale all numeric variables except for vanilla, salt and baking powder to lie between 0 and 1\n  step_range(all_numeric(), min = 0, max = 1, -vanilla, -salt, -baking_powder) %&gt;%\n  # remove predictor variables that are almost the same for every entry\n  step_nzv(all_predictors()) \n\n\nmodel_recipe_steps\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 9\n\n\n\n\n\n── Operations \n\n\n• Mean imputation for: all_numeric()\n\n\n• Dummy variables from: additional_ingredients\n\n\n• Range scaling to [0,1] for: all_numeric(), -vanilla, -salt, -baking_powder\n\n\n• Sparse, unbalanced variable filter on: all_predictors()\n\n\nNote that the order in which you apply the steps does matter to some extent. The recommended ordering (taken from here) is\n\nImpute\nIndividual transformations for skewness and other issues\nDiscretize (if needed and if you have no other choice)\nCreate dummy variables\nCreate interactions\nNormalization steps (center, scale, range, etc)\nMultivariate transformation (e.g. PCA, spatial sign, etc)\n\n\n\nPreparing the recipe\nNext, we need to provide a dataset on which to base the pre-processing steps. This allows the same recipe to be applied to multiple datasets.\n\nprepped_recipe &lt;- prep(model_recipe_steps, training = muffin_cupcake_train)\n\n\nprepped_recipe\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 9\n\n\n\n\n\n── Training information \n\n\nTraining data contained 15 data points and 6 incomplete rows.\n\n\n\n\n\n── Operations \n\n\n• Mean imputation for: flour, milk, sugar, butter, egg, ... | Trained\n\n\n• Dummy variables from: additional_ingredients | Trained\n\n\n• Range scaling to [0,1] for: flour, milk, sugar, butter, egg, ... | Trained\n\n\n• Sparse, unbalanced variable filter removed: salt | Trained\n\n\n\n\nBake the recipe\nNext, you apply your recipe to your datasets.\nSo what did our recipe do?\n\nstep_meanimpute(all_numeric()) imputed all of the missing values with the mean value for that variable\nstep_dummy(additional_ingredients) converted the additional_ingredients into three dummy variables corresponding to three of the four levels of the original variable\nstep_range(all_numeric(), min = 0, max = 1, -vanilla, -salt, -baking_powder) converted the range of all of the numeric variables except for those specified to lie between 0 and 1\nstep_nzv(all_predictors()) removed the salt variable since it was 0 across all rows (except where it was missing)\n\n\nmuffin_cupcake_train_preprocessed &lt;- bake(prepped_recipe, muffin_cupcake_train) \nmuffin_cupcake_train_preprocessed\n\n# A tibble: 15 × 11\n    flour  milk sugar butter   egg baking_powder vanilla type   \n    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;         &lt;dbl&gt;   &lt;dbl&gt; &lt;fct&gt;  \n 1 0.381  0.667 0.957 0.426    0.1          1      0     Cupcake\n 2 0.762  0.630 0.609 0.154    0.2          1      0     Muffin \n 3 0.190  0.556 0.870 0.426    0.4          0      1     Cupcake\n 4 0.238  0.481 0.609 1        0.6          1      1     Cupcake\n 5 0.619  0.963 0.304 0.308    0            1.23   0.538 Muffin \n 6 1      1     0     0.0769   0.1          2      1     Muffin \n 7 0.381  0.778 0.565 0.308    0.4          3      0     Cupcake\n 8 0.762  0.661 0.609 0.385    0            1      0     Muffin \n 9 0.619  0.852 0.652 0        0            1      0     Muffin \n10 0      0.630 0.870 0.385    0.9          0      1     Cupcake\n11 0.238  0     1     1        1            1      1     Cupcake\n12 0.0952 0.519 0.783 0.615    0.7          2      1     Cupcake\n13 0.439  0.661 0.609 0.846    0.4          1.23   0     Muffin \n14 0.762  0.926 0.391 0        0.1          2      1     Muffin \n15 0.0952 0.593 0.913 0.462    0.5          1      0.538 Cupcake\n   additional_ingredients_icing additional_ingredients_none\n                          &lt;dbl&gt;                       &lt;dbl&gt;\n 1                            1                           0\n 2                            0                           1\n 3                            0                           0\n 4                            0                           1\n 5                            0                           1\n 6                            0                           0\n 7                            1                           0\n 8                            0                           0\n 9                            0                           1\n10                            1                           0\n11                            1                           0\n12                            0                           1\n13                            0                           0\n14                            0                           0\n15                            1                           0\n   additional_ingredients_nuts\n                         &lt;dbl&gt;\n 1                           0\n 2                           0\n 3                           0\n 4                           0\n 5                           0\n 6                           0\n 7                           0\n 8                           1\n 9                           0\n10                           0\n11                           0\n12                           0\n13                           1\n14                           0\n15                           0\n\n\n\nmuffin_cupcake_test_preprocessed &lt;- bake(prepped_recipe, muffin_cupcake_test)\nmuffin_cupcake_test_preprocessed\n\n# A tibble: 5 × 11\n  flour  milk sugar butter   egg baking_powder vanilla type   \n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;         &lt;dbl&gt;   &lt;dbl&gt; &lt;fct&gt;  \n1 1     1     0     0.0769   0.1             2   0     Muffin \n2 0.619 0.889 0.391 0        0.5             1   0.538 Muffin \n3 0.952 1     0.174 0        0.1             2   0     Muffin \n4 0.439 0.630 0.739 1        0.1             2   1     Cupcake\n5 0.190 0.556 1     0.154    0.2             1   1     Cupcake\n  additional_ingredients_icing additional_ingredients_none\n                         &lt;dbl&gt;                       &lt;dbl&gt;\n1                            0                           0\n2                            0                           0\n3                            0                           0\n4                            0                           0\n5                            0                           0\n  additional_ingredients_nuts\n                        &lt;dbl&gt;\n1                           0\n2                           0\n3                           1\n4                           0\n5                           0"
  },
  {
    "objectID": "blog/2019-08-05_base_r_to_tidyverse_pt2.html",
    "href": "blog/2019-08-05_base_r_to_tidyverse_pt2.html",
    "title": "Transitioning into the tidyverse (part 2)",
    "section": "",
    "text": "If you’re new to the tidyverse, I recommend that you first read part one of this two-part series on transitioning into the tidyverse. Part 1 focuses on what I feel are the most important aspects and packages of the tidyverse: tidy thinking, piping, dplyr and ggplot2.\nThis second part of the two-part series focuses on the remaining (less essential, but still immensely useful) packages that make up the tidyverse: tidyr, purrr, readr, tibbles, as well as some additional type-specific packages (lubridate, forcats and stringr). Additional resources include the set of tidyverse cheatsheets, as well as the R for Data Science book.\nStart by loading the tidyverse package into your environment.\nlibrary(tidyverse)\nThen load the gapminder data.\n# to download the data directly:\ngapminder_orig &lt;- read.csv(\"https://raw.githubusercontent.com/swcarpentry/r-novice-gapminder/gh-pages/_episodes_rmd/data/gapminder-FiveYearData.csv\")\n# define a copy of the original dataset that we will clean and play with \ngapminder &lt;- gapminder_orig"
  },
  {
    "objectID": "blog/2019-08-05_base_r_to_tidyverse_pt2.html#gathering-and-spreading",
    "href": "blog/2019-08-05_base_r_to_tidyverse_pt2.html#gathering-and-spreading",
    "title": "Transitioning into the tidyverse (part 2)",
    "section": "Gathering and spreading",
    "text": "Gathering and spreading\nThe main tidyr functions are spread() and gather(). If you are familiar with the older reshape2 R package, you can think of tidyr as the tidyverse version, where spread() is the equivalent of cast, and gather() is the equivalent of melt(). If not… never mind!\nThink of spread() as a function that will spread a single variable’s “values” across multiple columns based on a “key”, or grouping variable. Similarly, think of gather() as a function that will gather a variable whose “values” are spread across multiple columns (where the “key” is the grouping variable that distinguishes the columns) into a single column.\nThe main things you need to figure out when using spread() and gather() are what are the “key” and what are the “value” columns of your data frame. If you are spreading your data (to make it wider), then your key and value variables are existing variables in the data. If you are gathering your data (making it longer), then you will need to define key and value variables that will become variable names in your long-form data frame.\nBelow I’ll show how this works with a small subset of the gapminder dataset, corresponding to the life expectancy for US, Australia, and Canada for each year in the data after 1990.\nSuppose that you started with the long-form data.\n\ngapminder_sample_long &lt;- gapminder %&gt;%\n  filter(country %in% c(\"Australia\", \"United States\", \"Canada\"), year &gt; 1990) %&gt;%\n  select(country, year, lifeExp) \ngapminder_sample_long\n\n         country year lifeExp\n1      Australia 1992  77.560\n2      Australia 1997  78.830\n3      Australia 2002  80.370\n4      Australia 2007  81.235\n5         Canada 1992  77.950\n6         Canada 1997  78.610\n7         Canada 2002  79.770\n8         Canada 2007  80.653\n9  United States 1992  76.090\n10 United States 1997  76.810\n11 United States 2002  77.310\n12 United States 2007  78.242\n\n\nA wide-form version might have the life expectancy variable spread into three variables, one for each country (it would also be perfectly feasible to separate by year). So in this case, the value that you want to spread is the lifeExp variable, and the key that you want to spread/group by is the country variable.\n\ngapminder_sample_wide &lt;- gapminder_sample_long %&gt;% \n  spread(key = country, value = lifeExp)\ngapminder_sample_wide\n\n  year Australia Canada United States\n1 1992    77.560 77.950        76.090\n2 1997    78.830 78.610        76.810\n3 2002    80.370 79.770        77.310\n4 2007    81.235 80.653        78.242\n\n\nSo the columns with the country names, Australia, Canada, and United States contain the lifeExp values corresponding to those countries for each year. Note that the year variable has been retained in the wide form. If you had tried to do this without the year variable in the data frame, you would run into an error that said \"Error: Each row of output must be identified by a unique combination of keys.\" Try running the following code.\n\ngapminder_sample_long %&gt;%\n  select(-year) %&gt;%\n  spread(key = country, value = lifeExp)\n\nThis is because when the year column is missing, there is no variable that tells purrr which values should go in the same rows together. This error message is a common source of frustration in tidyr, and Hadley has been working on replacements for gather() and spread() called pivot_wider() and pivot_longer(): https://tidyr.tidyverse.org/dev/articles/pivot.html. They haven’t been incorporated into the CRAN versions of tidyr and the tidyverse yet though, but they probably will be soon. If you understand the principles of gather() and spread() then when the new pivot functions are introduced, it will be easy to learn how to use them.\nIf you wanted to go from the wide form to the long-form, you need to gather together the life expectancy values. This time, the country key and lifeExp value variable names do not currently exist in the data frame. The key and value arguments that you provide in the gather() function are what will be used as the names of the variables for the long-form version you’re about to create. Just so you can see that these variables did not need to exist in the original data, you will call the key country_var and the value lifeExp_var (previously unused names).\n\ngapminder_sample_wide\n\n  year Australia Canada United States\n1 1992    77.560 77.950        76.090\n2 1997    78.830 78.610        76.810\n3 2002    80.370 79.770        77.310\n4 2007    81.235 80.653        78.242\n\ngapminder_sample_wide %&gt;% \n  gather(key = country_var, value = lifeExp_var)\n\n     country_var lifeExp_var\n1           year    1992.000\n2           year    1997.000\n3           year    2002.000\n4           year    2007.000\n5      Australia      77.560\n6      Australia      78.830\n7      Australia      80.370\n8      Australia      81.235\n9         Canada      77.950\n10        Canada      78.610\n11        Canada      79.770\n12        Canada      80.653\n13 United States      76.090\n14 United States      76.810\n15 United States      77.310\n16 United States      78.242\n\n\nOh no…. something went wrong! The year variable has been included as a key (country). Since there is no distinction between the three country columns (Australia, Canada, and United States) and the year column, the year column was included in the gathering process. To exclude a column from the gathering process, you can explicitly remove it using e.g. -year as an argument to the gather function.\n\ngapminder_sample_wide %&gt;% \n  gather(key = country_var, value = lifeExp_var, -year)\n\n   year   country_var lifeExp_var\n1  1992     Australia      77.560\n2  1997     Australia      78.830\n3  2002     Australia      80.370\n4  2007     Australia      81.235\n5  1992        Canada      77.950\n6  1997        Canada      78.610\n7  2002        Canada      79.770\n8  2007        Canada      80.653\n9  1992 United States      76.090\n10 1997 United States      76.810\n11 2002 United States      77.310\n12 2007 United States      78.242"
  },
  {
    "objectID": "blog/2019-08-05_base_r_to_tidyverse_pt2.html#combining-and-separating-variables",
    "href": "blog/2019-08-05_base_r_to_tidyverse_pt2.html#combining-and-separating-variables",
    "title": "Transitioning into the tidyverse (part 2)",
    "section": "Combining and separating variables",
    "text": "Combining and separating variables\nThe unite() function combines columns into a single column. For instance, you can combine the country and year variables into a single variable, countryyear.\n\ngapminder_sample_united &lt;- gapminder_sample_long %&gt;%\n  unite(\"countryyear\", country, year, sep = \"_\")\ngapminder_sample_united\n\n          countryyear lifeExp\n1      Australia_1992  77.560\n2      Australia_1997  78.830\n3      Australia_2002  80.370\n4      Australia_2007  81.235\n5         Canada_1992  77.950\n6         Canada_1997  78.610\n7         Canada_2002  79.770\n8         Canada_2007  80.653\n9  United States_1992  76.090\n10 United States_1997  76.810\n11 United States_2002  77.310\n12 United States_2007  78.242\n\n\nConversely, you can separate single columns into multiple columns. Below, I undo the unite() that I performed above using separate().\n\ngapminder_sample_united %&gt;%\n  separate(countryyear, c(\"country\", \"year\"), sep = \"_\")\n\n         country year lifeExp\n1      Australia 1992  77.560\n2      Australia 1997  78.830\n3      Australia 2002  80.370\n4      Australia 2007  81.235\n5         Canada 1992  77.950\n6         Canada 1997  78.610\n7         Canada 2002  79.770\n8         Canada 2007  80.653\n9  United States 1992  76.090\n10 United States 1997  76.810\n11 United States 2002  77.310\n12 United States 2007  78.242\n\n\nTidyr also has some nice functions for dealing with missing values including\n\ndrop_na() that will remove every row that has a missing value (NA) in it.\nreplace_na() that will replace every missing value (NA) with whatever value you specify."
  },
  {
    "objectID": "blog/2019-08-05_base_r_to_tidyverse_pt2.html#handling-dates-and-times-lubridate",
    "href": "blog/2019-08-05_base_r_to_tidyverse_pt2.html#handling-dates-and-times-lubridate",
    "title": "Transitioning into the tidyverse (part 2)",
    "section": "Handling dates and times: lubridate",
    "text": "Handling dates and times: lubridate\n\n\n\n\n\n\n\n\n\nLubridate makes it really straightforward to deal with dates. One might say… it lubricates them… one might also not say that, because it’s a bit weird.\nLubridate offers a simple way of converting dates/times stored as strings to dates/times stored as dates/times, and it makes it easy to do math with dates.\nThe primary set of functions are date-time-reading functions that convert strings to dates. To decide which function to use, you will need to figure out what format your dates are in (by… looking at them…). For instance, if your date is coded as \"August 2nd 2019\" or \"08-05-19\" or \"08/02/19\", then you would use the mdy() function because it is coded as “month-day-year”:\n\nlibrary(lubridate)\nmdy(\"August 2nd 2019\")\n\n[1] \"2019-08-02\"\n\nmdy(\"8-2-2019\")\n\n[1] \"2019-08-02\"\n\nmdy(\"8/2/19\")\n\n[1] \"2019-08-02\"\n\n\nIf your dates were coded as “year-month-day” then you would use the ymd() function, and so on.\nStrings that contain times can be parsed using hms() for “hour-minute-second”.\n\nhms(\"8:45:12\")\n\n[1] \"8H 45M 12S\"\n\n\nAnd date-times can be parsed using ymd_hms(), ymd_hm(), ymd_h(), as well as for the other date versions (mdy_hms(), dmy_hms(), etc…).\n\nmdy_hms(\"March 13th 2019 at 9:02:00\")\n\n[1] \"2019-03-13 09:02:00 UTC\"\n\nmdy_hm(\"03-13-19, 9:02\")\n\n[1] \"2019-03-13 09:02:00 UTC\"\n\n\nYou can add fixed periods of time to dates easily using the years(), months(), days(), hours(), etc… functions. For instance:\n\nmdy(\"August 2nd 2019\") + days(42)\n\n[1] \"2019-09-13\"\n\n\nOnce your dates are in an actual date format, you can do intuitive mathematical calculations with date-times:\n\nmdy_hms(\"August 2nd 2019, 1:21:30 pm\") - mdy_hms(\"August 1st 2019, 11:23:33 am\")\n\nTime difference of 1.08191 days\n\n\nPlus ggplot2 handles lubridate-dates really well."
  },
  {
    "objectID": "blog/2019-08-05_base_r_to_tidyverse_pt2.html#handling-factors-forcats",
    "href": "blog/2019-08-05_base_r_to_tidyverse_pt2.html#handling-factors-forcats",
    "title": "Transitioning into the tidyverse (part 2)",
    "section": "Handling factors: forcats",
    "text": "Handling factors: forcats\n\n\n\n\n\n\n\n\n\nFactors are somehow simultaneously very useful and the worst thing ever. Fortunately, since I discovered the forcats package, my factors have been on their best behaviour.\nThe forcats package has a few really useful functions. The ones I use most often are\n\nfct_inorder() for reordering the levels of a factor so that the levels are in the order that they appear in the factor vector.\nfct_infreq() for reordering the levels of a factor so that the levels are in order of most to least frequent.\nfct_rev() for reversing the order of the levels of a factor.\nfct_relevel() for manually reordering the levels of the factor.\nfct_reorder() for reordering the levels based on their relationship to another variable.\n\nThere are other functions too, but I rarely use them. Check out the forcats cheatsheet!\nAn example of how the forcats package makes my life easier is when I want to reorder the factor levels. Factor levels are usually alphabetical by default, and I often want the factor levels to be in a specific order.\nAs an exercise both in ggplot2 and dplyr, I want to make a plot that shows the difference between life expectancy between 2007 and 1952 and arrange the countries in order of greatest difference in life expectancy.\n\ngapminder_life_exp_diff &lt;- gapminder %&gt;%\n  # filter to the starting and ending years only\n  filter(year == 1952 | year == 2007) %&gt;%\n  # ensure that the data are arranged so that 1952 is first and 2007 is second \n  # within each year\n  arrange(country, year) %&gt;%\n  # for country, add a variable corresponding to the difference between life \n  # expectency in 2007 and 1952\n  group_by(country) %&gt;%\n  mutate(lifeExp_diff = lifeExp[2] - lifeExp[1],\n         # also calculate the largest population for the country (based on the two years)\n         max_pop = max(pop)) %&gt;%\n  ungroup() %&gt;%\n  # arrange in order of the biggest difference in life expectency\n  arrange(lifeExp_diff) %&gt;%\n  # restrict to countries with a population of at least 30,000 so we can fit \n  # the plot in a reasonable space\n  filter(max_pop &gt; 30000000) %&gt;%\n  select(country, year, continent, lifeExp, lifeExp_diff)\ngapminder_life_exp_diff  \n\n# A tibble: 72 × 5\n   country          year continent lifeExp lifeExp_diff\n   &lt;chr&gt;           &lt;int&gt; &lt;chr&gt;       &lt;dbl&gt;        &lt;dbl&gt;\n 1 South Africa     1952 Africa       45.0         4.33\n 2 South Africa     2007 Africa       49.3         4.33\n 3 Congo Dem. Rep.  1952 Africa       39.1         7.32\n 4 Congo Dem. Rep.  2007 Africa       46.5         7.32\n 5 United States    1952 Americas     68.4         9.80\n 6 United States    2007 Americas     78.2         9.80\n 7 United Kingdom   1952 Europe       69.2        10.2 \n 8 United Kingdom   2007 Europe       79.4        10.2 \n 9 Nigeria          1952 Africa       36.3        10.5 \n10 Nigeria          2007 Africa       46.9        10.5 \n# … with 62 more rows\n\n\nTo understand what the intermediate dplyr steps are doing in the code below, I suggest printing each step out to the console (without defining a new data frame) - i.e. first print gapminder %&gt;% filter(year == 1952 | year == 2007), then print gapminder %&gt;% filter(year == 1952 | year == 2007) %&gt;% arrange(country, year), etc.\nThe next task is to make a dot plot that shows the life expectancy in 1952 and 2007 for each country. Since the countries in our data frame is arranged in order of smallest to biggest difference in life expectancy, one would expect that the plot will be too. However, the countries in the plot still appear in alphabetical order! The problem is that ggplot2 plots factors in order of their levels, but the arrange() dplyr function rearranges the order of the rows in the data frame but does not change the order of the factor levels.\n\ngapminder_life_exp_diff %&gt;%\n  ggplot() +\n  geom_point(aes(x = lifeExp, y = country, col = as.factor(year)))\n\n\n\n\n\n\n\n\nIf I tried to fix this using base R, I would undoubtedly end up messing up which country is which. Fortunately this is really, really easy to fix using forcats! The fct_inorder() function will automatically reorder the levels of a factor in the order in which they appear in the vector. So all I need to do is add one line of pre-processing code before I make my plot: mutate(country = fct_inorder(country)).\n\ngapminder_life_exp_diff %&gt;%\n  mutate(country = fct_inorder(country)) %&gt;%\n  ggplot() +\n  geom_point(aes(x = lifeExp, y = country, col = as.factor(year)))\n\n\n\n\n\n\n\n\nI’m a bit pedantic about data viz, so I can’t leave this plot looking like this, so I’m just going to place some ggplot2 code here for making this plot waaaaay more badass. Try to read through the code and understand what its doing. This isn’t a lesson in forcats, it’s a lesson in EDA!\n\ngapminder_life_exp_diff %&gt;%\n  mutate(country = fct_inorder(country)) %&gt;%\n  # for each country define a varaible for min and max life expectancy\n  group_by(country) %&gt;%\n  mutate(max_lifeExp = max(lifeExp),\n         min_lifeExp = min(lifeExp)) %&gt;% \n  ungroup() %&gt;%\n  ggplot() +\n  # plot a horizontal line from min to max life expectency for each country\n  geom_segment(aes(x = min_lifeExp, xend = max_lifeExp, \n                   y = country, yend = country,\n                   col = continent), alpha = 0.5, size = 7) +\n  # add a point for each life expectancy data point\n  geom_point(aes(x = lifeExp, y = country, col = continent), size = 8) +\n  # add text of the country name as well as the max and min life expectency \n  geom_text(aes(x = min_lifeExp + 0.7, y = country, \n                label = paste(country, round(min_lifeExp))), \n            col = \"grey50\", hjust = \"right\") +\n  geom_text(aes(x = max_lifeExp - 0.7, y = country, \n                label = round(max_lifeExp)), \n            col = \"grey50\", hjust = \"left\") +\n  # ensure that the left-most text is not cut off \n  scale_x_continuous(limits = c(20, 85)) +\n  # choose a different colour palette\n  scale_colour_brewer(palette = \"Pastel2\") +\n  # set the title\n  labs(title = \"Change in life expectancy\",\n       subtitle = \"Between 1952 and 2007\",\n       x = \"Life expectancy (in 1952 and 2007)\",\n       y = NULL, \n       col = \"Continent\") +\n  # remove the grey background\n  theme_classic() +\n  # remove the axes and move the legend to the top\n  theme(legend.position = \"top\", \n        axis.line = element_blank(),\n        axis.ticks = element_blank(),\n        axis.text = element_blank())\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead."
  },
  {
    "objectID": "blog/2019-08-05_base_r_to_tidyverse_pt2.html#handling-strings-stringr",
    "href": "blog/2019-08-05_base_r_to_tidyverse_pt2.html#handling-strings-stringr",
    "title": "Transitioning into the tidyverse (part 2)",
    "section": "Handling strings: stringr",
    "text": "Handling strings: stringr\n\n\n\n\n\n\n\n\n\nR used to be terrible at handling strings. Stringr has made string-handling a LOT easier. The functions all start with str_ and end with what you want to do to the string.\nFor instance, to return a logical that specifies whether a specific pattern exists in the string (the equivalent of grepl() in base R), you can use the str_detect() function\n\nstr_detect(\"I like bananas\", \"banana\")\n\n[1] TRUE\n\n\nMy friend Sara Stoudt @sastoudt wrote a wrote a very useful post for the tidyverse website comparing stringr with its Base R equivalents (https://stringr.tidyverse.org/articles/from-base.html). She provides the following useful table (hers is a bit longer - I’m just showing the parts I find most useful):\n\n\n\n\n\n\n\n\nAction\nBase R\nTidyverse\n\n\n\n\nIdentify the location of a pattern\ngregexpr(pattern, x)\nstr_locate_all(x, pattern)\n\n\nKeep strings matching a pattern\ngrep(pattern, x, value = TRUE)\nstr_subset(x, pattern)\n\n\nIdentify position matching a pattern\ngrep(pattern, x)\nstr_which(x, pattern)\n\n\nDetect presence or absence of a pattern\ngrepl(pattern, x)\nstr_detect(x, pattern)\n\n\nReplace a pattern\ngsub(pattern, replacement, x)\nstr_replace_all(x, pattern, replacement)\n\n\nCalculate the number of characters in a string\nnchar(x)\nstr_length(x)\n\n\nSplit a string into pieces\nstrsplit(x, pattern)\nstr_split(x, pattern)\n\n\nExtract a subset of a string\nsubstr(x, start, end)\nstr_sub(x, start, end)\n\n\nConvert a string to lowercase\ntolower(x)\nstr_to_lower(x)\n\n\nConvert a string to “Title Case”\ntools::toTitleCase(x)\nstr_to_title(x)\n\n\nConvert a string to uppercase\ntoupper(x)\nstr_to_upper(x)\n\n\nTrim white space from a string\ntrimws(x)\nstr_trim(x)\n\n\n\nIf you’d like to see a little more of stringr, check out Sara’s post!"
  },
  {
    "objectID": "blog/2017-02-02-how-to-present-good.html",
    "href": "blog/2017-02-02-how-to-present-good.html",
    "title": "How to Present Good",
    "section": "",
    "text": "This week I was asked to give a presentation on giving presentations.\nSo meta.\nAnyway, my lack of desire to spend time writing something that may have already been written by someone else led me to find a number of existing slides written by others with the same goal: to present on presenting. The only problem was that although these slides all contained excellent tips on making and subsequently presenting talks, the slides themselves were perfect examples of what not to do when making slides; they were insurmountable walls of text.\nI decided that I would have to make my own slides after all. The final product can be found here and is embedded below:\nRead on for a more in-depth glimpse into my presentation pro-tips"
  },
  {
    "objectID": "blog/2017-02-02-how-to-present-good.html#dedicate-a-lot-of-time",
    "href": "blog/2017-02-02-how-to-present-good.html#dedicate-a-lot-of-time",
    "title": "How to Present Good",
    "section": "Dedicate a lot of time",
    "text": "Dedicate a lot of time\nMaking good slides takes time. You not only have to think about what you want to say, but also how to say it, as well as how to arrange your slide so that it is not overwhelming yet still gets your point across.\nEven making these fairly simple slides on presenting took me an hour and a half!\nThere is a quote by someone or other that says\n\nFor every minute you speak an hour should be spent in preparation. So for a 60 minute seminar, one should work for 60 hours.\n\nWhile this is clearly excessive, the point is clear: making a good presentation will take time."
  },
  {
    "objectID": "blog/2017-02-02-how-to-present-good.html#consider-your-audience",
    "href": "blog/2017-02-02-how-to-present-good.html#consider-your-audience",
    "title": "How to Present Good",
    "section": "Consider your audience",
    "text": "Consider your audience\nGiving the same presentation to vastly different groups of people is a definite no-no. For example, when I’m presenting to my biological collaborators, I need to simplify the technical details and can focus less on the biology background than when I’m presenting to my more data-literate peers. On the other hand, if I’m presenting to a lay-audience who is unfamiliar with both the technical details of the methods I’m using as well as the biology, I will remove all mathematical notation and stick to explanatory pictures that describe what I’m doing.\nOn that note, statisticians tend to fill their slides with far too much math even for a technical audience. Even the most accomplished statisticians cannot absorb all that notation at once! Whenever possible, try to use annotated graphics in place of equations.\nOne tip to try to tailor your talk to your audience is to ask the following questions:\nHow much background does my audience have? If they have a strong math background, I can assume that they know the basics and don’t need to explain them in excruciating detail, but if they don’t know any biology, I need to give a very simple explanation in lay-terms of the how the process works.\nWhat is it that I want my audience to take away from my presentation? If I want them to be able to walk away and implement my method, I will give many more details, but if, as is more likely the case, I simply want them to have a broad understanding of why my project is important and how I’ve solved the problem at hand, then I can keep most of the details to myself and present a very high-level picture."
  },
  {
    "objectID": "blog/2017-02-02-how-to-present-good.html#tell-a-story",
    "href": "blog/2017-02-02-how-to-present-good.html#tell-a-story",
    "title": "How to Present Good",
    "section": "Tell a story",
    "text": "Tell a story\nEveryone enjoys a good story (but few people truly enjoy listening to a long, dry proof). Your talk should follow a structure as follows:\nThe setup: what is the context?\nThe issue: what was the problem that prompted your project?\nThe resolution: what steps did you take to solve the problem? What’s are the next steps?\nFor example, I am currently working on a project involving kidney transplants in HIV patients. My story is as follows:\nThe setup: due to prolonged survival of HIV patients, additional complications including HIV-associated nephrology (kidney disease) are becomming more common resulting in an increased demand for kidney transplants among this population.\nThe issue: HIV-positive kidney transplant patients suffer from acute rejection of their transplanted kidney at a significantly higher rate than their HIV-negative counterparts.\nThe resolution: we are developing models for predicting acute rejection in the hopes of elucidating the cause of this increased rejection rate.\n\nTransition slides\nTransition slides are a great way to separate one part of your story from another. Their presence serves to indicate to the audience that you’re moving onto something new. They are particularly effective if your main content slides and your transition slides have different color templates. For example in the presentation above, the content slides have a white background and black text (with a blue heading), but the transition slides have a blue background with white text.\nThe stark difference between content and transition is an incredibly effective way to teach the audience that a blue slide with white text means that we’re moving onto something new."
  },
  {
    "objectID": "blog/2017-02-02-how-to-present-good.html#outlines",
    "href": "blog/2017-02-02-how-to-present-good.html#outlines",
    "title": "How to Present Good",
    "section": "Outlines",
    "text": "Outlines\nPeople really like outlines (that slide at the beginning of your talk that describes its contents). I happen to think outlines are great, it’s just that most people are terrible at making them.\nAn example of a terrible outline is as follows:\n\nMotivation\nLiterature review\nContributions\nResults\nConclusion\n\nWell that’s fairly useless. Of course your talk is going to begin with motivation, followed by a review of the literature, at which point you will move onto your contributions, show me some results and then conclude. This is the basic structure of every academic talk, ever.\nA much better outline slide tells the audience what you are going to talk about, not the skeletal structure of a general presentation.\nFor example, an outline for my kidney transplant project might look more like this:\n\nWhy HIV causes kidney disease\nOutline of the UCSF Study\nCohort selection (and why we struggled with it)\nDynamic approaches to predicting rejection\nAnd why they don’t work well (yet…)"
  },
  {
    "objectID": "blog/2017-02-02-how-to-present-good.html#slide-contents",
    "href": "blog/2017-02-02-how-to-present-good.html#slide-contents",
    "title": "How to Present Good",
    "section": "Slide contents",
    "text": "Slide contents\n\nNo walls of text please!\nThe golden rule of making presentations should be with text, less is always more. There is a nice TED blog that has a nice paragraph discussing this point that I’m going to shamelessly drop here:\n\nOne thing to avoid—slides with a lot of text, especially if it’s a repeat of what you’re saying out loud. It’s like if you give a paper handout in a meeting—everyone’s head goes down and they read, rather than staying heads-up and listening. If there are a lot of words on your slide, you’re asking your audience to split their attention between what they’re reading and what they’re hearing. That’s really hard for a brain to do, and it compromises the effectiveness of both your slide text and your spoken words. If you can’t avoid having text-y slides, try to progressively reveal text (like unveiling bullet points one by one) as you need it.\n\nI particularly like the point about how having a lot of text on your slides forces your audience to split their attention between reading and listening without achieving either goal successfully, because it’s so true!.\nIt’s important to realize that this holds true even if all of your text lives in bullet point format.\n\n\nFont size\nOne of the worst (but incredibly common) mistakes that people make when preparing slides is tiny font size. Anything smaller than 18pt is way too small! This is equally (possible, even more) true for figures. Pro-tip: make the axis labels, legend and annotations obnoxiously large.\n\n\nImages\nImages are a great way to lighten up your presentation. Try to find simple photos relevant to the point being made on the slide.\nFor example, I recently made a presentation that included a section on “Weak learners”, which are basically a number of classification models that each individually perform poorly (at least a little better than random guessing), but when pooled together made a powerful learning algorithm.\nI included an image alongside my description displaying an army of soldiers. Get it?"
  },
  {
    "objectID": "blog/2017-02-02-how-to-present-good.html#how-to-write-the-presentation",
    "href": "blog/2017-02-02-how-to-present-good.html#how-to-write-the-presentation",
    "title": "How to Present Good",
    "section": "How to write the presentation",
    "text": "How to write the presentation\nWhile I usually encourage people to start their slide preparation with a structural skeleton by\n\nStart with your transition slides that indicate the distinct sections of your talk,\nThen add blank content slides with headers,\nFinally fill in the content slides with content.\n\nIn practice, this isn’t quite what I do. While I will often start this way, as I start filling in the content, I will subsequently do a lot of moving things around, merging sections, and adding new ones.\n\nRepetition for emphasis\nIf there is something that is really crucial to your story say it many, many times. Often in bold font and in a different color. Say it in different ways, and keep coming back to it verbally: “remember that here we don’t have a control group, well blah blah blah”. Sometimes it’s okay to even tell your audience that you’re emphasizing something by saying something like “just in case it’s not obvious, I’m really trying to emphasize this point”."
  },
  {
    "objectID": "blog/2017-02-02-how-to-present-good.html#presenting-your-results",
    "href": "blog/2017-02-02-how-to-present-good.html#presenting-your-results",
    "title": "How to Present Good",
    "section": "Presenting your results",
    "text": "Presenting your results\nConsider a slide containing the figure below.\n\n\n\n\n\n\n\n\n\nHow does this figure make you feel? Does it make you feel happy because of the pretty colors? Perhaps you’re just happy to see that the author used ggplot rather than base R plot…\nTo be honest, I think this plot is a mess; the colors are distracting, the axis labels are useless, the text is way too small and the heading “Results” is entirely uninformative.\nFortunately, there are many ways that we can make it better: we can\n\nuniversally increase the font size,\nhighlight our method in bold and blue, while fading the competing methods into the background by shading them in grey,\ngive the figure informative axis names,\nmake an informative slide header with the take-away message that “our method has lower prediction error than the competitors”.\n\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\nFor more tips like this, I encourage you to check out Cole Nussbaumer Knaflic’s book, Storytelling with Data, which I really enjoyed. It is a great read if you want to learn how to present your figures to maximize impact and to use your data to tell a story.\nAnother important step to presenting results is to treat the audience like idiots. Trust me, they’ll appreciate it (mostly becauase they won’t realize that you’re doing it). Explain in excruciating detail what every single element of your figure means. For example, in presenting the plot above, rather than simply saying “this plot shows that our method clearly outperforms our competitors”, I would say:\n\n“In the figure on this slide, we are plotting the prediction error rate over time. That means that the lower the error, the better. The x-axis depicts time, while the y-axis depicts the error rate at that time for each method reporesented by the three lines. Our method is highlighted in blue and we see that the error rate hovers around 10%, but that this is better than our competitors whose error rates tend to lie between 12 and 16% or so.\n\nJust because your figure is obvious to you after having put a lot of thought into making the figure in the first place, that doesn’t mean that it is obvious to your audience who have viewed the figure for a total of 2 seconds.\n\nPresenting many plots on the same slide\nI have seen far to many presentations where the presenter moves to the next slide and, BAM! - 10 plots that look almost identical are shoved in my face all at the same time. My brain subsequently has a meltdown and stops listening to a word the presenter is saying while I try to figure out what this army of plots is showing me.\nHere is a simple example that I found on the interwebs:\n\n\n\nIf you have to show a plot with many, many panels (perhaps from simulations under different paramter settings), then a much more effective way to present these results is to reveal them one-by-one, and explain what each plot shows as it is revealed, and how it compares to the previous plots."
  },
  {
    "objectID": "blog/2017-02-02-how-to-present-good.html#practice-makes-perfect",
    "href": "blog/2017-02-02-how-to-present-good.html#practice-makes-perfect",
    "title": "How to Present Good",
    "section": "Practice makes perfect",
    "text": "Practice makes perfect\nAfter the slides are made it’s time to practice, practice, practice! Practice by yourself to ensure that your timing is correct and to reduce the number of “um”s and “er”s as you present. Then practice in front of other people, preferably in front of an actual screen so that you can figure out where to stand (or sit), what to do with your hands (they always seem to be in the way), and how to point at things (with a laser pointer? using your mouse? with a big stick?).\nIt is always a good idea to practice speaking slowly and clearly and to pause at each slide transition. Enthusiasm is encouraged - no one wants to listen to someone talk for an hour when they dont even sound excited about what they’re presenting. Varying the tone of your voice is very helpful with exuding an aura of enthusiasm.\nEye contact is also vital. Why should your audience pay attention to you if you aren’t paying attention to them? If you are nervous, one idea is to pick a few spots in the back of the room, above people’s heads and look at those.\nLastly, my greatest tip of all is to use humour, but only if you feel comfortable doing so (I actually find that using humour helps me relax). But it’s never a good idea to force it."
  },
  {
    "objectID": "blog/2017-03-7-docathon.html",
    "href": "blog/2017-03-7-docathon.html",
    "title": "Docathon: A Week of Doumentation",
    "section": "",
    "text": "This week is the Docathon at BIDS (a.k.a. that wonderful place that I spend all my time).\nA docathon is like a hackathon but is focused on developing material and tools for documentation. We have loads of projects signed up to receive some documentation-love and an impressive number of excited participants!\n\n\n\n\n\n\n\n\n\nWe kicked off the event with a series of tutorials for writing “good” documentation. I gave an R-specific tutorial where I discussed using devtools to both develop and document your package. I ended with a sneak peak into bookdown book-based documentation for complex packages. Skip ahead to 1:54:02 to hear my presentation!\n\n\n\n\nThe materials for my tutorial can be found in this github repo. Stay tuned for a more detailed blog post about writing documentation in R."
  },
  {
    "objectID": "blog/2017-02-02-superheat-example.html",
    "href": "blog/2017-02-02-superheat-example.html",
    "title": "Superheat: a simple example",
    "section": "",
    "text": "Making beautiful and customizable heatmaps just got way easier… Introducing the superheat R package!\nUsing superheat, it is now extremely easy to produce plots like the example below describing 10 randomly selected cars from the famous mtcars dataset.\n\nlibrary(superheat)\nset.seed(1347983)\nselected.rows &lt;- sample(1:nrow(mtcars), 10)\nX.col &lt;- matrix(\"black\", ncol = ncol(mtcars), nrow = 10)\nX.col[scale(mtcars[selected.rows, ]) &lt; 0] &lt;- \"white\"\nsuperheat(mtcars[selected.rows,], \n          # add text\n          X.text = round(as.matrix(mtcars[selected.rows, ])),\n          X.text.col = X.col,\n          # scale columns\n          scale = T, \n          # label aesthetics\n          left.label.size = 0.5,\n          bottom.label.size = 0.15,\n          bottom.label.text.angle = 90,\n          bottom.label.text.alignment = \"right\",\n          bottom.label.col = \"white\",\n          # dendrogram\n          row.dendrogram = T,\n          # top plot\n          yt = sapply(mtcars, function(x) cor(x, mtcars$hp)),\n          yt.plot.type = \"bar\",\n          yt.axis.name = \"correlation\\nwith\\nhorsepower\",\n          # column order\n          order.cols = order(sapply(mtcars, function(x) cor(x, mtcars$hp))),\n          # grid lines\n          grid.vline.col = \"white\",\n          grid.hline.col = \"white\")\n\n\n\n\n\n\n\n\nTo see more details on what you can do with superheat, see the vignette, as well as our paper outlining case studies using Superheat currently available on arXiv."
  },
  {
    "objectID": "blog/2017-06-29-website.html",
    "href": "blog/2017-06-29-website.html",
    "title": "Migrating from GitHub Pages to Netlify: how and why?",
    "section": "",
    "text": "After finding myself increasingly frustrated with GitHub Pages inability to cooperate with my website engine of choice, Hugo, I’ve decided to make a move. Here are the reasons why:\n\nHaving all of my project pages being subpages of my personal page felt weird.\nDealing with git subtrees, merge conflicts on the master branch, and having to do all kinds of work-arounds to get Hugo to play nice with GitHub Pages was driving me crazy.\nI found a much easier way!"
  },
  {
    "objectID": "blog/2017-06-29-website.html#hello-netlify",
    "href": "blog/2017-06-29-website.html#hello-netlify",
    "title": "Migrating from GitHub Pages to Netlify: how and why?",
    "section": "Hello Netlify!",
    "text": "Hello Netlify!\nI’ve moved to Netlify. It’s amazing, almost too easy. There must be a catch; nothing in life is ever this easy…\nThe new workflow to get the website set up is pretty basic: simply make a GitHub repository containing your website, tell Netlify what the repository is called and push. Bam! Your website is now ready!\nIf you’re looking for some more explicit details, here they are:\n\nStart a new github repository for your website and call it anything you like. By that I mean that it doesn’t have to be username.github.io. I called mine personal-webpage. So creative, I know.\nUse the R package blogdown to design your page with Hugo. There are lots of cool themes available. Choose a good one.\nPush your website, content and all, to your new repo.\nHead on over to netlify, create and account/log in and hit “New site from Git”.\n\n\nStep 1: Set the Continuous Deployment Git provider to GitHub (or whichever provider you use).\nStep 2: Choose the repository containing your website.\nStep 3: Set the Build command to hugo_0.19 (or whichever version you want), and the Publish directory to “public” (this is the folder in which Hugo by default puts the actual webpage files when it’s built).\n\n\nHit “Deploy site”.\nIf you like you can choose to “Change site name”.\nYour site can now be found at sitename.netlify.com!\nEverytime you push new content to the GitHub repo, Netlify will automatically rebuild and deploy your site. You can just sit back and relax :).\n\nIt’s probably a good idea to buy a domain name from somewhere like Google Domains for like $10/year and point that domain name towards your netlify site, but otherwise you’re good to go!"
  },
  {
    "objectID": "blog/2017-06-27-colors.html",
    "href": "blog/2017-06-27-colors.html",
    "title": "Coolors: choosing color schemes",
    "section": "",
    "text": "Choosing a color palette for a visualization can be one of the most time consuming parts for perfectionists like me. It can be surprisingly difficult to decide on a palette that is both visually appealing and practical, but fortunately there do exist websites to help!\nFor example, Coolors shoots random, appealing, color palettes at you and you can swipe from one to the next with a hit of a space-bar."
  },
  {
    "objectID": "blog/2017-07-05-confounding.html",
    "href": "blog/2017-07-05-confounding.html",
    "title": "Confounding in causal inference: what is it, and what to do about it?",
    "section": "",
    "text": "Often in science we want to be able to quantify the effect of an action on some outcome. For example, perhaps we are interested in estimating the effect of a drug on blood pressure. While it is easy to show whether or not taking the drug is associated with an increase in blood pressure, it is surprisingly difficult to show that taking the drug actually caused an increase (or decrease) in blood pressure.\nCausal inference is the field of statistics (or economics, depending on who you ask) that is concerned with estimating the causal effect of a treatment.[^check]"
  },
  {
    "objectID": "blog/2017-07-05-confounding.html#confounders-are-the-worst",
    "href": "blog/2017-07-05-confounding.html#confounders-are-the-worst",
    "title": "Confounding in causal inference: what is it, and what to do about it?",
    "section": "Confounders are the worst!",
    "text": "Confounders are the worst!\nSuppose that we have conducted a randomized experiment in which we measure some outcome (say mortality) within two groups, treated and untreated, into which individuals were randomly assigned. In this case, the random assignment of treatment means that there should, on average, be no significant differences between the treated and untreated groups. Thus, any difference in mortality that we observe between the treated group and the untreated group must be due to the treatment itself (as this is the only thing that differs between the treated and untreated groups).\nOne of the primary problems that arise when attempting to estimate average causal effects in an observational study (i.e. a study in which the individuals assign themselves into a treatment group, e.g. because they themselves choose to smoke or not, rather than a scientist choosing for them) is that there may exist differences between the treated group and the untreated group, other than the treatment itself, such as gender (males are more likely to smoke).\nWhy is this a problem? Basically, if there is something other than the treatment that differs between the treated and untreated groups, then we cannot conclusively say that any difference observed in mortality (or any other outcome of interest) between the two groups is due solely to the treatment. Such a difference could also plausibly be due to these other variables that differ between these groups. These variables that differ between the treatment and control groups are called confounders if they also influence the outcome.\nFor a simplified example, it is known that males are more likely to smoke than females, but males are also more likely to die young as a result of other general risk-raking behavior. In this case, if we notice a difference in mortality between smokers and non-smokers, we cannot be sure that this difference is due to the smoking rather than the fact that the group of smokers consists more of males than the control group and subsequently (as a result of the risk-taking behavior, rather than just the smoking) the treatment group has a higher mortality rate than the untreated group. In this case, the higher mortality rate among smokers has nothing to do with (or at least is not entirely due to) the smoking itself, but rather is to do gender discrepancies and the differences in risk-taking behaviors afforded by such a discrepancy.\nIn the graph below, our example corresponds to the outcome, \\(Y\\), mortality; the treatment, \\(T\\), smoking, and the confounder, \\(X\\), gender.\n\n\n\n\n\n\n\n\n\nNote that if gender differed between the treatment and untreated group (e.g. smokers vs non-smokers), but had no association with the outcome (e.g. mortality), then gender would not be considered a confounder (as it would not be a common cause of treatment the and outcome).\nClearly estimating causal effects in the presence of confounders is going to be a problem!"
  },
  {
    "objectID": "blog/2017-07-05-confounding.html#what-about-unmeasured-confounders",
    "href": "blog/2017-07-05-confounding.html#what-about-unmeasured-confounders",
    "title": "Confounding in causal inference: what is it, and what to do about it?",
    "section": "What about unmeasured confounders?",
    "text": "What about unmeasured confounders?\nIf there exist unmeasured confounders that may be a common cause of both the outcome and the treatment, then it is impossible to accurately estimate the causal effect. Period. Moreover, it is impossible to actually check whether there exists unobserved confounding. Everything relies on the validity of your assumptions. Sorry about that."
  },
  {
    "objectID": "blog/2018-05-23-instrumental_variables.html",
    "href": "blog/2018-05-23-instrumental_variables.html",
    "title": "Understanding Instrumental Variables",
    "section": "",
    "text": "Suppose, as many do, that we want to estimate the effect of an action (or treatment) on an outcome. As an example, we might be interested in estimating the effect of receiving a drug vs not receiving a drug on the incidence of heart disease. In an ideal futuristic world, we would take each individual in our population and split them into two identical humans: one who receives the treatment and the other who doesn’t. Since these two humans are 100% identical in every way other than the treatment received, any difference we subsequently observe in their outcome (i.e. development of heart disease) must be due to the effect of the treatment alone.\nUnfortunately, since our humble 21st-century technology does not yet posses a human-duplication machine, the natural thing to do is to split our existing population of humans randomly into two groups: those who are to receive the drug (the treatment group) and those who won’t (the control group).\nSince there was nothing special about how we determined who gets the drug and who doesn’t, on average, there should be no difference between the treatment group and control group other than in the treatment itself. Thus, comparing the incidence of heart disease in the treatment group and in the control group should give us a reasonable estimate of the average effect of the treatment in the population.\n\nUnobserved confounding in observational studies\nWhile the randomized experiment is a perfectly reasonable thing to do in many situations, unfortunately there are many more scenarios where we can’t just randomly assign the treatment. For instance in the case of the effect of organ transplants on survival, it is probably not the best idea to randomly assign transplants to people. Instead, those who receive transplants first are those who need it the most. Such a study is necessarily observational: we simply observe who happened to receive a transplant based on the existing system.\nIt is highly likely that there will be fairly extreme differences between those who receive a transplant earlier and those who receive one later; namely that those who are transplanted earlier are much sicker than those who are transplanted later. Thus, differences in survival between those who receive a transplant within a month (our definition of the treatment group) and those who receive a transplant in more than one month (our definition of the control group) cannot be attributable solely to the time of the transplant itself, but also to the fact that the earlier transplantees were sicker at the start of the study. Thus sickness is a confounder: something that is different between the treatment and control groups other than the treatment itself that also affects the outcome.\nIf you’d like to quickly brush up on your causal inference, the fundamental issue associated with making causal inferences, and in particular, the troubles that arise in the presence of confounding, you might like to take a peak at my earlier post on this topic.\nIf a confounder is observable, you can adapt your estimator using methods such as matching or regression adjustment to obtain an unbiased treatment effect estimate. The key issue that I will explore in this post is what to do when you have unobserved confounding. For example, what if we cannot quantitatively measure “sickness” (the variable that determines whether you will be transplanted soon or not) and so transplants between blood-type compatible donors and recipients are assigned primarily based on sickness as measured by a doctor’s intuition (a fundamentally unmeasurable quantity). Note that this isn’t actually how transplants are allocated, but it will serve nicely for instrumental variables explanation purposes.\n\n\nWhat is an instrument and can I play it?\nIn most situations, if you have an unobservable confounder, there isn’t too much you can do to get around it since changes in the treatment will necessarily also change the confounder (in ways we cannot measure), both of which will in turn change the outcome. In this case, there is no way to measure the effect of the treatment alone (i.e. in isolation from the confounder) on the outcome.\nIn a few situations, however, you will have an instrument. An instrument, it turns out, is not a tuba, a piano, nor a flute, but rather an instrument is a magical measurable quantity that happens to be correlated with the treatment, and is only related to the outcome via the treatment (the instrument has no influence on the outcome except via the fact that it influences the treatment which inturn influences the outcome). The second part of this definition (that the instrument is only related to the outcome via the treatment) is called the exclusion restriction.\nIn our transplant setting, a particularly nice instrument is blood type. Patients with blood type AB can receive organs from donors with any blood type (and donors with blood type AB can only donate to recipients with blood type AB), whereas patients with blood type O can only receive organs from donors with blood type O (but donors with blood type O can donate to recipients of any blood type). The result is that patients with blood type O will, in general, need to wait longer for a transplant than patients of a similar sickness level with blood type AB.\nSince this is a lot to explain in words, below you will find a picture that demonstrates how recipients with blood type AB have a larger pool of potential donors than do recipients with blood type O. An arrow from a donor blood type to a recipient blood type implies that donors with the specified blood type can donate organs to the corresponding recipient blood type. Since we will focus only on blood types AB and O (pretending that blood types A and B don’t exist for technical convenience), these two blood types have been enclosed in a snug little box within the diagram.\n\n\n\n\n\n\n\n\n\nThe idea behind instrumental variables is that the changes in treatment that are caused by the instrument are unconfounded (since changes in the instrument will change the treatment but not the outcome or confounders) and can thus be used to estimate the treatment effect (among those individuals who are influenced by the instrument). This idea will be explained in great detail in the next section, but for now I will try to convince you that blood type is indeed an instrument.\nClearly blood type will affect the treatment (whether or not the patient is transplanted within a month), but for blood type to be an instrument, we also need to make sure that blood type has no effect on the outcome (other than through it’s effect on the treatment). That is, we need to check the exclusion restriction. It turns out that, unfortunately, the exclusion restriction is uncheckable (similarly to how it is impossible to check that there are no unobserved confounders). The exclusion restriction will forever remain a critical assumption that will need to be backed up by domain knowledge.\nIn the blood type case, it is easy to find faults in the exclusion restriction assumption: it is well-known that blood type and race are correlated, and that race and life-expectancy are correlated (implying that blood type and survival might be related via race). However, to keep things simple, we will define our outcome to be a binary variable corresponding to whether or not death occurs within 1 year. It is highly unlikely that blood type will be related to such a short term survival outcome, even via correlation with race. Moreover, blood type is more-or-less randomly assigned (given your parent’s blood type). In this case, the exclusion restriction is fairly plausible, but in many IV setups, a lot of care will need to be taken to back up the exclusion restriction assumption.\nNow that we (hopefully) understand what an instrument is, the next question is how to use it (don’t worry if you have no idea why instruments are useful yet, it’s not at all obvious!).\n\n\nHow do instruments work?\nOur big question is whether there is an effect of being transplanted within one month (the treatment) on death within 1 year (the outcome). However, our standard approaches to estimating the treatment effect are foiled by the presence of unobserved confounding by sickness, a feature that we cannot accurately quantify. If we could accurately measure an individual’s level of sickness then we could adjust for it using regression adjustment or matching techniques. In our case, however, sickness is defined based on doctor intuition (an unmeasurable quantity), so these traditional approaches to dealing with confounding cannot help us.\n\n\n\n\n\n\n\n\n\nFortunately, we have an instrument (blood type) which modifies the treatment (time to transplantation) without modifying any confounders (the exclusion restriction says that the instrument has no effect on the outcome in any way except through the treatment - this would not be true if the instrument was correlated with a confounder). The exclusion restriction is embodied by the lack of an arrow between the instrument (blood type) and the outcome (death within 1 year) in the graph below.\n\n\n\n\n\n\n\n\n\nNow, since the instrument affects the outcome solely through the treatment, any observed difference between the outcome (survival) between different levels of the instrument (blood types) must be due to the subsequent differences in the treatment (wait time).\nIt turns out that the effect of having blood type AB versus blood type O is that an individual is 7% less likely to die within 1 year. Since under the exclusion restriction there is no direct effect of blood type on death within 1 year (except via the treatment), this must be due to the difference in transplant wait time (the treatment) between the blood types.\nSince patients with blood type AB are 23% more likely to be transplanted within 1 month than patients with blood type O, we can conclude that the effect of a 23% increase in the chance of being transplanted within 1 month is a 7% decrease in the chance of death within one year.\n\n\n\n\n\n\n\n\n\nMathematically, to scale up the effect of “being 23% more likely to be transplanted within a month” to the effect of “being 100% more likely to be transplanted within a month” (i.e. of being transplanted within a month vs not being transplanted within a month) is equal to\n\\[\\frac{\\textrm{Effect of AB vs O on death in 1 year}}{\\textrm{Effect of AB vs O on transplantation in 1 month}} = \\frac{-0.07}{0.23} = -0.3\\]\nThat is, being transplanted within 1 month decreases the chance of death within 1 year by 30%!\nIt is important to note that since we are estimating the effect of changes in the treatment that were caused by the instrument only (rather than the effects of arbitrary changes in the treatment), the instrumental variables treatment effect estimate is not for the entire population, but only for those who are influenced in some way by the instrument (the “compliers”). Thus this estimate is called the Local Average Treatment Effect (LATE).\nThe estimator above is often referred to as the Wald estimator and can be written as\n\\[\\textrm{Wald estimator} = \\frac{\\textrm{Cov(outcome, instrument)}}{\\textrm{Cov(treatment, instrument)}}\\]\nIn practice, IV is often implemented in a two-stage lease squares (2SLS) procedure that can be shown quite easily to be equivalent to the Wald estimator in simple cases (i.e. when not adjusting for other variables).\n\n\nResources\n\nThe resource that I found to be most useful was Chapter 10.5 of Andrew Gelman and Jennifer Hill’s book Data Analysis Using Regression and Multilevel/Hierarchical Models\nThe series of YouTube videos by Ben Lambert were also really helpful https://youtu.be/NLgB2WGGKUw\n\n\n\nNotes\n\nIt turns out that you can quantify sickness in the context of liver transplantation using something called the MELD score (model for end-stage liver disease score). But even if we know current MELD score, future MELD score will still be a confounder since if your MELD rapidly increases then you are getting sicker faster and will be transplanted sooner.\nIf there is anything that is unclear in this post, please feel free to ask questions in the comments section so that I can improve it."
  },
  {
    "objectID": "blog/2017-07-05-ip-weighting.html",
    "href": "blog/2017-07-05-ip-weighting.html",
    "title": "The intuition behind inverse probability weighting in causal inference",
    "section": "",
    "text": "In my previous post, I introduced causal inference as a field interested in estimating the unobservable causal effects of a treatment: i.e. the difference between some measured outcome when the individual is assigned a treatment and the same outcome when the individual is not assigned the treatment.\nIf you’d like to quickly brush up on your causal inference, the fundamental issue associated with making causal inferences, and in particular, the troubles that arise in the presence of confounding, I suggest you read my previous post on this topic."
  },
  {
    "objectID": "blog/2017-07-05-ip-weighting.html#standardized-ip-weighting",
    "href": "blog/2017-07-05-ip-weighting.html#standardized-ip-weighting",
    "title": "The intuition behind inverse probability weighting in causal inference",
    "section": "Standardized IP-weighting",
    "text": "Standardized IP-weighting\nOne common issue with IP-weighting is that individuals with a propensity score very close to 0 (i.e. those extremely unlikely to be treated) will end up with a horrifyingly large weight, potentially making the weighted estimator highly unstable.\nA common alternative to the conventional weights that at least “kind of” addresses this problem are the stabilized weights, which use the marginal probability of treatment instead of \\(1\\) in the weight numerator.\nFor treated individuals, the stabilized weight is given by\n\\[w(x) = \\frac{P(T = 1)}{p(x)} = \\frac{P(T = 1)}{P(T = 1 | X = x)}\\]\nand for control individuals, the stabilized weight is \\[w(x) = \\frac{1 - P(T = 1)}{1 - p(x)} = \\frac{1 - P(T = 1)}{1 - P(T = 1 | X = x)}\\]\nNote that whereas the original weights essentially doubles the sample size, these stabilized weights preserve the sample size."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Rebecca",
    "section": "",
    "text": "Hi there! I’m Rebecca. I’m so glad you’ve made it to my page.\nI’m a statistician, data scientist, educator, and communicator who has a dual passion for empowering others by teaching critical thinking and technical skills for data science, and for uncovering the hidden patterns and stories that live inside complex datasets (primarily related to healthcare and medicine). I spend most of my time working towards improving current approaches to teaching statistics, data literacy and communication, both at introductory and advanced levels, and conducting deep dives into healthcare data, developing predictive models and producing explanatory data visualizations. I am currently a Research Assistant Professor at the University of Utah in Salt Lake City, where I teach data science across campus and am working to make healthcare data more accessible.\nOriginally from Australia, I moved to California in 2014 and graduated from my PhD in Statistics at UC Berkeley in December 2019, during which time I was advised primarily by Prof Bin Yu. During much of my PhD I was a Data Science Fellow at the Berkeley Institute for Data Science (BIDS). My PhD research focused on data science in healthcare (such as predicting surgical site infections using electronic health records, and using causal inference to investigate the impact of liver transplant wait time on survival).\nAs an experienced educator, certified RStudio instructor, and certified Software Carpentry instructor, I have been the primary lecture for an undergraduate statistics course at UC Berkeley, I have taught dozens of R and data science-related workshops, and have helped hundreds of thousands of people worldwide learn R and statistics through this blog.\nMy primary upcoming project is a book, “Veridical Data Science: The Practice of Responsible Data Analysis and Decision Making”, that I am co-authoring with my PhD advisor and ongoing mentor, Professor Bin Yu. Veridical Data Science combines Professor Yu’s decades-long experience in the field of data science and her innovative Veridical Data Science framework with my enthusiasm for explaining complex data science topics in a simple and intuitive way. We hope that Veridical Data Science can be a practical handbook for new (and old) Data Scientists, highlighting the role of judgment calls and critical thinking in data science, and we hope that it will play a significant role in the education of the next generation of data scientists. Veridical Data Science will be published by MIT Press sometime in 2023. Stay tuned!\nWhen I’m not doing R or data stuff, I’m usually found hiking, climbing rocks somewhere, or working some art project or another!\nIf you’ve found my blogs helpful and want to buy me a coffee or tea, feel free to contribute over at https://ko-fi.com/rlbarter.\n\nIf you want to get in touch, you can email me at rlbarter at gmail dot com"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Rebecca Barter",
    "section": "",
    "text": "AI Tutorial: Using Text Embeddings to Label Synthetic Doctor’s Notes Generated with ChatGPT\n\n\n\n\n\n\ndata science\n\n\ngenerative AI\n\n\nChatGPT\n\n\nOpenAI\n\n\nmachine learning\n\n\n\nI’ve been playing around with OpenAI and ChatGPT in my research, and I thought I’d put together a short tutorial that demonstrates using ChatGPT API to generate synthetic doctor’s notes, and then using OpenAI’s text embedding models to label the notes according to whether they involve a chronic or acute condition. And yes, I’m fully aware that what I write here will probably be out of date in about 3 hours.\n\n\n\n\n\nApr 11, 2024\n\n\nRebecca Barter\n\n\n\n\n\n\n\n\n\n\n\n\nAn introduction to Python for R Users\n\n\n\n\n\n\npython\n\n\n\nI have a confession to make: I am now a Python user. Don’t judge me, join me! In this post, I introduce Python for data analysis from the perspective of an R (tidyverse) user. This post is a must-read if you are an R user hoping to dip your toes in the Python pool.\n\n\n\n\n\nSep 12, 2023\n\n\nRebecca Barter\n\n\n\n\n\n\n\n\n\n\n\n\nA list of public data repositories\n\n\n\n\n\n\ndata\n\n\ndatasets\n\n\n\nSources for lesser-known and better-known public datasets.\n\n\n\n\n\nMar 28, 2023\n\n\nRebecca Barter\n\n\n\n\n\n\n\n\n\n\n\n\nThanks, Quarto, for saving my blog!\n\n\n\n\n\n\nblog\n\n\n\nMy old blog broke so I decided to re-make my entire website using quarto, which was disturbingly easy. Welcome to my new and improved blog!\n\n\n\n\n\nMar 21, 2023\n\n\nRebecca Barter\n\n\n\n\n\n\n\n\n\n\n\n\nIt’s time for statistics departments to start supporting their applied students\n\n\n\n\n\n\nstatistics\n\n\n\nStatistics departments are failing their applied students. In this post, I have a lot of opinions and give two pieces of advice: statistics departments need to start supporting their applied students, and they need to hire applied faculty. \n\n\n\n\n\nJul 30, 2020\n\n\nRebecca Barter\n\n\n\n\n\n\n\n\n\n\n\n\nAcross (dplyr 1.0.0): applying dplyr functions simultaneously across multiple columns\n\n\n\n\n\n\nR\n\n\ntidyverse\n\n\ndplyr\n\n\n\nWith the introduction of dplyr 1.0.0, there are a few new features: the biggest of which is across() which supersedes the scoped versions of dplyr functions.\n\n\n\n\n\nJul 9, 2020\n\n\nRebecca Barter\n\n\n\n\n\n\n\n\n\n\n\n\nTidymodels: tidy machine learning in R\n\n\n\n\n\n\nR\n\n\ntidyverse\n\n\nmachine learning\n\n\ntidymodels\n\n\ncaret\n\n\nrecipes\n\n\nparsnip\n\n\ntune\n\n\nrsample\n\n\n\nThe tidyverse’s take on machine learning is finally here. Tidymodels forms the basis of tidy machine learning, and this post provides a whirlwind tour to get you started.\n\n\n\n\n\nApr 14, 2020\n\n\nRebecca Barter\n\n\n\n\n\n\n\n\n\n\n\n\n5 useful R tips from rstudio::conf(2020) - tidy eval, piping, conflicts, bar charts and colors\n\n\n\n\n\n\nR\n\n\nrstudioconf\n\n\ntidyverse\n\n\nggplot2\n\n\ntidyeval\n\n\nvisualization\n\n\n\nLast week I had the pleasure of attending rstudio::conf(2020) in San Francisco. Throughout the course of the week I met many wonderful people and learnt many things. This post covers some of the little tips and tricks that I learnt throughout the conference.\n\n\n\n\n\nFeb 6, 2020\n\n\nRebecca Barter\n\n\n\n\n\n\n\n\n\n\n\n\nBecoming an R blogger\n\n\n\n\n\n\nR\n\n\nrstudioconf\n\n\nblog\n\n\n\nIn this post I discuss why I became an R blogger, why you should too, and some tips and tricks to get your started. This post is based on my rstudio::conf(2020) talk called ‘Becoming an R blogger’\n\n\n\n\n\nFeb 3, 2020\n\n\nRebecca Barter\n\n\n\n\n\n\n\n\n\n\n\n\nLearn to purrr\n\n\n\n\n\n\nR\n\n\npurrr\n\n\ntidyverse\n\n\n\nPurrr is the tidyverse’s answer to apply functions for iteration. It’s one of those packages that you might have heard of, but seemed too complicated to sit down and learn. Starting with map functions, and taking you on a journey that will harness the power of the list, this post will have you purrring in no time.\n\n\n\n\n\nAug 19, 2019\n\n\nRebecca Barter\n\n\n\n\n\n\n\n\n\n\n\n\nTransitioning into the tidyverse (part 1)\n\n\n\n\n\n\nR\n\n\ntidyverse\n\n\ndplyr\n\n\nggplot2\n\n\npipes\n\n\n\nThis post walks through what base R users need to know for their transition into the tidyverse. Part 1 focuses on piping and the ‘base’ packages, dplyr and ggplot2.\n\n\n\n\n\nAug 5, 2019\n\n\nRebecca Barter\n\n\n\n\n\n\n\n\n\n\n\n\nTransitioning into the tidyverse (part 2)\n\n\n\n\n\n\nR\n\n\ntidyverse\n\n\ntidyr\n\n\npurrr\n\n\nreadr\n\n\ntibbles\n\n\nlubridate\n\n\nforcats\n\n\nstringr\n\n\n\nThis post walks through what base R users need to know for their transition into the tidyverse. Part 2 focuses on the more specialized R packages tidyr, purrr, readr, lubridate, forcats, etc\n\n\n\n\n\nAug 5, 2019\n\n\nRebecca Barter\n\n\n\n\n\n\n\n\n\n\n\n\nUsing the recipes package for easy pre-processing\n\n\n\n\n\n\nR\n\n\nworkflow\n\n\nmachine learning\n\n\n\nHaving to apply the same pre-processing steps to training, testing and validation data to do some machine learning can be surprisingly frustrating. But thanks to the recipes R package, it’s now super-duper easy. Instead of having five functions and maybe hundreds of lines of code, you can preprocess multiple datasets using a single ‘recipe’ in fewer than 10 lines of code.\n\n\n\n\n\nJun 6, 2019\n\n\nRebecca Barter\n\n\n\n\n\n\n\n\n\n\n\n\nA quick guide to developing a reproducible and consistent data science workflow\n\n\n\n\n\n\ndata science\n\n\nworkflow\n\n\nreproducibility\n\n\n\nWhen you’re learning to code and perform data analysis, it can be overwhelming to figure out how to structure your projects. To help data scientists develop a reproducible and consistent workflow, I’ve put together a short document with some guiding advice.\n\n\n\n\n\nMar 23, 2019\n\n\nRebecca Barter\n\n\n\n\n\n\n\n\n\n\n\n\nmutate_all(), select_if(), summarise_at()… what’s the deal with scoped verbs?!\n\n\n\n\n\n\ndplyr\n\n\nR\n\n\ntidyverse\n\n\n\nWhat’s the deal with these mutate_all(), select_if(), summarise_at(), functions? They seem so useful, but there doesn’t seem to be a decent explanation of how to use them anywhere on the internet. Turns out, they’re called ‘scoped verbs’ and hopefully this post will become one of many decent explanations of how to use them!\n\n\n\n\n\nJan 23, 2019\n\n\nRebecca Barter\n\n\n\n\n\n\n\n\n\n\n\n\nWhich hypothesis test should I use? A flowchart\n\n\n\n\n\n\nstatistics\n\n\nhypothesis testing\n\n\n\nA flowchart to decide what hypothesis test to use.\n\n\n\n\n\nDec 4, 2018\n\n\nRebecca Barter\n\n\n\n\n\n\n\n\n\n\n\n\nAlternatives to grouped bar charts\n\n\n\n\n\n\nR\n\n\nvisualization\n\n\n\nOne of the most common chart types that is simultaneously the most difficult to read is the grouped bar chart. Fortunately, there exist several substantially more effective alternatives that convey the same information without overwhelming our visual cognition abilities.\n\n\n\n\n\nMay 29, 2018\n\n\nRebecca Barter\n\n\n\n\n\n\n\n\n\n\n\n\nGetting fancy with ggplot2: code for alternatives to grouped bar charts\n\n\n\n\n\n\nR\n\n\nvisualization\n\n\n\nIn this post I present the code I wrote to prodocue the figures in my previous post about alternatives to grouped bar charts.\n\n\n\n\n\nMay 29, 2018\n\n\nRebecca Barter\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Instrumental Variables\n\n\n\n\n\n\ncausal inference\n\n\nstatistics\n\n\n\nInstrumental variables is one of the most mystical concepts in causal inference. For some reason, most of the existing explanations are overly complicated and focus on specific nuanced aspects of generating IV estimates without really providing the intuition for why it makes sense. In this post, you will not find too many technical details, but rather a narrative introducing instruments and why they are useful.\n\n\n\n\n\nMay 23, 2018\n\n\nRebecca Barter\n\n\n\n\n\n\n\n\n\n\n\n\nggplot2: Mastering the basics\n\n\n\n\n\n\nR\n\n\nvisualization\n\n\n\nMaking graphs in R with ggplot2 is easy! This post will cover the basics to get you started on your ggplot2 adventures\n\n\n\n\n\nNov 17, 2017\n\n\nRebecca Barter\n\n\n\n\n\n\n\n\n\n\n\n\nA basic tutorial of caret: the machine learning package in R\n\n\n\n\n\n\nR\n\n\nmachine learning\n\n\n\nR has a wide number of packages for machine learning (ML), which is great, but also quite frustrating since each package was designed independently and has very different syntax, inputs and outputs. Caret unifies these packages into a single package with constant syntax, saving everyone a lot of frustration and time!\n\n\n\n\n\nNov 17, 2017\n\n\nRebecca Barter\n\n\n\n\n\n\n\n\n\n\n\n\nA Basic Data Science Workflow\n\n\n\n\n\n\nR\n\n\nworkflow\n\n\n\nDeveloping a clean and easy analysis workflow takes a really, really long time. In this post, I outline the workflow that I have developed over the last few years.\n\n\n\n\n\nAug 18, 2017\n\n\nRebecca Barter\n\n\n\n\n\n\n\n\n\n\n\n\nConfounding in causal inference: what is it, and what to do about it?\n\n\n\n\n\n\ncausal inference\n\n\nstatistics\n\n\n\nAn introduction to the field of causal inference and the issues surrounding confounding.\n\n\n\n\n\nJul 5, 2017\n\n\nRebecca Barter\n\n\n\n\n\n\n\n\n\n\n\n\nThe intuition behind inverse probability weighting in causal inference\n\n\n\n\n\n\ncausal inference\n\n\nstatistics\n\n\n\nRemoving confounding can be done via a variety methods including IP-weighting. This post provides a summary of the intuition behind IP-weighting.\n\n\n\n\n\nJul 5, 2017\n\n\nRebecca Barter\n\n\n\n\n\n\n\n\n\n\n\n\nMigrating from GitHub Pages to Netlify: how and why?\n\n\n\n\n\n\nblog\n\n\n\nSorry GitHub Pages, but we need to break up. I’ve found a new web hosting service.\n\n\n\n\n\nJun 29, 2017\n\n\nRebecca Barter\n\n\n\n\n\n\n\n\n\n\n\n\nCoolors: choosing color schemes\n\n\n\n\n\n\nvisualization\n\n\nR\n\n\n\nA really cool website for choosing color palattes\n\n\n\n\n\nJun 27, 2017\n\n\nRebecca Barter\n\n\n\n\n\n\n\n\n\n\n\n\nInteractive visualization in R\n\n\n\n\n\n\nvisualization\n\n\nR\n\n\ninteractivity\n\n\n\nLearn about creating interactive visualizations in R.\n\n\n\n\n\nApr 20, 2017\n\n\nRebecca Barter\n\n\n\n\n\n\n\n\n\n\n\n\nDocathon: A Week of Doumentation\n\n\n\n\n\n\ndocumentation\n\n\nR\n\n\ncommunication\n\n\n\nWe’re hosting a week-long docathon over at BIDS.\n\n\n\n\n\nMar 7, 2017\n\n\nRebecca Barter\n\n\n\n\n\n\n\n\n\n\n\n\nANOVA\n\n\n\n\n\n\nstatistics\n\n\nanova\n\n\n\nA bunch of statisticians met to learn about ANOVA, a method that they’re supposed to already know about. Here is my summary.\n\n\n\n\n\nFeb 20, 2017\n\n\nRebecca Barter\n\n\n\n\n\n\n\n\n\n\n\n\nsuperheat 0.1.0\n\n\n\n\n\n\nR\n\n\nsuperheat\n\n\n\nFirst version of superheat now available on CRAN.\n\n\n\n\n\nFeb 5, 2017\n\n\nRebecca Barter\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Present Good\n\n\n\n\n\n\ncommunication\n\n\n\nThis week I was asked to give a presentation on presenting. Here lies a summary of my efforts.\n\n\n\n\n\nFeb 2, 2017\n\n\nRebecca Barter\n\n\n\n\n\n\n\n\n\n\n\n\nSuperheat: a simple example\n\n\n\n\n\n\nR\n\n\nsuperheat\n\n\n\nA simple example of using superheat to create beautiful heatmaps.\n\n\n\n\n\nFeb 2, 2017\n\n\nRebecca Barter\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/2023-03-28-data_sources.html",
    "href": "blog/2023-03-28-data_sources.html",
    "title": "A list of public data repositories",
    "section": "",
    "text": "I spend a lot of my time developing educational materials for data science, and since I always teach from a real-data perspective, this means that I spend a lot of my time looking for datasets. This blog post summarizes the first places I go when looking for a new dataset to play around with.\nWhile there are big data repositories like Kaggle, UCI Machine Learning repository, and data.gov, I often find more useful datasets in smaller, more curated repositories, which are often compiled by just one person, rather than being a massive repository of open datasets."
  },
  {
    "objectID": "blog/2023-03-28-data_sources.html#tidy-tuesday",
    "href": "blog/2023-03-28-data_sources.html#tidy-tuesday",
    "title": "A list of public data repositories",
    "section": "Tidy Tuesday",
    "text": "Tidy Tuesday\n\nLink: https://github.com/rfordatascience/tidytuesday\nTidy Tuesday is much more than “just a data repository”. Tidy Tuesday is an entire community, originally brought together by Tom Mock to help people improve their data analysis and visualization skills using R. Tidy Tuesday provides weekly data sets and data exploration challenges, with thousands of participants from all around the world sharing their work, offering feedback, and collaborating on projects. Whether you are a seasoned data professional or just starting out, Tidy Tuesday is an excellent resource for learning and growth in the data science field."
  },
  {
    "objectID": "blog/2023-03-28-data_sources.html#data-is-plural",
    "href": "blog/2023-03-28-data_sources.html#data-is-plural",
    "title": "A list of public data repositories",
    "section": "Data is plural",
    "text": "Data is plural\n\nLink: https://www.data-is-plural.com/\n“Data is Plural” is a weekly newsletter of “useful/curious datasets”, published by Jeremy Singer-Vine. The Data is Plural newsletter is delivered each week straight to your inbox and typically features a curated collection of recent and relevant high-quality and diverse datasets from a wide range of domains, including economics, sports, politics, science, and more."
  },
  {
    "objectID": "blog/2023-03-28-data_sources.html#flowingdata",
    "href": "blog/2023-03-28-data_sources.html#flowingdata",
    "title": "A list of public data repositories",
    "section": "FlowingData",
    "text": "FlowingData\n\nLink: https://flowingdata.com/\nWhile it is not technically a data repository, Nathan Yau’s website FlowingData contains a range of detailed visualizations and analyses of a range of interesting datasets, and he always provides a reference to the datasets that underlie his articles. Yau also offers a range of courses and tutorials for readers who are intersted in improving their data visualization and analysis skills"
  },
  {
    "objectID": "blog/2023-03-28-data_sources.html#fivethirtyeight",
    "href": "blog/2023-03-28-data_sources.html#fivethirtyeight",
    "title": "A list of public data repositories",
    "section": "FiveThirtyEight",
    "text": "FiveThirtyEight\n\nLink: https://data.fivethirtyeight.com/ (gihub repo: https://github.com/fivethirtyeight/data)\nFiveThirtyEight, originally a blog created by Nate Silver, and now a large journalism site provides copies of the data that underlies their data-driven articles on github. Since FiveThirtyEight’s initial focus was political polls and elections (and I guess since Nate Silver is into sports), much of their data tends to be political and sports-related, but there are also other data goodies to be found in their repo too."
  },
  {
    "objectID": "blog/2023-03-28-data_sources.html#propublica",
    "href": "blog/2023-03-28-data_sources.html#propublica",
    "title": "A list of public data repositories",
    "section": "ProPublica",
    "text": "ProPublica\n\nLink: https://www.propublica.org/datastore/datasets\nProPublica, a non-profit investigative journalism outfit, provides a large number of datasets for free (and even more for a premium membership fee). I honestly haven’t used this resource much myself, but I’ve been aware of it, and it seems to have some fairly interesting datasets at first glance!"
  },
  {
    "objectID": "blog/2023-03-28-data_sources.html#inbuilt-datasets-in-r",
    "href": "blog/2023-03-28-data_sources.html#inbuilt-datasets-in-r",
    "title": "A list of public data repositories",
    "section": "Inbuilt datasets in R",
    "text": "Inbuilt datasets in R\n\nLink: https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/00Index.html\nSometimes, when I need something quick-and-dirty, the inbuilt datasets in R will do the job! You can look at the list of datasets available in R by typing data() in R. They are also listed here.\nThere are also several packages that import datasets along with their functions, including ggplot2. You can look at the specific packages that are loaded with a package using the package argument in the data() function, for example:\n\ndata(package = \"ggplot2\")"
  },
  {
    "objectID": "blog/2023-03-28-data_sources.html#uci-machine-learning-repository",
    "href": "blog/2023-03-28-data_sources.html#uci-machine-learning-repository",
    "title": "A list of public data repositories",
    "section": "UCI Machine Learning repository",
    "text": "UCI Machine Learning repository\n\nLink: https://archive.ics.uci.edu/ml/index.php\nThe UCI Machine Learning repository is a public dataset repository maintained by the Machine Learning Group at the University of California, Irvine. The datasets are typically organized into the type of machine learning problem they can be used for, but they come from various domains, including biology, finance, social sciences, and engineering. If I’m being honest, the UCI Machine Learning repository is fairly “old-school”, and much of what you can find on the UCI ML repository can also now be found on Kaggle."
  },
  {
    "objectID": "blog/2023-03-28-data_sources.html#kaggle",
    "href": "blog/2023-03-28-data_sources.html#kaggle",
    "title": "A list of public data repositories",
    "section": "Kaggle",
    "text": "Kaggle\n\nLink: https://www.kaggle.com/\nKaggle began as a machine learning competition platform, and by nature of machine learning competitions, kaggle has also ended up being a place that hosts a wide range of datasets that are particularly well-suited to machine learning applications (the datasets for all past challenges can be downloaded from the Kaggle Website)."
  },
  {
    "objectID": "blog/2023-03-28-data_sources.html#google-dataset-search",
    "href": "blog/2023-03-28-data_sources.html#google-dataset-search",
    "title": "A list of public data repositories",
    "section": "Google Dataset Search",
    "text": "Google Dataset Search\n\nLink: https://datasetsearch.research.google.com/\nGoogle Dataset Search is… well… Google but for datasets. As a search engine specifically designed for finding datasets, it provides access to a wide range of datasets from various sources and domains, including research publications and government databases. My issue with Google Dataset Search, however, is that you basically already need to know what you’re looking for in order to search for it. It is a great resource for people looking for datasets on specific topics though."
  },
  {
    "objectID": "blog/2023-03-28-data_sources.html#amazon-web-services-registry-of-open-data",
    "href": "blog/2023-03-28-data_sources.html#amazon-web-services-registry-of-open-data",
    "title": "A list of public data repositories",
    "section": "Amazon Web Services Registry of Open Data",
    "text": "Amazon Web Services Registry of Open Data\n\nLink: https://registry.opendata.aws/\nAmazon Web Services provides access to a large collection of public datasets that can be used for research and development projects. As far as I can tell, the focus is on medical and environmental datasets. These datasets are hosted on the Amazon cloud infrastructure, and can be accessed using Amazon Web Services tools."
  },
  {
    "objectID": "blog/2023-03-28-data_sources.html#data.gov",
    "href": "blog/2023-03-28-data_sources.html#data.gov",
    "title": "A list of public data repositories",
    "section": "data.gov",
    "text": "data.gov\n\nLink: https://data.gov/\ndata.gov is a government-owned online platform that provides public access to a vast collection of datasets from different federal agencies in the United States, which includes demographic data, weather data, environmental data, and economic data, among others.\n\n\n\n\n\n\n\n\nIf you have some go-to resources for data that I haven’t mentioned here, let me know by leaving a comment below!"
  },
  {
    "objectID": "consulting.html",
    "href": "consulting.html",
    "title": "Consulting and workshops",
    "section": "",
    "text": "I regularly teach multi-day workshops for small and large groups.\nThe kinds of topics I love teaching include programming skills such as:\n\nR\nPython\nSQL\nGit/GitHub\nShell/UNIX\n\nAs well as more general data science skills including:\n\nData cleaning\nData visualization\nData science workflows\n\nFeel free to get in touch via email at rlbarter at gmail dot com!"
  },
  {
    "objectID": "blog/2023-09-11-from_r_to_python.html",
    "href": "blog/2023-09-11-from_r_to_python.html",
    "title": "An introduction to Python for R Users",
    "section": "",
    "text": "I have a confession to make: I am now a Python user. To be fair, I’ve used Python here and there over the years, but it was never my primary language (that has always been R). This year was the first time I had to actually sit down and really use Python for real projects. In fact, not only did I have to use Python, but I also had to teach Python. For those of you who teach, you’ll know that the best way to make sure you know something really well is to teach it. The end result is that I now consider myself a Python user. (But don’t worry, I’m still an R Lady at heart.)\nWhy did I decide to learn Python? I’ve been using R my whole data science life, and while I still think that R (with the tidyverse) is still the best language for data wrangling and data visualization, there is no denying that as a data scientist these days, Python is a required skill. If you’re doing machine learning, Python is still light-years ahead of R. If you’re working with software engineers, they’re going to be much happier working with you if you use Python. If you’re looking for a job, you’re going to be much more employable if you’re already comfortable with Python.\nWhile there are certainly similarities between R and Python, don’t assume that knowing how to use one will automatically mean that you know how to use the other. That said, already knowing R does mean that the learning curve for learning Python won’t be too steep (but it will still take a few months of regular use to reach competence).\nIn this post, I’m going to introduce you to the world of data analysis with Python from an R perspective. Obviously this post won’t be exhaustive, but if you’re a tidyverse R user who is looking to learn Python, this post can hopefully serve as a helpful launching point and will provide some relatable context on your Python journey.\nNote that this blog post will focus on working with data in Python using pandas. I won’t be talking too much about things like SciPy, arrays, or scikit-learn here."
  },
  {
    "objectID": "blog/2023-09-11-from_r_to_python.html#helpful-resources",
    "href": "blog/2023-09-11-from_r_to_python.html#helpful-resources",
    "title": "An introduction to Python for R Users",
    "section": "Helpful resources",
    "text": "Helpful resources\nThere are a lot of resources out there for learning Python. The one that I found most useful was Wes McKinney’s Python for Data Analysis book (Wes is the creator of pandas). There is a lot of information in there, and to be fair, you can skip a lot of it when you’re starting out (like the stuff about sets, tuples, and arrays). These are important things to know about to be a well-rounded Python programmer, but for just doing simple data analysis with pandas data frames, these needn’t be the focus."
  },
  {
    "objectID": "blog/2023-09-11-from_r_to_python.html#installing-python-and-vs-code",
    "href": "blog/2023-09-11-from_r_to_python.html#installing-python-and-vs-code",
    "title": "An introduction to Python for R Users",
    "section": "Installing Python and VS Code",
    "text": "Installing Python and VS Code\nManaging Python installations on my computer used to give me a headache. While I used to use Python in Jupyter notebooks via the Jupyter notebook IDE installed using anaconda, I’ve found that the simplest approach to getting Python up and running is now to install the latest version of Python directly from the python website and install the Visual Studio Code IDE. Then you can select your preferred python installation within VS code and you’re good to go. (You might want to watch a couple of YouTube videos to get started with VS code if you’ve never seen it before. It definitely took me a minute to orient myself.)\n\nJupyter notebooks\nWhile, these days, you can use Python together with quarto within RStudio, this isn’t really what Python users do (yet…). So if you want to fit in with the cool Python kids, I’d recommend working with Jupyter notebooks (.ipynb files) in the Visual Studio Code IDE. You can install a jupyter notebook extension within the VS Code IDE (fortunately, there is no need to install jupyter notebooks or anaconda separately)."
  },
  {
    "objectID": "blog/2023-09-11-from_r_to_python.html#libraries",
    "href": "blog/2023-09-11-from_r_to_python.html#libraries",
    "title": "An introduction to Python for R Users",
    "section": "Libraries",
    "text": "Libraries\nLike R, Python is mostly useful for data science because of the add-on libraries that some very smart people wrote to help us work with data. The main python libraries you’ll need to start with are NumPy and pandas.\n\nInstalling libraries\nLike with R, you need to install a library before you can use it.\nThere are many ways to install python libraries, but if you installed python in the same way that I did above, the way that I usually install libraries is in the terminal on my computer, where I write:\n\npython3 -m pip install pandas\n\nwhich will install the pandas library, for example. You may have a different preferred way of installing python libraries. That’s fine. Your way is probably better than mine.\n\n\nLoading libraries\nOne difference between R and Python is that once you have loaded a package into R, you can use the functions from the package without having to specify which library the package comes from every time you use it like this:\n\nlibrary(dplyr)\n\n\n# I can use `filter()` from dplyr without having to specify that it comes from dplyr\nfilter(iris, Species == \"virginica\")\n\n   Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n1           6.3         3.3          6.0         2.5 virginica\n2           5.8         2.7          5.1         1.9 virginica\n3           7.1         3.0          5.9         2.1 virginica\n4           6.3         2.9          5.6         1.8 virginica\n5           6.5         3.0          5.8         2.2 virginica\n6           7.6         3.0          6.6         2.1 virginica\n7           4.9         2.5          4.5         1.7 virginica\n8           7.3         2.9          6.3         1.8 virginica\n9           6.7         2.5          5.8         1.8 virginica\n10          7.2         3.6          6.1         2.5 virginica\n11          6.5         3.2          5.1         2.0 virginica\n12          6.4         2.7          5.3         1.9 virginica\n13          6.8         3.0          5.5         2.1 virginica\n14          5.7         2.5          5.0         2.0 virginica\n15          5.8         2.8          5.1         2.4 virginica\n16          6.4         3.2          5.3         2.3 virginica\n17          6.5         3.0          5.5         1.8 virginica\n18          7.7         3.8          6.7         2.2 virginica\n19          7.7         2.6          6.9         2.3 virginica\n20          6.0         2.2          5.0         1.5 virginica\n21          6.9         3.2          5.7         2.3 virginica\n22          5.6         2.8          4.9         2.0 virginica\n23          7.7         2.8          6.7         2.0 virginica\n24          6.3         2.7          4.9         1.8 virginica\n25          6.7         3.3          5.7         2.1 virginica\n26          7.2         3.2          6.0         1.8 virginica\n27          6.2         2.8          4.8         1.8 virginica\n28          6.1         3.0          4.9         1.8 virginica\n29          6.4         2.8          5.6         2.1 virginica\n30          7.2         3.0          5.8         1.6 virginica\n31          7.4         2.8          6.1         1.9 virginica\n32          7.9         3.8          6.4         2.0 virginica\n33          6.4         2.8          5.6         2.2 virginica\n34          6.3         2.8          5.1         1.5 virginica\n35          6.1         2.6          5.6         1.4 virginica\n36          7.7         3.0          6.1         2.3 virginica\n37          6.3         3.4          5.6         2.4 virginica\n38          6.4         3.1          5.5         1.8 virginica\n39          6.0         3.0          4.8         1.8 virginica\n40          6.9         3.1          5.4         2.1 virginica\n41          6.7         3.1          5.6         2.4 virginica\n42          6.9         3.1          5.1         2.3 virginica\n43          5.8         2.7          5.1         1.9 virginica\n44          6.8         3.2          5.9         2.3 virginica\n45          6.7         3.3          5.7         2.5 virginica\n46          6.7         3.0          5.2         2.3 virginica\n47          6.3         2.5          5.0         1.9 virginica\n48          6.5         3.0          5.2         2.0 virginica\n49          6.2         3.4          5.4         2.3 virginica\n50          5.9         3.0          5.1         1.8 virginica\n\n\nI could choose to explicitly specify that filter() comes from dplyr using :: like this:\n\n# I can explicitly specify that `filter()` comes from dplyr using `::`\ndplyr::filter(iris, Species == \"virginica\")\n\n   Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n1           6.3         3.3          6.0         2.5 virginica\n2           5.8         2.7          5.1         1.9 virginica\n3           7.1         3.0          5.9         2.1 virginica\n4           6.3         2.9          5.6         1.8 virginica\n5           6.5         3.0          5.8         2.2 virginica\n6           7.6         3.0          6.6         2.1 virginica\n7           4.9         2.5          4.5         1.7 virginica\n8           7.3         2.9          6.3         1.8 virginica\n9           6.7         2.5          5.8         1.8 virginica\n10          7.2         3.6          6.1         2.5 virginica\n11          6.5         3.2          5.1         2.0 virginica\n12          6.4         2.7          5.3         1.9 virginica\n13          6.8         3.0          5.5         2.1 virginica\n14          5.7         2.5          5.0         2.0 virginica\n15          5.8         2.8          5.1         2.4 virginica\n16          6.4         3.2          5.3         2.3 virginica\n17          6.5         3.0          5.5         1.8 virginica\n18          7.7         3.8          6.7         2.2 virginica\n19          7.7         2.6          6.9         2.3 virginica\n20          6.0         2.2          5.0         1.5 virginica\n21          6.9         3.2          5.7         2.3 virginica\n22          5.6         2.8          4.9         2.0 virginica\n23          7.7         2.8          6.7         2.0 virginica\n24          6.3         2.7          4.9         1.8 virginica\n25          6.7         3.3          5.7         2.1 virginica\n26          7.2         3.2          6.0         1.8 virginica\n27          6.2         2.8          4.8         1.8 virginica\n28          6.1         3.0          4.9         1.8 virginica\n29          6.4         2.8          5.6         2.1 virginica\n30          7.2         3.0          5.8         1.6 virginica\n31          7.4         2.8          6.1         1.9 virginica\n32          7.9         3.8          6.4         2.0 virginica\n33          6.4         2.8          5.6         2.2 virginica\n34          6.3         2.8          5.1         1.5 virginica\n35          6.1         2.6          5.6         1.4 virginica\n36          7.7         3.0          6.1         2.3 virginica\n37          6.3         3.4          5.6         2.4 virginica\n38          6.4         3.1          5.5         1.8 virginica\n39          6.0         3.0          4.8         1.8 virginica\n40          6.9         3.1          5.4         2.1 virginica\n41          6.7         3.1          5.6         2.4 virginica\n42          6.9         3.1          5.1         2.3 virginica\n43          5.8         2.7          5.1         1.9 virginica\n44          6.8         3.2          5.9         2.3 virginica\n45          6.7         3.3          5.7         2.5 virginica\n46          6.7         3.0          5.2         2.3 virginica\n47          6.3         2.5          5.0         1.9 virginica\n48          6.5         3.0          5.2         2.0 virginica\n49          6.2         3.4          5.4         2.3 virginica\n50          5.9         3.0          5.1         1.8 virginica\n\n\nBut I don’t need to.\nIn Python, however, you do need to specify which library the functions you are using come from, even after importing them. This is why every time you import a python library, you should give it a nickname so you don’t have to type out the entire library name each time you want to use a function from it. Fortunately, there are some generally agreed-upon nicknames that everyone uses. E.g. pandas’ nickname is “pd”, and NumPy’s nickname is “np”. (In Python people usually say ‘library’ instead of ‘package’.)\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib as plt\n\nSo to use a function from the NumPy library, such as log(), you need to first specify the library nickname and then the function name, separated by a dot: np.log(). The following code will compute the logarithm of 7 using the log() function from the NumPy library whose nickname is np:\n\nnp.log(7)\n\n1.9459101490553132\n\n\nThat’s another thing about Python: common mathematical functions like log(), sqrt(), and exp() all need to be imported from the NumPy library (unlike in R, where “native” versions of these functions exist)."
  },
  {
    "objectID": "blog/2023-09-11-from_r_to_python.html#pandas-data-frames",
    "href": "blog/2023-09-11-from_r_to_python.html#pandas-data-frames",
    "title": "An introduction to Python for R Users",
    "section": "Pandas data frames",
    "text": "Pandas data frames\nIn R, the building block of data science is the data frame. In Python, the building block of data science is also the DataFrame (but they spell it as one word with camel case). While R contains a native data frame, python’s DataFrame comes from the “pandas” library (whose nickname is “pd”).\nIn R, we can create a toy data frame like this:\n\nr_df &lt;- data.frame(a = c(1, 2, 3, 4),\n                   b = c(5, 6, 7, 8))\nr_df\n\n  a b\n1 1 5\n2 2 6\n3 3 7\n4 4 8\n\n\nAnd in Python, we can create a toy DataFrame like this:\n\npandas_df = pd.DataFrame({'a': [1,2,3,4], \n                          'b': [5,6,7,8]})\npandas_df\n\n   a  b\n0  1  5\n1  2  6\n2  3  7\n3  4  8\n\n\nIn the R version above we provided two arguments to the data.frame() function, each of which contained an R vector (c(1, 2, 3, 4) and c(5, 6, 7, 8)) of the values that will form a column. The syntax for the python version, however, involves a single argument corresponding to a type of object called a dictionary (a dictionary is defined with curly brackets) whose named entries each contain a python list ([1,2,3,4] and [5,6,7,8]) of the values that will form a column.\n\nLists in python\nA list in python is actually a lot like a list in R, in that it can contain a variety of object types. Lists in python are created using square brackets:\n\nmy_py_list = [1,'a',[3,4]]\nmy_py_list\n\n[1, 'a', [3, 4]]\n\n\nNote that in general, python users tend to use fewer spaces than R users, assignment is always done with = (there is no &lt;- operator in python), and there is a general preference for single quotes ' over double quotes \" (though I might have imagined that one).\nYou can change the first entry of a python list similarly to how you would do it in R, except that python uses zero-indexing, which means that the first entry is in position 0, the second entry is in position 1, and so on:\n\nmy_py_list[0]\n\n1\n\nmy_py_list[0] = \"one\"\nmy_py_list\n\n['one', 'a', [3, 4]]\n\n\n\n\nDictionaries\nA dictionary is kind of like a named list. In the example below, both of the entries in the dictionary are themselves lists:\n\nmy_py_dict = {'name': ['Jane', 'Joe', 'Jerry'],\n              'age': [13, 15, 12]}\nmy_py_dict\n\n{'name': ['Jane', 'Joe', 'Jerry'], 'age': [13, 15, 12]}\n\n\nBut while you cannot extract positional entries from it (my_py_dict[0] won’t work), you can extract named entries:\n\nmy_py_dict['name']\n\n['Jane', 'Joe', 'Jerry']\n\n\nI can extract the first (position 0) entry from the name dict entry of my_py_dict as you would expect:\n\nmy_py_dict['name'][0]\n\n'Jane'\n\n\nThis is all super exciting, I’m sure, but I should probably point out that pretty much the only time I ever really use dictionaries is when I’m defining pandas DataFrames on the fly like I did above.\n\n\nColumn and row Indexing\nWhile both the R and python/pandas data frame have column names, the way you extract them is different.\nTo extract the column names from the R data frame, we would use:\n\n# print the R column names\ncolnames(r_df)\n\n[1] \"a\" \"b\"\n\n\nHowever, to extract the column names (or column index) from the pandas data frame, we use:\n\n# print the pandas column names\npandas_df.columns\n\nIndex(['a', 'b'], dtype='object')\n\n\nThe output of which is an “index” type object.\nThere are two main uses of the syntax object.xyz. The first, which is being used here, is used to extract an “attribute” xyz from an object (e.g., here we are extracting the columns attribute from the pandas_df DataFrame object).\nAnother “attribute” that you can extract from a DataFrame is the shape (which is equivalent to dim() in R):\n\npandas_df.shape\n\n(4, 2)\n\n\nThe shape output here has a “tuple” type (I won’t go into tuples here because I’m afraid of them).\nAnother important attribute of a DataFrame is its row index:\n\npandas_df.index\n\nRangeIndex(start=0, stop=4, step=1)\n\n\nThis is a “RangeIndex” object that spans from 0 to 2 (not inclusive) with a step-size of 1 and corresponds to the row index of the DataFrame.\nBoth the column and row indexes of pandas DataFrames are special index objects that can be coerced to a python list using list() (which is a function in python for creating lists).\n\nlist(pandas_df.index)\n\n[0, 1, 2, 3]\n\n\n\nlist(pandas_df.columns)\n\n['a', 'b']\n\n\nWhenever python objects display weirdly, I try and convert them to something simpler, like a list, to see what they actually contain. Sometimes I even succeed.\n\n\nMethods\nExtracting attributes is not the only use of the object.xyz dot syntax. Similar syntax also lets you apply object-specific functions to the object using the object.fun() dot syntax.\nNote that functions that are applied using the object.fun() (syntax where the function name comes after the dot) are called methods. These are functions that are specific to the type of object it is being applied to.\nFor example, we can apply the mean() and sum() DataFrame methods to our pandas_df DataFrame object to compute the mean and sum of each column in the data frame:\n\npandas_df.mean()\n\na    2.5\nb    6.5\ndtype: float64\n\npandas_df.sum()\n\na    10\nb    26\ndtype: int64\n\n\nNote that the output of these functions are pandas “Series” type objects, which is like a single column of a data frame. More on Series’ objects below.\nIt is important to note that these mean() and sum() methods are not standalone functions. If we try to apply them to our DataFrame as we would do for regular functions, we get errors:\n\nmean(pandas_df)\n\nname 'mean' is not defined\n\n\n\nsum(pandas_df)\n\nunsupported operand type(s) for +: 'int' and 'str'\n\n\nSince they are “methods”, these mean() and sum() method functions are specific to the DataFrame type object and must be applied using the pandas_df.fun() dot syntax.\nMoreover, since these method functions are specific to DataFrame objects, they also won’t work on non-DataFrame objects, including lists:\n\npy_list = [1, 3, 4]\npy_list.mean()\n\n'list' object has no attribute 'mean'\n\n\nSometimes the errors in Python are even helpful! Thanks Python!\nComing from R where a function is always just a function, this method vs function thing was quite confusing at first. Don’t be scare. You’ll get used to it. I hope.\n\n\nExtracting columns\nWhile you can create a pandas DataFrame using a dictionary containing list entries (as we did earlier), the columns of a DataFrame themselves, when extracted, are a “Series” type object (rather than a list).\nFor example, the column 'a' can be extracted using []:\n\npandas_df['a']\n\n0    1\n1    2\n2    3\n3    4\nName: a, dtype: int64\n\n\nwhich is the same as in R:\n\nr_df[\"a\"]\n\n  a\n1 1\n2 2\n3 3\n4 4\n\n\nNote that the columns of a DataFrame are also “attributes” of the data frame and can be extracted using the df.col dot syntax:\n\npandas_df.a\n\n0    1\n1    2\n2    3\n3    4\nName: a, dtype: int64\n\n\nThe output above is the same as from the pandas_df['a'] syntax: both outputs are a pandas Series object.\nWe can check the type of an object in python using the type() function:\n\ntype(pandas_df['a'])\n\n&lt;class 'pandas.core.series.Series'&gt;\n\n\nwhich tells us that this column is a pandas “Series” object.\n\n\nSeries\nA pandas Series is like a one-dimensional DataFrame, and you can recognize that your object is a Series because there will be has two attributes printed out at the bottom: the Name (if indeed there is a name) and dtype (type).\n\npandas_df['a']\n\n0    1\n1    2\n2    3\n3    4\nName: a, dtype: int64\n\n\nA series object has a row index but no columns:\n\npandas_df['a'].index\n\nRangeIndex(start=0, stop=4, step=1)\n\n\n\npandas_df['a'].columns\n\n'Series' object has no attribute 'columns'\n\n\n\n\nAdding columns to a DataFrame\nYou can add a column to a pandas DataFrame just as you would in base R:\n\npandas_df['c'] = [9,10,11,12]\npandas_df\n\n   a  b   c\n0  1  5   9\n1  2  6  10\n2  3  7  11\n3  4  8  12\n\n\nHowever, if you define a new DataFrame using an existing data frame and then modify it, e.g., by adding a new column:\n\n# define a new data frame using the original one\npandas_df_new = pandas_df\n# add a new column to the new data frame\npandas_df_new['d'] = [13,14,15,16]\n# print out the new data frame\npandas_df_new\n\n   a  b   c   d\n0  1  5   9  13\n1  2  6  10  14\n2  3  7  11  15\n3  4  8  12  16\n\n\nNotice that the original DataFrame will also change:\n\npandas_df\n\n   a  b   c   d\n0  1  5   9  13\n1  2  6  10  14\n2  3  7  11  15\n3  4  8  12  16\n\n\nThis is very different from what would happen in R, where the pandas_df_new object would be a separate object from the original pandas_df object, and making modifications to the new object would not be reflected in the original one.\n\n\nDon’t forget to .copy()\nThis is because in python when you define a new object to be an existing object (pandas_df_new = pandas_df), you are actually creating a new “pointer” to the same underlying object being pointed to by the original name pandas_df. pandas_df_new becomes like an “alias” for pandas_df, rather than an entirely new object.\nTo avoid this issue, when defining a new DataFrame based on an existing DataFrame, you need to explicitly create a new object using the copy() method:\n\n# redefine the original pandas DataFrame:\npandas_df = pd.DataFrame({'a': [1,2,3,4], \n                          'b': [5,6,7,8]})\npandas_df\n\n   a  b\n0  1  5\n1  2  6\n2  3  7\n3  4  8\n\n\n\n# define pandas_df_new as a \"copy\" of pandas_df:\npandas_df_new = pandas_df.copy()\npandas_df_new['c'] = [9,10,11,12]\npandas_df_new\n\n   a  b   c\n0  1  5   9\n1  2  6  10\n2  3  7  11\n3  4  8  12\n\n\n\n# check that pandas_df did not change this time\npandas_df\n\n   a  b\n0  1  5\n1  2  6\n2  3  7\n3  4  8\n\n\nThis issue of unintentionally creating an alias or a “view” of an object, rather than a new object itself is another confusing thing about python. You will undoubtedly run into issues with it when you’re starting out (I certainly did). When in doubt, just copy().\n\n\nFiltering\nPandas Series objects act like vectors in R in that you can ask logical questions of them (side-note: this does not work with python lists, but it does with with pandas Series objects):\n\n# ask which entries in the column \"a\" are greater than 1\npandas_df['a'] &gt; 1\n\n0    False\n1     True\n2     True\n3     True\nName: a, dtype: bool\n\n\nWhich is very similar to the R version:\n\n# ask which entries in the column \"a\" are greater than 1\nr_df[\"a\"] &gt; 1\n\n         a\n[1,] FALSE\n[2,]  TRUE\n[3,]  TRUE\n[4,]  TRUE\n\n\nIn base R, we can use this to “filter” our data frame to the rows where the condition above is true (the first row):\n\nr_df[r_df[\"a\"] &gt; 1, ]\n\n  a b\n2 2 6\n3 3 7\n4 4 8\n\n\nThis syntax doesn’t work for the pandas DataFrame, however, but if we make a few syntax modifications where we:\n\nemploy the .loc method and\nprovide : to the column dimension (which says to “return all of the columns”),\n\nthen we get the expected result:\n\npandas_df.loc[pandas_df.a &gt; 1,:]\n\n   a  b\n1  2  6\n2  3  7\n3  4  8\n\n\nNote that loc isn’t a normal function per se in that it is not followed by round parentheses (), but it is followed by the square indexing parentheses [].\nIf we wanted to just return the second column, we would provide the second column’s name ('b') into the second dimension of the .loc[,] square parentheses.\n\npandas_df.loc[pandas_df.a &gt; 1,'b']\n\n1    6\n2    7\n3    8\nName: b, dtype: int64\n\n\nHowever, loc expects either a Boolean series or a name in its index. The iloc method, on the other hand, takes integer index positions. The following code will extract the second, third, and fourth rows of the second column (remember zero-indexing!).\n\npandas_df.iloc[[1,2,3],1]\n\n1    6\n2    7\n3    8\nName: b, dtype: int64\n\n\nAgain, the output is a pandas Series (as is almost always the case when the output of your code involves a single column).\nIt’s a bit annoying that you can’t use the same syntax to do both named (.loc) and integer (.iloc) indexing for pandas DataFrames, but such is life.\n\nThe query() method\nSince this type of square bracket syntax for indexing feels clunky both in R and Python, you can alternatively use the query() method of a pandas DataFrame similar to the filter() dplyr function in the tidyverse.\nRecall that with the pipe, the filter() syntax in R looks like:\n\nr_df |&gt; filter(a &gt; 1)\n\n  a b\n1 2 6\n2 3 7\n3 4 8\n\n\nFor pandas DataFrame, the query() syntax looks like\n\npandas_df.query('a &gt; 1')\n\n   a  b\n1  2  6\n2  3  7\n3  4  8\n\n\nNote that there is no “tidy eval” in python, so you need to provide a quoted string (“string” is the python word for “character”) argument to the query() method.\nIf you want to use an external variable, you need to access it using @ within the query argument:\n\nthresh = 1\npandas_df.query('a &gt; @thresh')\n\n   a  b\n1  2  6\n2  3  7\n3  4  8\n\n\n\n\n\nGrouping data frames\nOne of the most powerful parts of the tidyverse is the ability to perform grouped operations on data frames. For example, if we wanted to add a categorical column to our R data frame:\n\n# add a new categorical column to r_df\nr_df[\"cat\"] = c(\"red\", \"red\", \"yellow\", \"yellow\")\nr_df\n\n  a b    cat\n1 1 5    red\n2 2 6    red\n3 3 7 yellow\n4 4 8 yellow\n\n\nThen we could group by this new cat column and compute the mean of the values in column a in R as follows:\n\nr_df |&gt; group_by(cat) |&gt; summarize(mean_a = mean(a))\n\n# A tibble: 2 × 2\n  cat    mean_a\n  &lt;chr&gt;   &lt;dbl&gt;\n1 red       1.5\n2 yellow    3.5\n\n\nWe can do something very similar in Python with pandas DataFrames using the groupby() method. First, let’s add a column of color categories to our pandas DataFrame:\n\n# add a new categorical column to pandas_df\npandas_df[\"cat\"] = [\"red\",\"red\",\"yellow\",\"yellow\"]\npandas_df\n\n   a  b     cat\n0  1  5     red\n1  2  6     red\n2  3  7  yellow\n3  4  8  yellow\n\n\nand then let’s group the data frame by the new categorical column, extract the column “a”, and then apply the mean method to the grouped column:\n\npandas_df.groupby('cat')[\"a\"].mean()\n\ncat\nred       1.5\nyellow    3.5\nName: a, dtype: float64\n\n\nThe output is a Series object with the grouping variable values as the row index.\nNote that if we didn’t extract the column “a”, we would be applying grouped mean to all columns:\n\npandas_df.groupby('cat').mean()\n\n          a    b\ncat             \nred     1.5  5.5\nyellow  3.5  7.5\n\n\nThe output is now a DataFrame with the grouping variable values as the row index."
  },
  {
    "objectID": "blog/2023-09-11-from_r_to_python.html#whats-the-deal-with-methods",
    "href": "blog/2023-09-11-from_r_to_python.html#whats-the-deal-with-methods",
    "title": "A guide to Python for R Users",
    "section": "What’s the deal with methods?",
    "text": "What’s the deal with methods?\nUnlike R where functions exist in your environment and can be used on whichever object you like (though R may complain if the type of input you provide isn’t what is expected), there exist some functions in Python that can only be applied to objects of certain types."
  },
  {
    "objectID": "blog/2023-09-11-from_r_to_python.html#chaining-functions-together",
    "href": "blog/2023-09-11-from_r_to_python.html#chaining-functions-together",
    "title": "An introduction to Python for R Users",
    "section": "Chaining functions together",
    "text": "Chaining functions together\nNote that python methods can be chained together using the . similarly to how we chain methods together in R using the pipe |&gt;.\nFor instance, if we wanted to filter to rows where a was greater than 1, then group by the ‘cat’ column, and then compute the mean of the column b, we would do this in R as follows:\n\nr_df |&gt; filter(a &gt; 1) |&gt; group_by(cat) |&gt; summarize(mean_b = mean(b))\n\n# A tibble: 2 × 2\n  cat    mean_b\n  &lt;chr&gt;   &lt;dbl&gt;\n1 red       6  \n2 yellow    7.5\n\n\nIn python, we do a similar thing, but using . instead of |&gt;:\n\npandas_df.query('a &gt; 1').groupby('cat')['b'].mean()\n\ncat\nred       6.0\nyellow    7.5\nName: b, dtype: float64\n\n\nNote that the code up to the mean() part results in a “grouped Series”. Unlike in R, many objects in python don’t always display nicely when called:\n\npandas_df.query('a &gt; 1').groupby('cat')['b']\n\n&lt;pandas.core.groupby.generic.SeriesGroupBy object at 0x120ac8940&gt;\n\n\nWe can use the pd.Series() function to force the grouped Series object back into a regular ungrouped Series (pd.Series() is used to define Series objects on the fly just as pd.DataFrame() is used to define DataFrames on the fly – this is what I used at the beginning of this tutorial).\nLet’s try to force the grouped Series object back into a regular pandas Series object:\n\npd.Series(pandas_df.query('a &gt; 1').groupby('cat')['b'])\n\n0          (red, [6])\n1    (yellow, [7, 8])\ndtype: object\n\n\nThis Series contains tuples (I can tell from the round parentheses ()), and I’m not going to lie, I’m afraid. But at least I can see what values the grouped Series contains."
  },
  {
    "objectID": "blog/2023-09-11-from_r_to_python.html#data-visualization",
    "href": "blog/2023-09-11-from_r_to_python.html#data-visualization",
    "title": "An introduction to Python for R Users",
    "section": "Data visualization",
    "text": "Data visualization\nUnlike in R, where ggplot is the clear “best” way to create data visualizations (at least in my opinion), in python there are many different libraries for doing data visualization. The two you are most likely to have heard of include matplotlib and seaborn.\nSeaborn is a little prettier than matplotlib, but all I’m going to show in this tutorial is the inbuilt data visualization methods for pandas Series and DataFrame objects that are built on matplotlib.\nTo do some more interesting visualizations, let’s load the gapminder dataset (from a URL) into a pandas DataFrame using the pd.read_csv() pandas function. This pd.read_csv() function can also be used to load a local .csv file.\n\ngapminder = pd.read_csv(\"https://raw.githubusercontent.com/rlbarter/gapminder-data/main/gapminder.csv\")\n\nLet’s use the head() DataFrame method to look at the first 6 rows:\n\ngapminder.head()\n\n       country continent  year  lifeExp       pop   gdpPercap\n0  Afghanistan      Asia  1952   28.801   8425333  779.445314\n1  Afghanistan      Asia  1957   30.332   9240934  820.853030\n2  Afghanistan      Asia  1962   31.997  10267083  853.100710\n3  Afghanistan      Asia  1967   34.020  11537966  836.197138\n4  Afghanistan      Asia  1972   36.088  13079460  739.981106\n\n\nLet’s practice some of our new python skills. The following code will filter to the data for Australia and compute the mean life expectancy:\n\ngapminder.query('country == \"Australia\"')['lifeExp'].mean()\n\n74.66291666666667\n\n\nAnd the following will compute the average gdpPercap by continent:\n\ngdp_by_continent = gapminder.groupby('continent')['gdpPercap'].mean()\ngdp_by_continent\n\ncontinent\nAfrica       2193.754578\nAmericas     7136.110356\nAsia         7902.150428\nEurope      14469.475533\nOceania     18621.609223\nName: gdpPercap, dtype: float64\n\n\nNotice that the output of this last piece of code is a Series object. Notice also that the (row) index is the continent:\n\ngdp_by_continent.index\n\nIndex(['Africa', 'Americas', 'Asia', 'Europe', 'Oceania'], dtype='object', name='continent')\n\n\nWhat will happen if we apply the plot() method to this Series object?\n\ngdp_by_continent.plot()\n\n\n\n\n\n\n\n\nBy default a line plot is created and the row index is used as the x-axis labels by default. The DataFrame’s plot() method here acts as a wrapper for the matplotlib library’s plotting function.\nIf I wanted a bar chart instead of a line plot, I could create one using the plot.bar() method (instead of just the plot() method):\n\ngdp_by_continent.plot.bar()\n\n\n\n\n\n\n\n\nAgain, the row index of the Series object is used as the x-axis labels by default.\nWe could similarly create a scatterplot of the gdpPercap and lifeExp columns from gapminder using the plot.scatter() method:\n\ngapminder.plot.scatter(x='gdpPercap', y='lifeExp')\n\n\n\n\n\n\n\n\nWhile these built-in pandas plots are fine, I personally prefer the look of the seaborn library and the workflow of the plotly express library, neither of which I’ve covered in this post (and neither of which are as good as ggplot2 in my opinion). I recommend you look into both of these libraries (and more!) to figure out the data visualization workflow you prefer!"
  },
  {
    "objectID": "blog/2023-09-11-from_r_to_python.html#writing-functions",
    "href": "blog/2023-09-11-from_r_to_python.html#writing-functions",
    "title": "A guide to Python for R Users",
    "section": "Writing functions",
    "text": "Writing functions"
  },
  {
    "objectID": "blog/2023-09-11-from_r_to_python.html#wrapping-up",
    "href": "blog/2023-09-11-from_r_to_python.html#wrapping-up",
    "title": "An introduction to Python for R Users",
    "section": "Wrapping up",
    "text": "Wrapping up\nObviously, there’s a lot more to learn about Python, but I’ll leave it here for now. I hope this post has helped some of you R users get started on your Python journey!"
  },
  {
    "objectID": "blog/2024-04-11-labeling_doctors_notes.html",
    "href": "blog/2024-04-11-labeling_doctors_notes.html",
    "title": "AI Tutorial: Using Text Embeddings to Label Synthetic Doctor’s Notes Generated with ChatGPT",
    "section": "",
    "text": "Hello friends, welcome to the new rapidly evolving world of AI. Even the wacky image that you see at the top of this blog was created using AI (specifically, using dall-e-3), just for this blog post!\nLike me, I’m sure you’re dwelling on the question of whether AI is going to take all of our jobs. I think, for now, I’ve settled on the opinion that for those of us who adapt and learn to work with AI, our jobs are pretty safe. However, there is no question that AI is going to fundamentally change our jobs, hopefully for the better. In my day-to-day I have already found that GitHub Copilot has boosted my coding efficiency, chatGPT has boosted my efficiency for mundane writing tasks, and the various other ways that AI has snuck into my workday are probably helping me out in other ways too.\nAnd, as you’ll see in this blog post, using the tools provided by OpenAI, NLP tasks like generating synthetic text, and labeling mass amounts of text data are now fairly straightforward too.\nIn this blog post, I will walk through an example of generating a small collection of synthetic doctor’s notes using chatGPT and then using text OpenAI’s embedding models to automatically label these doctor’s notes in terms of whether they correspond to a patient experiencing a chronic or an acute event. Fortunately, the documentation provided on the OpenAI website is very good, and that will certainly be the best point of reference.\nNote that, for now, if you want to use these tools effectively, you’ll want to be fairly proficient in Python (if you’re an R user who is interested in learning Python, check out my “An introduction to Python for R Users” blog post)."
  },
  {
    "objectID": "blog/2024-04-11-labeling_doctors_notes.html#getting-set-up",
    "href": "blog/2024-04-11-labeling_doctors_notes.html#getting-set-up",
    "title": "AI Tutorial: Using Text Embeddings to Label Synthetic Doctor’s Notes Generated with ChatGPT",
    "section": "Getting set up",
    "text": "Getting set up\nAs usual, the first thing you’ll want to do is to import the libraries you’ll need.\n\nimport pandas as pd\nfrom scipy.spatial.distance import cosine\nfrom openai import OpenAI\n\npd.set_option('display.max_colwidth', None)\n\nThen you’ll need to set up your OpenAI API key. An API key is literally a jumble of letters that OpenAI uses to identify you so that they can track your usage. After you create an OpenAI account, see the OpenAI API keys website for information on creating an API key.\nOnce you have created an API key, and you have saved it somewhere on your computer, say in a file called “api_key.txt”, you will need to use it to connect to the OpenAI client.\nTo do this, either create a local variable called your_key or read in your key from your text file using code like the following:\n\nwith open('api_key.txt', 'r') as file:\n  your_key = file.read().strip()\n\nThen you can set up your OpenAI client which is how you will connect to the OpenAI API:\n\n# Define a local variable called `your_key` that contains your OpenAI key.\n# Try really hard to avoid writing your key in your notebook\n# And when you do, try even harder to avoid uploading it to GitHub\nclient = OpenAI(api_key=your_key)\n\nNote that it is really, really important that you do not write your API key in plain text in your notebook file, especially if you are going to upload your notebook publicly to GitHub. This would allow someone else to pretend to be you and use your OpenAI account.\nInstead, you will want to only ever define your key in a local variable (that is not saved in your code) or load it in from a file that should not be uploaded to GitHub!\nOnce you’re set up, let’s generate some fake doctor’s notes to analyze!"
  },
  {
    "objectID": "blog/2024-04-11-labeling_doctors_notes.html#creating-synthetic-doctors-notes-with-chatgpt",
    "href": "blog/2024-04-11-labeling_doctors_notes.html#creating-synthetic-doctors-notes-with-chatgpt",
    "title": "AI Tutorial: Using Text Embeddings to Label Synthetic Doctor’s Notes Generated with ChatGPT",
    "section": "Creating synthetic doctor’s notes with chatGPT",
    "text": "Creating synthetic doctor’s notes with chatGPT\nThe first thing I did was use chatGPT-4 to create a collection of synthetic doctor’s notes.\nNote that at the time of writing, you have to pay to access chatGPT-4 through the API. If you don’t want to pay OpenAI, you can replace model=\"gpt-4\" with model=\"gpt-3.5-turbo\", which is currently free (but is also less good).\nThe following code uses chatGPT-4 to generate 50 doctor’s notes using the prompt:\n\n“Provide a collection of 50 doctors notes that resemble the kind that you would enter into an EHR for your patients on a day-to-day basis. Each note should have around 3 sentences. Each note should appear on a new line. Do not add any numbers or superfluous text.”\n\n\ncompletion = client.chat.completions.create(\n  model=\"gpt-4\",\n  messages=[\n    {\"role\": \"system\", \n     \"content\": \"You are a doctor working in a large hospital.\"},\n    {\"role\": \"user\",\n     \"content\": \"\"\"Provide a collection of 50 doctors notes that \n                   resemble the kind that you would enter into an \n                   EHR for your patients on a day-to-day basis. \n                   Each note should have around 3 sentences. \n                   Each note should appear on a new line.\n                   Do not add any numbers or superfluous text.\"\"\"}\n  ]\n)\n\nThe text created by ChatGPT can be extracted using completion.choices[0].message.content. But since this is just one big string value, I want to use the .split method to separate each note into a list element.\n\n# extract the notes and place them in a list, where each list element is detected by the presence of a line break ('\\n')\nnotes_list = completion.choices[0].message.content.split('\\n')\n\nSince the output provided tended to add additional blank lines between the notes (even when I asked it not to), I used the following code to remove any entries in my notes_list that contain blank strings:\n\n# remove any empty notes\n# may or may not be required for you\nnotes_list = [note for note in notes_list if note != '']\n# place the notes in a DataFrame\nnotes_df = pd.DataFrame({'notes': notes_list}) \n\nLet’s take a look at the notes it created:\n\n# look at the first 5\nnotes_df.head(5)\n\n\n\n\n\n\n\n\n\nnotes\n\n\n\n\n0\nPatient presented signs of common cold; symptoms include a runny nose and mild fever. Advised rest, hydration, and over-the-counter medication to manage symptoms.\n\n\n1\nPatient reports difficulty sleeping. Exhibiting signs of insomnia characterized by frequent night time awakenings. Recommended a sleep study to find underlying cause.\n\n\n2\nPatient came in with complaints of stomachache. Upon examination, no obvious signs of appendicitis or gallstones were found. Recommended patient to undergo an ultrasound to assess internal organs.\n\n\n3\nHypertensive patient presented with higher blood pressure at 160/100. Adjusted medications and emphasized importance of diet and exercise.\n\n\n4\nExamined patient for frequent headaches. No alarming neurological signs detected. Recommended reducing caffeine intake and keeping hydrated.\n\n\n\n\n\n\n\n\nHowever, since chatGPT returns a different collection of notes every time, you can load the notes I created above using the following code (if you prefer to follow along with my doctor’s notes, that is):\n\nnotes_df = pd.read_csv(\"https://raw.githubusercontent.com/rlbarter/personal-website-quarto/main/blog/data/doctors_notes.csv\", index_col=0)\n\nIf you just loaded the notes using the URL above, take a look at the first 5 notes.\n\n# look at the first 5\nnotes_df.head(5)\n\n\n\n\n\n\n\n\n\nnotes\n\n\n\n\n0\nPatient presented signs of common cold; symptoms include a runny nose and mild fever. Advised rest, hydration, and over-the-counter medication to manage symptoms.\n\n\n1\nPatient reports difficulty sleeping. Exhibiting signs of insomnia characterized by frequent night time awakenings. Recommended a sleep study to find underlying cause.\n\n\n2\nPatient came in with complaints of stomachache. Upon examination, no obvious signs of appendicitis or gallstones were found. Recommended patient to undergo an ultrasound to assess internal organs.\n\n\n3\nHypertensive patient presented with higher blood pressure at 160/100. Adjusted medications and emphasized importance of diet and exercise.\n\n\n4\nExamined patient for frequent headaches. No alarming neurological signs detected. Recommended reducing caffeine intake and keeping hydrated."
  },
  {
    "objectID": "blog/2024-04-11-labeling_doctors_notes.html#computing-text-embeddings",
    "href": "blog/2024-04-11-labeling_doctors_notes.html#computing-text-embeddings",
    "title": "AI Tutorial: Using Text Embeddings to Label Synthetic Doctor’s Notes Generated with ChatGPT",
    "section": "Computing text embeddings",
    "text": "Computing text embeddings\nMy goal in the rest of this post will be to identify whether each note corresponds to a patient with a chronic condition (which I define as a condition that they have been experiencing for more than one month) or an acute condition (e.g., a new condition).\nTo do this, I will use something called text embeddings. The idea is that I will use OpenAI’s pre-trained text embedding models to embed each note into a numeric 1,536-dimensional that somehow approximately respects semantic distance.\nThe equivalent of this in a two-dimensional world would be taking each individual doctor’s note and placing it on a scatterplot so that notes that we might intuitively consider to be more “similar” to one another are closer together in the plot, and notes that we might consider to be more “different” from one another are further apart. Unfortunately, it is unlikely that we would be able to come up with any two quantifiable values that we could use to define the two axes of our scatterplot, such that when we place each note in the scatterplot we achieve our desired property that “similar” notes are closer together. But it turns out that once your space has, say, 1,536 dimensions, this task somehow becomes possible (although what each dimension/axis represents is not necessarily going to be meaningful to us).\nFortunately, OpenAI has already trained some general text embedding models that we can use to embed each of the doctor’s notes in such a 1,536-dimensional space. In this post, I will be using the “text-embedding-3-small” OpenAI model to compute the text embeddings.\nAfter computing the embeddings, our task becomes determining how close each doctor’s note is to the embedding of the following text: “Patient presenting with ongoing chronic condition defined as ongoing for more than one month”, which I will call the target. The closer a note’s embedding is to the target’s embedding, the more likely the note corresponds to a patient who is presenting with a chronic condition!\nI hear you. That sounds crazy, right? But it works… eerily well! Let me show you.\nFirst, I need to compute the embeddings for each of the chatGPT-generated doctor’s notes. I can do that using this custom function get_embedding() function that I literally just copy-and-pasted from the OpenAI documentation:\n\ndef get_embedding(text, model=\"text-embedding-3-small\"):\n   text = text.replace(\"\\n\", \" \")\n   return client.embeddings.create(input = [text], model=model).data[0].embedding\n\nThis function basically takes a string text, and uses the “text-embedding-3-small” model to create a list of 1,536 numbers corresponding to the 1,536-dimensional embedding of the text entry.\nFor example, the following code creates an embedding of the text “hello, how are you”:\n\nget_embedding('hello, how are you')\n\n[0.020681990310549736,\n -0.03974681720137596,\n -0.000452458014478907,\n 0.029285797849297523,\n -0.013973292894661427,\n -0.059632860124111176,\n 0.0007248803740367293,\n 0.02625361829996109,\n 0.0012578805908560753,\n -0.039999499917030334,\n 0.004143978469073772,\n -0.032242171466350555,\n -0.011370671913027763,\n -0.04505313187837601,\n -0.005956968758255243,\n 0.06473702937364578,\n -0.02131369337439537,\n 0.05190080404281616,\n -0.0056916531175374985,\n 0.02920999377965927,\n 0.058015696704387665,\n 0.01499665342271328,\n 0.009684022516012192,\n 0.04065646976232529,\n 0.0349964015185833,\n 0.025268161669373512,\n 0.023827875033020973,\n 0.01146542839705944,\n 0.02449748106300831,\n -0.017624542117118835,\n 0.05356850102543831,\n -0.03800331428647041,\n 0.012665665708482265,\n 0.013291052542626858,\n 0.002278872299939394,\n 0.013834318146109581,\n -0.0029911184683442116,\n 0.009267098270356655,\n -0.001953544793650508,\n -0.060896266251802444,\n -0.006733964662998915,\n -0.01929224096238613,\n 0.0412629060447216,\n 0.016411669552326202,\n 0.01106745470315218,\n 0.0036165055353194475,\n 0.003145886119455099,\n -0.018572097644209862,\n 0.04836326092481613,\n 0.020088188350200653,\n 0.02371416985988617,\n -0.01297520101070404,\n 0.014529192820191383,\n 0.13715557754039764,\n 0.04500259459018707,\n -0.04282953217625618,\n 0.036563027650117874,\n 0.00012782136036548764,\n 0.005268411710858345,\n 0.03388460353016853,\n 0.002237811451777816,\n -0.04341069981455803,\n 0.01293729804456234,\n 0.027264345437288284,\n -0.0006838195840828121,\n -0.030422866344451904,\n -0.013758513145148754,\n 0.012728836387395859,\n -0.016828594729304314,\n 3.400344212423079e-05,\n 0.02913418971002102,\n 0.04070700705051422,\n -0.008610125631093979,\n -0.018572097644209862,\n -0.03648722544312477,\n 0.016007380560040474,\n -0.0004595646751113236,\n -0.032823339104652405,\n 0.01567889377474785,\n 0.02673371322453022,\n -0.0628671869635582,\n -0.023524656891822815,\n -0.03421308845281601,\n -0.006399161648005247,\n -0.032444316893815994,\n 0.010827407240867615,\n -0.014794507995247841,\n 0.01747293397784233,\n 0.005975920241326094,\n 0.004292428959161043,\n -0.02663264237344265,\n 0.012634080834686756,\n -0.02045457623898983,\n 0.005565312225371599,\n 0.022488662973046303,\n -0.04763048142194748,\n 4.67115496576298e-05,\n 0.02693585865199566,\n 0.036360882222652435,\n -0.00024103456235025078,\n 0.05119329318404198,\n -0.02443431131541729,\n -0.0230319295078516,\n -0.002187275094911456,\n 0.05336635559797287,\n 0.03901404142379761,\n 0.021717984229326248,\n -0.020290333777666092,\n -0.014124901965260506,\n 0.011219063773751259,\n -0.10142640024423599,\n -0.002395737450569868,\n -0.012570910155773163,\n 0.006550770718604326,\n 0.04090915247797966,\n 0.03982262313365936,\n 0.05958232283592224,\n -0.04581117630004883,\n 0.025470305234193802,\n -0.02701166458427906,\n 0.007056133821606636,\n -0.015982111915946007,\n 0.03939306363463402,\n -0.04042905569076538,\n -0.061755385249853134,\n -0.010941113345324993,\n -0.014491289854049683,\n 0.02185695990920067,\n -0.03727053850889206,\n -0.006525502540171146,\n -0.008875441737473011,\n 0.023082464933395386,\n 0.009854583069682121,\n 0.0023704692721366882,\n -0.028376145288348198,\n 0.013152077794075012,\n -0.020745160058140755,\n 0.03964574262499809,\n -0.045659568160772324,\n 0.014327047392725945,\n 0.061654310673475266,\n -0.04992988705635071,\n -0.013632172718644142,\n 0.01441548578441143,\n 0.005697970278561115,\n -0.06215967610478401,\n -0.0039228820241987705,\n 0.04992988705635071,\n 0.010953747667372227,\n -0.014251242391765118,\n 0.03524908423423767,\n 0.012861493974924088,\n -0.0747937560081482,\n 0.023309879004955292,\n 0.002479438204318285,\n 0.008066860027611256,\n 0.030448133125901222,\n -0.021086279302835464,\n -0.023486755788326263,\n 0.03287387639284134,\n 0.013000468723475933,\n 0.0094060730189085,\n -0.03125671669840813,\n -0.04914657399058342,\n -0.029285797849297523,\n 0.00018635268497746438,\n -0.021490570157766342,\n 0.04257684946060181,\n 0.0014047517906874418,\n 0.017422396689653397,\n -0.03153466433286667,\n 0.03924145549535751,\n -0.04664502665400505,\n -0.02923526242375374,\n 0.0100061921402812,\n -0.02749175950884819,\n 0.028805702924728394,\n -0.01360690500587225,\n -0.05020783469080925,\n 0.003398567670956254,\n 0.023865777999162674,\n 0.02221071347594261,\n 0.019608093425631523,\n -0.015249335207045078,\n -0.013291052542626858,\n 0.016310598701238632,\n -0.025167088955640793,\n 0.010656846687197685,\n -0.028199266642332077,\n 0.017460299655795097,\n 0.010770553722977638,\n 0.0555899553000927,\n 0.004349282011389732,\n 0.018799511715769768,\n 0.025697719305753708,\n -0.04399186745285988,\n 0.07878612726926804,\n -0.0081489821895957,\n -0.04550795629620552,\n 0.023385683074593544,\n -0.008824905380606651,\n -0.008212151937186718,\n 0.00013364487676881254,\n -0.02683478593826294,\n 0.0020340869668871164,\n 0.007744691334664822,\n -0.02515445463359356,\n 0.06332200765609741,\n 0.008654344826936722,\n -0.0013494776794686913,\n 0.04816111549735069,\n -0.020568283274769783,\n 0.003951308783143759,\n 0.04667029157280922,\n -0.009046001359820366,\n -0.03171154111623764,\n 0.04404240474104881,\n 0.03380879759788513,\n 0.00862907711416483,\n 0.004924132954329252,\n -0.06377683579921722,\n 0.04295587167143822,\n 0.08070650696754456,\n -0.02221071347594261,\n 0.026278886944055557,\n 0.012071863748133183,\n 0.027087468653917313,\n 0.005540044046938419,\n 0.013316321186721325,\n -0.003521749982610345,\n 0.007605716586112976,\n 0.028780434280633926,\n 0.025621915236115456,\n -0.021440034732222557,\n -0.03997423127293587,\n 0.011484378948807716,\n 0.015779966488480568,\n -0.04282953217625618,\n 0.019140630960464478,\n -0.00490834005177021,\n -0.021819056943058968,\n -0.026657909154891968,\n -0.02341095171868801,\n 0.017510835081338882,\n 0.03393514081835747,\n -0.005634800065308809,\n 0.021149450913071632,\n -0.0350722074508667,\n -0.00835744384676218,\n 0.011901304125785828,\n -0.003910247702151537,\n 0.01939331367611885,\n 0.014781873673200607,\n -0.041338711977005005,\n -0.003625981044024229,\n -0.002013556659221649,\n -0.008231103420257568,\n 0.004658817313611507,\n 0.00835744384676218,\n -0.02357519418001175,\n -0.028881506994366646,\n -0.037523217499256134,\n -0.008104762993752956,\n -0.03054920583963394,\n -0.021149450913071632,\n -0.045836444944143295,\n -0.015552553348243237,\n 0.01571679674088955,\n 0.012002376839518547,\n 0.02059355191886425,\n 0.04214729368686676,\n -0.0374726839363575,\n 0.03459211066365242,\n 0.01058735977858305,\n -0.022981392219662666,\n 0.02759283222258091,\n 0.02728961408138275,\n 0.008679613471031189,\n 0.03075135126709938,\n -0.008603808470070362,\n -0.0325959287583828,\n -0.009791412390768528,\n 0.003357506822794676,\n -0.02835087664425373,\n -0.043056946247816086,\n 0.030877692624926567,\n -0.024901771917939186,\n -0.02213490940630436,\n 0.035678643733263016,\n 0.019191168248653412,\n 0.01446602214127779,\n -0.00023274344857782125,\n -0.002964271232485771,\n -0.028805702924728394,\n 0.0007979211513884366,\n 0.01111799106001854,\n 0.036739904433488846,\n -0.049980420619249344,\n 0.01627269573509693,\n -0.012949932366609573,\n 0.052456703037023544,\n 0.004175563808530569,\n 0.013240516185760498,\n -0.01747293397784233,\n -0.0157546978443861,\n 0.04368865117430687,\n 0.021402131766080856,\n -0.0054516056552529335,\n 0.0010115160839632154,\n 0.0004481150535866618,\n -0.03982262313365936,\n 0.03297495096921921,\n 0.05766194313764572,\n -0.02741595357656479,\n -0.014604996889829636,\n -0.010833724401891232,\n 0.02855302207171917,\n 0.0026926384307444096,\n 0.004750414285808802,\n -0.034263625741004944,\n 0.020201895385980606,\n -0.0313577875494957,\n -0.016866497695446014,\n -0.018812146037817,\n -0.032444316893815994,\n -0.014289145357906818,\n 0.024825967848300934,\n -0.003780748462304473,\n -0.04176827147603035,\n -0.021490570157766342,\n -0.0008843856048770249,\n -0.06908315420150757,\n 0.006279137916862965,\n -0.005394752137362957,\n 0.04414347559213638,\n 0.008717515505850315,\n -0.03236851468682289,\n 0.033303435891866684,\n 0.002240970032289624,\n 0.014920849353075027,\n -0.015186164528131485,\n -0.0399489626288414,\n -0.019936578348279,\n -0.051218561828136444,\n -0.003907089587301016,\n -0.019090095534920692,\n 0.01955755613744259,\n 0.002414688700810075,\n -0.031281981617212296,\n -0.021920129656791687,\n 0.018989022821187973,\n 0.01154754962772131,\n -0.0017545579466968775,\n -0.027643367648124695,\n -0.012659348547458649,\n -0.0206314530223608,\n 0.07332820445299149,\n -0.0028174000326544046,\n -0.013253150507807732,\n 0.025280794128775597,\n 0.0007900248165242374,\n -0.008420614525675774,\n 2.5046078008017503e-05,\n -0.024067923426628113,\n 0.04245050996541977,\n 0.022804515436291695,\n -0.06023929640650749,\n -0.008572223596274853,\n 0.018445758149027824,\n 0.049197107553482056,\n 0.025204990059137344,\n 0.0005176024860702455,\n 0.0021714826580137014,\n 0.03476898744702339,\n -0.06498970836400986,\n 0.02979116141796112,\n 0.037220001220703125,\n 0.004434562288224697,\n 0.0020909402519464493,\n 0.011724426411092281,\n 0.0931384414434433,\n 0.03727053850889206,\n -0.013948025181889534,\n -0.025722987949848175,\n 0.027062200009822845,\n 0.00029196569812484086,\n -0.009791412390768528,\n -0.006696062628179789,\n -0.03974681720137596,\n 0.033758264034986496,\n 0.08611389249563217,\n -0.012867811135947704,\n 0.015514650382101536,\n -0.037801168859004974,\n 0.0338340662419796,\n 0.007668886799365282,\n 0.028022389858961105,\n 0.0259756688028574,\n -0.04942452162504196,\n -0.04308221489191055,\n 0.03171154111623764,\n -0.055135127156972885,\n 0.009551364928483963,\n -0.004949401132762432,\n 0.014807142317295074,\n 0.035880789160728455,\n -0.03792750835418701,\n -0.011105356737971306,\n 0.04760521650314331,\n 0.012166619300842285,\n -0.0026942177210003138,\n -0.05745979771018028,\n 0.03643668815493584,\n 0.060694120824337006,\n -0.010220970958471298,\n -0.018281513825058937,\n -0.027820244431495667,\n -0.02971535734832287,\n -0.0034522623755037785,\n -0.0050504738464951515,\n 0.025167088955640793,\n 0.012318228371441364,\n 0.026607373729348183,\n 0.02683478593826294,\n -0.031004033982753754,\n 0.019026925787329674,\n 0.025760889053344727,\n -0.04502786323428154,\n 0.03600712865591049,\n -0.06888100504875183,\n -0.03355611860752106,\n 0.0016124245012179017,\n 0.009835631586611271,\n 0.014327047392725945,\n 0.009785095229744911,\n -0.017864590510725975,\n 0.02855302207171917,\n 0.015640990808606148,\n 0.013569002039730549,\n 0.003704944159835577,\n 0.029563747346401215,\n -0.027087468653917313,\n -0.037801168859004974,\n -0.048060040920972824,\n -0.009349219501018524,\n -0.01725815422832966,\n 0.004706195089966059,\n 0.020151358097791672,\n -0.01653801091015339,\n -0.012810957618057728,\n -0.039797354489564896,\n 0.03161047026515007,\n 0.013303686864674091,\n 0.018243612721562386,\n 0.00651918537914753,\n 0.01388485450297594,\n -0.027138004079461098,\n -0.006377052050083876,\n 0.017662445083260536,\n -0.005874847527593374,\n -0.0029579540714621544,\n -0.00447878148406744,\n -0.034920599311590195,\n 0.06140163168311119,\n 0.00862907711416483,\n -0.008003690280020237,\n -0.012539324350655079,\n -0.004687243606895208,\n -0.005878006108105183,\n -0.011730743572115898,\n 0.003411201760172844,\n 0.003979735542088747,\n 0.044320352375507355,\n 0.013960658572614193,\n -0.03719473257660866,\n -0.03140832483768463,\n -0.027466490864753723,\n 0.020543014630675316,\n 0.013240516185760498,\n -0.02327197603881359,\n -0.013695343397557735,\n 0.02025243081152439,\n 0.0024004753213375807,\n 0.04816111549735069,\n -0.01360690500587225,\n 0.019519655033946037,\n -0.002253604121506214,\n 0.014832410030066967,\n -0.06049197539687157,\n 0.03178734704852104,\n 0.018129905685782433,\n 0.045634299516677856,\n 0.04022691026329994,\n -0.013695343397557735,\n -0.03676517307758331,\n 0.05013203248381615,\n -0.03724526986479759,\n -0.00802895799279213,\n -0.007321449462324381,\n -0.019759701564908028,\n -0.00988616794347763,\n -0.024585921317338943,\n -0.000546818773727864,\n 0.002975326031446457,\n 0.07534965872764587,\n 0.019987115636467934,\n -0.007908934727311134,\n -0.02797185443341732,\n 0.004289270378649235,\n -0.04237470403313637,\n 0.005915908142924309,\n 0.006209650542587042,\n -0.02289295382797718,\n -0.016222158446907997,\n -0.007005597464740276,\n 0.02595040202140808,\n 0.01771298050880432,\n 0.03219163790345192,\n -0.010239922441542149,\n -0.050814270973205566,\n 0.025647183880209923,\n 0.010378897190093994,\n 0.018824780359864235,\n -0.013392125256359577,\n -0.02563454955816269,\n 0.006449698004871607,\n -0.05948125198483467,\n -0.027921317145228386,\n -0.01785195618867874,\n 0.018167806789278984,\n 0.014693435281515121,\n -0.03727053850889206,\n -0.02776970900595188,\n 0.012596177868545055,\n -0.00855958927422762,\n -0.013569002039730549,\n 0.055893171578645706,\n -0.02739068679511547,\n 0.006822403520345688,\n -0.06468649208545685,\n 0.014314413070678711,\n 0.04141451418399811,\n -0.003040075534954667,\n -0.014377583749592304,\n -0.007902617566287518,\n 0.01255827583372593,\n 0.01733395829796791,\n -0.013720611110329628,\n 0.014554460532963276,\n 0.014440753497183323,\n 0.018951119855046272,\n -0.0007438315078616142,\n -0.015097726136446,\n 0.004245051182806492,\n 0.004570378456264734,\n 0.006569721736013889,\n -0.02741595357656479,\n -0.03168627247214317,\n 0.023688901215791702,\n 0.03547649830579758,\n -0.03229270875453949,\n -0.014743971638381481,\n -0.005846420768648386,\n 0.0059853955172002316,\n 0.006525502540171146,\n 0.04712511971592903,\n -0.0026926384307444096,\n -0.0008496419177390635,\n -0.004990461748093367,\n -0.011692841537296772,\n -0.04543215408921242,\n 0.0009396597160957754,\n -0.013177345506846905,\n -0.08060543239116669,\n 0.03340451046824455,\n 0.007706788834184408,\n 0.00399868655949831,\n 0.021187352016568184,\n -0.02203383669257164,\n 0.016121085733175278,\n -0.012463520281016827,\n -0.03840760514140129,\n -0.022147543728351593,\n 0.0014797666808590293,\n -0.03643668815493584,\n 0.030448133125901222,\n 0.0022630796302109957,\n 0.014655533246695995,\n 0.028856240212917328,\n 0.05250723659992218,\n 0.0012602495262399316,\n -0.0028237169608473778,\n 0.029058385640382767,\n -0.02749175950884819,\n 0.015640990808606148,\n -0.04328436031937599,\n -0.032722268253564835,\n -0.02883097156882286,\n -0.006446539424359798,\n 0.011162210255861282,\n -0.023082464933395386,\n -0.0059032742865383625,\n -0.0058053601533174515,\n -0.0521029494702816,\n 0.02055564895272255,\n -0.003995527978986502,\n -0.019759701564908028,\n 0.009317634627223015,\n -0.024585921317338943,\n -0.049879349768161774,\n -0.021048378199338913,\n -0.003600712865591049,\n 0.01647484116256237,\n -0.04368865117430687,\n 0.0026452606543898582,\n 0.019671263173222542,\n -0.019683897495269775,\n 0.013227881863713264,\n -0.0014758185716345906,\n -0.0006309144082479179,\n -0.042399972677230835,\n 0.009159708395600319,\n 0.016424303874373436,\n 0.021528473123908043,\n 0.03423835709691048,\n 0.01139594055712223,\n -0.018685804679989815,\n -0.019279606640338898,\n -0.03843287378549576,\n 0.016904398798942566,\n 0.01715708151459694,\n 0.01257722731679678,\n 0.00628861365839839,\n -0.01911536417901516,\n -0.008332176133990288,\n -0.012387716211378574,\n 0.0033827750012278557,\n -0.02645576372742653,\n 0.04495205730199814,\n -0.03269699960947037,\n 0.024863870814442635,\n 0.0015310925664380193,\n 0.019443849101662636,\n -0.023448852822184563,\n 0.029816430062055588,\n 0.006396003067493439,\n -0.013922756537795067,\n 0.0100061921402812,\n -0.016386402770876884,\n -0.01237508188933134,\n 0.031105106696486473,\n -0.026379959657788277,\n -0.020025016739964485,\n -0.028502484783530235,\n 0.004845169838517904,\n 0.021098913624882698,\n -0.042197827249765396,\n 0.019848139956593513,\n -0.0049367668107151985,\n 0.0067402818240225315,\n 0.014743971638381481,\n 0.03734634071588516,\n 0.013126809149980545,\n 0.007195108570158482,\n -0.005631641484797001,\n 0.019709166139364243,\n 0.028325608000159264,\n -0.012962566688656807,\n -0.006165431346744299,\n -0.01204659603536129,\n -0.013758513145148754,\n -0.004668292589485645,\n 0.05265884846448898,\n -0.037017855793237686,\n 0.036259811371564865,\n -0.014301778748631477,\n -0.014390217140316963,\n -0.0026373642031103373,\n -0.0060296147130429745,\n -0.023549925535917282,\n -0.03661356493830681,\n 0.018205709755420685,\n -0.034162554889917374,\n -0.002649998292326927,\n 0.0009459767607040703,\n 0.038938235491514206,\n 0.06498970836400986,\n -0.016727522015571594,\n -0.02779497765004635,\n 0.0223370548337698,\n -0.018951119855046272,\n -0.0023657316341996193,\n 0.0200123842805624,\n 0.020429307594895363,\n 0.029487943276762962,\n 0.01567889377474785,\n 0.023284610360860825,\n 0.030827155336737633,\n 0.03724526986479759,\n -0.01245720311999321,\n -0.014339681714773178,\n -0.05020783469080925,\n 0.009159708395600319,\n -0.003430152777582407,\n 0.003888138337060809,\n -0.0002218860317952931,\n -0.028376145288348198,\n 0.017346592620015144,\n 0.023966850712895393,\n 0.05291152745485306,\n -0.017763517796993256,\n 0.015236700884997845,\n -0.0033038121182471514,\n -0.005919066723436117,\n -0.004374550189822912,\n 0.019001657143235207,\n -0.0017719297902658582,\n -0.016790693625807762,\n 0.005116802640259266,\n -0.007984738796949387,\n -0.011503330431878567,\n -0.0035185914020985365,\n -0.03388460353016853,\n -0.026986395940184593,\n 0.005028363782912493,\n -0.02523025870323181,\n -0.03229270875453949,\n -0.021124182268977165,\n 0.013430027291178703,\n -0.013164712116122246,\n 0.010707383044064045,\n -0.0272138100117445,\n 0.01049892045557499,\n 0.02409319207072258,\n 0.020163992419838905,\n 0.012640397064387798,\n -0.009551364928483963,\n -0.00644338084384799,\n -0.03633561357855797,\n -0.029260531067848206,\n -0.008142665028572083,\n 0.015742063522338867,\n 0.020366137847304344,\n 0.0010249398183077574,\n 0.010606310330331326,\n 0.011225380003452301,\n 0.007624667603522539,\n 0.009747193194925785,\n -0.03181261569261551,\n -0.012804640457034111,\n 0.004769365303218365,\n 0.00830690748989582,\n -0.020947305485606194,\n 0.022842416539788246,\n -0.011743377894163132,\n -0.04894442856311798,\n 0.01647484116256237,\n 0.009494511410593987,\n 0.011143258772790432,\n 0.014150169678032398,\n 0.01977233588695526,\n 0.018963754177093506,\n -0.0009965130593627691,\n 0.006702379789203405,\n -0.002929527312517166,\n 0.011452794075012207,\n -0.028856240212917328,\n 0.004219783004373312,\n -0.022867685183882713,\n 0.03294968232512474,\n 0.05508458986878395,\n -0.022349689155817032,\n -0.006557087879627943,\n -0.024901771917939186,\n 0.04502786323428154,\n -0.017447665333747864,\n -0.021819056943058968,\n 0.02011345513164997,\n -0.040681738406419754,\n -0.0010296775726601481,\n 0.022943489253520966,\n 0.019671263173222542,\n -0.0050346809439361095,\n 0.021263157948851585,\n -0.0057074460200965405,\n -0.012090815231204033,\n -0.0011962894350290298,\n -0.012292960658669472,\n 0.029159458354115486,\n -0.0006060410523787141,\n 0.02031560055911541,\n -0.01542621199041605,\n 0.015502016991376877,\n -0.044118206948041916,\n -0.007409888319671154,\n 0.022589735686779022,\n 0.007814179174602032,\n -0.009412390179932117,\n -0.0090586356818676,\n 0.02271607704460621,\n -0.028224535286426544,\n -0.004188197664916515,\n 0.002675266470760107,\n -0.015224066562950611,\n 0.019949212670326233,\n -0.01637376844882965,\n -0.0326717309653759,\n 0.011073771864175797,\n 0.012103448621928692,\n -0.02059355191886425,\n -0.00830690748989582,\n 0.014604996889829636,\n -0.015565186738967896,\n 0.024762798100709915,\n 0.014402851462364197,\n -0.03178734704852104,\n 0.020429307594895363,\n 0.004017637576907873,\n 0.037624292075634,\n 0.00696769542992115,\n 0.057813551276922226,\n 0.016310598701238632,\n -0.032722268253564835,\n 0.004052381496876478,\n 0.0035312254913151264,\n 0.006816086359322071,\n 0.00017786415992304683,\n -0.030726084485650063,\n -0.01891321875154972,\n -0.020126089453697205,\n 0.009715607389807701,\n -0.013948025181889534,\n -0.011332769878208637,\n -0.00497782789170742,\n 0.0064812833443284035,\n 0.03570391237735748,\n -0.013177345506846905,\n -0.002182537456974387,\n -0.015552553348243237,\n -0.013442661613225937,\n 0.002740016207098961,\n -0.017182350158691406,\n 0.0411112979054451,\n -0.008654344826936722,\n -0.01685386337339878,\n 0.041237637400627136,\n 0.006301247514784336,\n -0.036840979009866714,\n 0.003932357300072908,\n 0.009980923496186733,\n -0.005675860680639744,\n 0.02089676819741726,\n 0.06493917107582092,\n 0.01785195618867874,\n -0.009153391234576702,\n 0.007460424676537514,\n -0.029083652421832085,\n -0.014200706034898758,\n 0.035602837800979614,\n -0.018989022821187973,\n -0.017750883474946022,\n 0.022021202370524406,\n 0.03292441368103027,\n -0.005682177841663361,\n 0.017359226942062378,\n -0.01637376844882965,\n -0.00912812352180481,\n -0.016007380560040474,\n -0.016121085733175278,\n 0.027744440361857414,\n -0.02701166458427906,\n 0.0036607247311621904,\n -0.02069462463259697,\n -0.024990210309624672,\n 0.02817399986088276,\n 0.03666410222649574,\n -0.04495205730199814,\n 0.005634800065308809,\n -0.03342977538704872,\n -0.020568283274769783,\n 0.033859334886074066,\n -0.0026547361630946398,\n 0.010113581083714962,\n 0.014781873673200607,\n 0.007371985819190741,\n -0.004090283531695604,\n 0.038255997002124786,\n 0.03350558131933212,\n -0.0020656720735132694,\n 0.016209525987505913,\n 0.033859334886074066,\n -0.0010202019475400448,\n -0.010922162793576717,\n -0.001951965386979282,\n 0.013910122215747833,\n -0.00596328591927886,\n -0.05569102615118027,\n -0.0019803920295089483,\n -0.021010475233197212,\n 0.00425452645868063,\n -0.0124256182461977,\n 0.003106404561549425,\n -0.03075135126709938,\n -0.022905588150024414,\n 0.003079557092860341,\n 0.01771298050880432,\n 0.004879913758486509,\n 0.01921643689274788,\n 0.011762328445911407,\n -0.006784501019865274,\n -0.025520842522382736,\n -0.03901404142379761,\n 0.033202365040779114,\n 0.03651249408721924,\n -0.01949438638985157,\n 0.056853361427783966,\n -0.015363042242825031,\n 0.015312505885958672,\n 0.0073530348017811775,\n -0.015186164528131485,\n -0.004355599172413349,\n 0.008591175079345703,\n 0.00958926696330309,\n 0.011534915305674076,\n 0.020985208451747894,\n -0.01925433799624443,\n -0.02549557387828827,\n 0.026127278804779053,\n -0.0447499118745327,\n 0.01547674834728241,\n -0.0005452395416796207,\n 0.034920599311590195,\n -0.0223370548337698,\n 0.030245987698435783,\n 0.04232417047023773,\n 0.015742063522338867,\n -0.007100353017449379,\n 0.018850047141313553,\n -0.019418582320213318,\n -0.021528473123908043,\n -0.018925853073596954,\n 0.025242893025279045,\n 0.018951119855046272,\n 0.031105106696486473,\n 0.03782643750309944,\n 0.014213340356945992,\n -0.0230319295078516,\n 0.0040207961574196815,\n 0.0049367668107151985,\n -0.013733245432376862,\n -0.010170434601604939,\n 0.03380879759788513,\n 0.008660661987960339,\n 0.02573562227189541,\n -0.01134540420025587,\n -0.025861961767077446,\n 0.049980420619249344,\n -0.005249460227787495,\n 0.010890576988458633,\n 0.03858448192477226,\n -0.014743971638381481,\n -0.011263282969594002,\n -0.022589735686779022,\n -0.003392250509932637,\n -0.02824980393052101,\n 0.008869124576449394,\n -0.00402395473793149,\n 0.00573271419852972,\n 0.0229561235755682,\n 0.0387108214199543,\n -0.022084372118115425,\n 0.003357506822794676,\n 0.008534321561455727,\n -0.007409888319671154,\n 0.027163272723555565,\n -0.01154754962772131,\n 0.007270913105458021,\n 0.016045281663537025,\n 0.025647183880209923,\n -0.014263876713812351,\n 0.03254539147019386,\n 0.025242893025279045,\n 0.03461737930774689,\n 0.014478656463325024,\n -0.036461956799030304,\n 0.00034664757549762726,\n 0.0006245973636396229,\n 0.026102010160684586,\n -0.013316321186721325,\n -0.012261374853551388,\n 0.004469305742532015,\n -0.011768645606935024,\n 0.02317090332508087,\n 0.02327197603881359,\n -0.011206429451704025,\n -0.01527460291981697,\n -0.00888807512819767,\n -0.02693585865199566,\n 0.017308689653873444,\n 0.006279137916862965,\n ...]\n\n\nNext, I can apply this function to each of the notes stored in the notes column of the notes_df DataFrame, and save it in a new column of the DataFrame called embedding:\n\nnotes_df['embedding'] = notes_df['notes'].apply(lambda x: get_embedding(x))\n\nBelow, I show the first 5 rows. You can see that there is a new column containing a collection of 1,536-length lists corresponding to the note’s embedding.\n\nnotes_df.head(5)\n\n\n\n\n\n\n\n\n\nnotes\nembedding\n\n\n\n\n0\nPatient presented signs of common cold; symptoms include a runny nose and mild fever. Advised rest, hydration, and over-the-counter medication to manage symptoms.\n[0.007875490933656693, -0.0006737920339219272, -0.048676371574401855, 0.016742711886763573, -0.01964789256453514, 0.007379627320915461, -0.0036548112984746695, 0.01582098752260208, -0.00981811247766018, 0.05731024220585823, -0.05661020055413246, 0.05338999629020691, -0.0089722266420722, 0.005787027534097433, 0.009333916008472443, 0.002053461503237486, 0.020791297778487206, -0.013055814430117607, -0.03626226261258125, 0.06151050329208374, -0.0013227908639237285, 0.06081046164035797, 0.021164653822779655, 0.0037043977063149214, -0.004646540153771639, 0.009497258812189102, 0.006889596581459045, -0.02360313944518566, 0.04398607835173607, -0.06272391229867935, 0.02380148507654667, -0.01809612847864628, 0.003272704314440489, -0.017582762986421585, 0.030848590657114983, 0.023124776780605316, 0.022611411288380623, -0.046296220272779465, -0.006352896336466074, -0.006597911473363638, -0.023031437769532204, -0.01648602820932865, -0.01208742056041956, -0.006551241967827082, -0.030731918290257454, 0.0008546366589143872, 0.04699626564979553, -0.018119463697075844, 0.004751546308398247, -0.02417484112083912, 0.010763171128928661, -0.026694998145103455, 0.006452069152146578, 0.008348020724952221, 0.010535657405853271, 0.02921515703201294, 0.02755838632583618, 0.012624121271073818, -0.013405836187303066, 0.0015488466015085578, 0.0514298751950264, -0.03217867389321327, 0.02334645763039589, 0.013347499072551727, -0.004766130819916725, -0.013499175198376179, 0.03761567920446396, 0.01333583239465952, 0.026461651548743248, 0.010774838738143444, 0.006154550705105066, -0.028328433632850647, 0.025574930012226105, 0.003345625475049019, -0.025994954630732536, 0.04412608593702316, 0.019204530864953995, -0.041139233857393265, 0.024034833535552025, 0.014700917527079582, 0.0011288204696029425, -0.012075753882527351, 0.043052684515714645, -0.0011660102754831314, -0.023159777745604515, -0.05987706780433655, -0.033765438944101334, -0.023778149858117104, 0.027441712096333504, -0.031735312193632126, 0.031525298953056335, 0.03994916006922722, 0.006761255208402872, -0.028771795332431793, -0.013347499072551727, -0.02634497731924057, 0.02550492435693741, 0.02359147183597088, -0.03294872120022774, 0.03000853955745697, ...]\n\n\n1\nPatient reports difficulty sleeping. Exhibiting signs of insomnia characterized by frequent night time awakenings. Recommended a sleep study to find underlying cause.\n[-0.0345405712723732, -0.010776856914162636, -0.010913430713117123, 0.01344623975455761, -0.0371975414454937, 0.01843736506998539, 0.022373152896761894, -0.01380629651248455, 0.06128406524658203, -0.0055250017903745174, -0.02246006391942501, 0.03488821163773537, -0.060837097465991974, 0.019294051453471184, -0.005022164434194565, 0.017419274896383286, 0.014824386686086655, -0.016761241480708122, -0.030120572075247765, 0.0056708864867687225, 0.036700911819934845, 0.03622911125421524, -0.006679664831608534, -0.0573606938123703, 0.024992873892188072, 0.01998933218419552, 0.030319223180413246, -0.023142928257584572, 0.029872257262468338, -0.03113866224884987, 0.004680731799453497, -0.020436298102140427, 0.014042195864021778, -0.020846018567681313, -0.02521635591983795, 0.04705563187599182, 0.04226315766572952, 0.006909355986863375, -0.010733402334153652, -0.015283768996596336, -0.006704496685415506, -0.02394995093345642, 0.01163354329764843, -0.01688539795577526, -0.030865514650940895, -0.037445854395627975, -0.023502985015511513, -0.040425632148981094, 0.07345148175954819, 0.03406877443194389, -0.022149669006466866, 0.016016297042369843, 0.0220627598464489, -0.022075176239013672, 0.038935743272304535, -0.040624283254146576, 0.05368563532829285, 0.010013289749622345, -0.003945099655538797, 0.031188324093818665, 0.03250439092516899, 0.03357214480638504, 0.04186585545539856, 0.007896406576037407, 0.0016187013825401664, -0.0722595751285553, 0.01103137992322445, 0.01600388064980507, 0.008777923882007599, 0.043728217482566833, -0.005543625447899103, 0.010950678028166294, 0.005773316603153944, 0.05368563532829285, 0.01149076223373413, 0.024744559079408646, 0.006235802546143532, -0.028754839673638344, -0.00626994576305151, 0.009876716881990433, -0.02686764858663082, 0.006273049861192703, 0.009839469566941261, 0.046385183930397034, 0.0031737720128148794, -0.02080877125263214, -0.047105297446250916, -0.06217799708247185, 0.01106862723827362, -0.05676473677158356, 0.021963434293866158, -0.0022767353802919388, -0.00857306458055973, 0.015159611590206623, 0.005546729080379009, -0.050954174250364304, -0.0011461274698376656, 0.02709113247692585, -0.00894553679972887, 0.04352956265211105, ...]\n\n\n2\nPatient came in with complaints of stomachache. Upon examination, no obvious signs of appendicitis or gallstones were found. Recommended patient to undergo an ultrasound to assess internal organs.\n[-0.017208361998200417, 0.006888406351208687, -0.005633207969367504, 0.04822390526533127, -0.014728333801031113, 0.025873279199004173, -0.03370814397931099, -0.0009527865331619978, 0.03423452004790306, -0.010254159569740295, -0.04486321285367012, -0.008685161359608173, 0.01806878112256527, 0.019941454753279686, 0.028039507567882538, 0.016347944736480713, 0.0295174028724432, -0.05652441084384918, -0.020892975851893425, 0.009252025745809078, 0.053163718432188034, 0.015467280521988869, 0.02342361770570278, -0.0332222618162632, 0.04615890234708786, -0.051422636955976486, 0.01870650239288807, -0.00034986119135282934, -0.023038960993289948, -0.024010727182030678, 0.047697532922029495, -0.0066960775293409824, 0.03565167635679245, -0.026075730100274086, -0.035894621163606644, 0.06778070330619812, 0.03374863415956497, 0.0055269212462008, -0.015720345079898834, -0.02858612686395645, 0.009727786295115948, 0.024597834795713425, 0.019010178744792938, -0.020235009491443634, -0.007941152900457382, 0.025245679542422295, 0.008472587913274765, -0.02548862062394619, 0.02113591879606247, 0.0035783271305263042, 0.0027204395737499, 0.03320201858878136, 0.021115673705935478, -0.01451575942337513, 0.006088723428547382, -0.05980411916971207, -0.02872784249484539, -0.0322909876704216, 0.03806084766983986, 0.025144454091787338, 0.004529848229140043, 0.03957923501729965, 0.06259794533252716, 0.05907529592514038, -0.010821023024618626, -0.07883454114198685, 0.05713176354765892, 0.040915410965681076, 0.044458311051130295, -0.02583278901875019, 0.013078355230391026, 0.02773583121597767, -0.010238975286483765, 0.02148008532822132, 0.07474502921104431, 0.00839666835963726, 0.08215474337339401, -0.026865290477871895, 0.010861513204872608, -0.0009135616128332913, 0.004243885632604361, 0.019081037491559982, 0.005435817874968052, 0.03941727057099342, 0.014819436706602573, 0.02615671046078205, -0.0274321548640728, -0.034760892391204834, 0.02696651592850685, 0.00028643698897212744, 0.001967573771253228, 0.011448622681200504, 0.04247428849339485, -0.0023927215952426195, -0.006756812799721956, -0.03441672399640083, 0.001920756883919239, 0.03672466799616814, -0.031015543267130852, 0.02471930719912052, ...]\n\n\n3\nHypertensive patient presented with higher blood pressure at 160/100. Adjusted medications and emphasized importance of diet and exercise.\n[-0.01908160373568535, -0.007425420451909304, 0.017478106543421745, 0.0633997693657875, -0.03739846125245094, 0.0564923994243145, -0.014924848452210426, 0.03949534147977829, -0.022670967504382133, -0.008227168582379818, 0.0023235275875777006, 0.01861288957297802, -0.05338408425450325, 0.021585524082183838, 0.032809995114803314, 0.0043664430268108845, 0.05175592005252838, -0.01282796822488308, -0.01846487447619438, 0.010040352120995522, -0.01032404787838459, -0.01255044061690569, 0.03727511689066887, -0.03327871114015579, -0.01349403616040945, -0.02069743350148201, 0.026494689285755157, -0.05005374550819397, 0.042801011353731155, -0.04746348410844803, 0.04080280661582947, -0.02429913356900215, 0.025137884542346, 0.0548148974776268, -0.005189776886254549, -0.0036479535046964884, 0.0019658245146274567, -0.0015927032800391316, 0.022905325517058372, 0.0035554443020373583, -0.0008025189745239913, -0.028912268579006195, -0.020956460386514664, 0.0659160241484642, -0.03490687534213066, 0.02893693745136261, 0.017613787204027176, -0.03764515370130539, 0.0155539121478796, 0.02130182832479477, -0.019982028752565384, 0.010428891517221928, -0.023115012794733047, 0.019328294321894646, 0.03290867432951927, 0.015183874405920506, 0.03014572709798813, 0.020487746223807335, -0.007838629186153412, 0.031132493168115616, 0.014184772968292236, -0.013975084759294987, 0.022300930693745613, 0.025729944929480553, -0.04578598216176033, -0.02073443867266178, 0.015072863548994064, -0.011446495540440083, 0.03140385448932648, 0.021425174549221992, -0.014949517324566841, -0.030022380873560905, -0.030269071459770203, 0.05358143895864487, -0.0035708623472601175, -0.022892991080880165, -0.005436468403786421, -0.03009638749063015, -0.003157653845846653, 0.055308278650045395, -0.029603004455566406, -0.02204190380871296, -0.009065920487046242, 0.04381244629621506, 0.009756657294929028, -0.06902433931827545, 0.0014246446080505848, 9.824304288486019e-05, -0.005482723005115986, -0.08269105851650238, 0.0053686280734837055, 0.005710912868380547, -0.0006240529473870993, 0.004471287131309509, -0.03041708655655384, 0.003200824838131666, -0.04183891415596008, 0.03251396492123604, -0.0017438019858673215, -0.0027629469987004995, ...]\n\n\n4\nExamined patient for frequent headaches. No alarming neurological signs detected. Recommended reducing caffeine intake and keeping hydrated.\n[-0.010823288932442665, 0.03450469672679901, -0.0003536372387316078, 0.01706509292125702, -0.033481039106845856, 0.030859481543302536, -0.00017135703819803894, 0.05228135362267494, -0.010186624713242054, 0.017614372074604034, 0.003529740497469902, 0.021334487944841385, -0.02444290742278099, 0.054828010499477386, -0.012496092356741428, -0.005037136375904083, 0.008351534605026245, -0.02938641607761383, -0.005436611827462912, 0.02893700636923313, 0.03158353269100189, 0.06366640329360962, 0.05123273283243179, 0.015329872258007526, 0.0488358773291111, 0.01000561285763979, 0.010242801159620285, -0.041695255786180496, 0.03263215348124504, -0.022308209910988808, 0.02629048004746437, -0.00510579627007246, -0.018038814887404442, 0.03467946499586105, -0.005889142397791147, 0.022645266726613045, 0.025504013523459435, -0.004253789782524109, -0.013145240023732185, -0.0078896414488554, 0.016728036105632782, -0.020210962742567062, 0.016815422102808952, 0.01926220953464508, -0.015641963109374046, -0.046139419078826904, 0.02104736492037773, -0.03822481259703636, 0.051582273095846176, 0.006853501312434673, -0.039173565804958344, 0.012858116999268532, 0.02134697139263153, 0.013245109468698502, 0.014643273316323757, -0.010473747737705708, 0.003486047964543104, -0.024005981162190437, 0.06111975014209747, 0.02057298831641674, 0.052980437874794006, -0.003170836716890335, 0.02688969485461712, 0.030260268598794937, -0.03732599318027496, -0.037800367921590805, 0.03233254700899124, -0.04114597663283348, 0.06072027608752251, -0.013956675305962563, -0.019536849111318588, 0.005062103737145662, 0.04409210756421089, 0.004054052289575338, 0.008794702589511871, -0.013170207850635052, 0.05472814291715622, -0.06970847398042679, -0.025416627526283264, 0.05397912487387657, -0.0226327832788229, 0.025591399520635605, 0.008863362483680248, 0.04014728590846062, -0.012508576735854149, -0.05412892997264862, -0.029186679050326347, -0.03198300674557686, -0.015254970639944077, -0.03430495783686638, -0.024230685085058212, 0.012764490209519863, 0.010411330498754978, 0.012957986444234848, -0.016128823161125183, -0.012021715752780437, 0.015854183584451675, 0.00357655412517488, -0.015666930004954338, 0.02527930773794651, ...]\n\n\n\n\n\n\n\n\nSince I will want to compute the cosine distance to the “target” (“Patient presenting with ongoing chronic condition defined as ongoing for more than one month”), I will also need to compute the embedding of the label.\n\ntarget = 'Patient presenting with ongoing chronic condition defined as ongoing for more than one month'\n# Compute embedding of the target\ntarget_embedding = get_embedding(target)"
  },
  {
    "objectID": "blog/2024-04-11-labeling_doctors_notes.html#computing-the-cosine-distance-from-the-target",
    "href": "blog/2024-04-11-labeling_doctors_notes.html#computing-the-cosine-distance-from-the-target",
    "title": "AI Tutorial: Using Text Embeddings to Label Synthetic Doctor’s Notes Generated with ChatGPT",
    "section": "Computing the cosine distance from the target",
    "text": "Computing the cosine distance from the target\nNow that I have all of my embeddings, I need to compute the cosine distance from each doctor’s note’s embedding to this target embedding. Below, I compute and save this cosine distance in a new column of notes_df called cosine_dist (note that a lower cosine_dist value corresponds to a doctor’s note that is more similar to the target: “Patient presenting with ongoing chronic condition defined as ongoing for more than one month”).\n\nnotes_df['cosine_dist'] = notes_df.embedding.apply(lambda x: cosine(x, target_embedding))\n\nYou can see that the final column in notes_df is now the cosine distance:\n\nnotes_df.head(5)\n\n\n\n\n\n\n\n\n\nnotes\nembedding\ncosine_dist\n\n\n\n\n0\nPatient presented signs of common cold; symptoms include a runny nose and mild fever. Advised rest, hydration, and over-the-counter medication to manage symptoms.\n[0.007875490933656693, -0.0006737920339219272, -0.048676371574401855, 0.016742711886763573, -0.01964789256453514, 0.007379627320915461, -0.0036548112984746695, 0.01582098752260208, -0.00981811247766018, 0.05731024220585823, -0.05661020055413246, 0.05338999629020691, -0.0089722266420722, 0.005787027534097433, 0.009333916008472443, 0.002053461503237486, 0.020791297778487206, -0.013055814430117607, -0.03626226261258125, 0.06151050329208374, -0.0013227908639237285, 0.06081046164035797, 0.021164653822779655, 0.0037043977063149214, -0.004646540153771639, 0.009497258812189102, 0.006889596581459045, -0.02360313944518566, 0.04398607835173607, -0.06272391229867935, 0.02380148507654667, -0.01809612847864628, 0.003272704314440489, -0.017582762986421585, 0.030848590657114983, 0.023124776780605316, 0.022611411288380623, -0.046296220272779465, -0.006352896336466074, -0.006597911473363638, -0.023031437769532204, -0.01648602820932865, -0.01208742056041956, -0.006551241967827082, -0.030731918290257454, 0.0008546366589143872, 0.04699626564979553, -0.018119463697075844, 0.004751546308398247, -0.02417484112083912, 0.010763171128928661, -0.026694998145103455, 0.006452069152146578, 0.008348020724952221, 0.010535657405853271, 0.02921515703201294, 0.02755838632583618, 0.012624121271073818, -0.013405836187303066, 0.0015488466015085578, 0.0514298751950264, -0.03217867389321327, 0.02334645763039589, 0.013347499072551727, -0.004766130819916725, -0.013499175198376179, 0.03761567920446396, 0.01333583239465952, 0.026461651548743248, 0.010774838738143444, 0.006154550705105066, -0.028328433632850647, 0.025574930012226105, 0.003345625475049019, -0.025994954630732536, 0.04412608593702316, 0.019204530864953995, -0.041139233857393265, 0.024034833535552025, 0.014700917527079582, 0.0011288204696029425, -0.012075753882527351, 0.043052684515714645, -0.0011660102754831314, -0.023159777745604515, -0.05987706780433655, -0.033765438944101334, -0.023778149858117104, 0.027441712096333504, -0.031735312193632126, 0.031525298953056335, 0.03994916006922722, 0.006761255208402872, -0.028771795332431793, -0.013347499072551727, -0.02634497731924057, 0.02550492435693741, 0.02359147183597088, -0.03294872120022774, 0.03000853955745697, ...]\n0.697187\n\n\n1\nPatient reports difficulty sleeping. Exhibiting signs of insomnia characterized by frequent night time awakenings. Recommended a sleep study to find underlying cause.\n[-0.0345405712723732, -0.010776856914162636, -0.010913430713117123, 0.01344623975455761, -0.0371975414454937, 0.01843736506998539, 0.022373152896761894, -0.01380629651248455, 0.06128406524658203, -0.0055250017903745174, -0.02246006391942501, 0.03488821163773537, -0.060837097465991974, 0.019294051453471184, -0.005022164434194565, 0.017419274896383286, 0.014824386686086655, -0.016761241480708122, -0.030120572075247765, 0.0056708864867687225, 0.036700911819934845, 0.03622911125421524, -0.006679664831608534, -0.0573606938123703, 0.024992873892188072, 0.01998933218419552, 0.030319223180413246, -0.023142928257584572, 0.029872257262468338, -0.03113866224884987, 0.004680731799453497, -0.020436298102140427, 0.014042195864021778, -0.020846018567681313, -0.02521635591983795, 0.04705563187599182, 0.04226315766572952, 0.006909355986863375, -0.010733402334153652, -0.015283768996596336, -0.006704496685415506, -0.02394995093345642, 0.01163354329764843, -0.01688539795577526, -0.030865514650940895, -0.037445854395627975, -0.023502985015511513, -0.040425632148981094, 0.07345148175954819, 0.03406877443194389, -0.022149669006466866, 0.016016297042369843, 0.0220627598464489, -0.022075176239013672, 0.038935743272304535, -0.040624283254146576, 0.05368563532829285, 0.010013289749622345, -0.003945099655538797, 0.031188324093818665, 0.03250439092516899, 0.03357214480638504, 0.04186585545539856, 0.007896406576037407, 0.0016187013825401664, -0.0722595751285553, 0.01103137992322445, 0.01600388064980507, 0.008777923882007599, 0.043728217482566833, -0.005543625447899103, 0.010950678028166294, 0.005773316603153944, 0.05368563532829285, 0.01149076223373413, 0.024744559079408646, 0.006235802546143532, -0.028754839673638344, -0.00626994576305151, 0.009876716881990433, -0.02686764858663082, 0.006273049861192703, 0.009839469566941261, 0.046385183930397034, 0.0031737720128148794, -0.02080877125263214, -0.047105297446250916, -0.06217799708247185, 0.01106862723827362, -0.05676473677158356, 0.021963434293866158, -0.0022767353802919388, -0.00857306458055973, 0.015159611590206623, 0.005546729080379009, -0.050954174250364304, -0.0011461274698376656, 0.02709113247692585, -0.00894553679972887, 0.04352956265211105, ...]\n0.583531\n\n\n2\nPatient came in with complaints of stomachache. Upon examination, no obvious signs of appendicitis or gallstones were found. Recommended patient to undergo an ultrasound to assess internal organs.\n[-0.017208361998200417, 0.006888406351208687, -0.005633207969367504, 0.04822390526533127, -0.014728333801031113, 0.025873279199004173, -0.03370814397931099, -0.0009527865331619978, 0.03423452004790306, -0.010254159569740295, -0.04486321285367012, -0.008685161359608173, 0.01806878112256527, 0.019941454753279686, 0.028039507567882538, 0.016347944736480713, 0.0295174028724432, -0.05652441084384918, -0.020892975851893425, 0.009252025745809078, 0.053163718432188034, 0.015467280521988869, 0.02342361770570278, -0.0332222618162632, 0.04615890234708786, -0.051422636955976486, 0.01870650239288807, -0.00034986119135282934, -0.023038960993289948, -0.024010727182030678, 0.047697532922029495, -0.0066960775293409824, 0.03565167635679245, -0.026075730100274086, -0.035894621163606644, 0.06778070330619812, 0.03374863415956497, 0.0055269212462008, -0.015720345079898834, -0.02858612686395645, 0.009727786295115948, 0.024597834795713425, 0.019010178744792938, -0.020235009491443634, -0.007941152900457382, 0.025245679542422295, 0.008472587913274765, -0.02548862062394619, 0.02113591879606247, 0.0035783271305263042, 0.0027204395737499, 0.03320201858878136, 0.021115673705935478, -0.01451575942337513, 0.006088723428547382, -0.05980411916971207, -0.02872784249484539, -0.0322909876704216, 0.03806084766983986, 0.025144454091787338, 0.004529848229140043, 0.03957923501729965, 0.06259794533252716, 0.05907529592514038, -0.010821023024618626, -0.07883454114198685, 0.05713176354765892, 0.040915410965681076, 0.044458311051130295, -0.02583278901875019, 0.013078355230391026, 0.02773583121597767, -0.010238975286483765, 0.02148008532822132, 0.07474502921104431, 0.00839666835963726, 0.08215474337339401, -0.026865290477871895, 0.010861513204872608, -0.0009135616128332913, 0.004243885632604361, 0.019081037491559982, 0.005435817874968052, 0.03941727057099342, 0.014819436706602573, 0.02615671046078205, -0.0274321548640728, -0.034760892391204834, 0.02696651592850685, 0.00028643698897212744, 0.001967573771253228, 0.011448622681200504, 0.04247428849339485, -0.0023927215952426195, -0.006756812799721956, -0.03441672399640083, 0.001920756883919239, 0.03672466799616814, -0.031015543267130852, 0.02471930719912052, ...]\n0.748090\n\n\n3\nHypertensive patient presented with higher blood pressure at 160/100. Adjusted medications and emphasized importance of diet and exercise.\n[-0.01908160373568535, -0.007425420451909304, 0.017478106543421745, 0.0633997693657875, -0.03739846125245094, 0.0564923994243145, -0.014924848452210426, 0.03949534147977829, -0.022670967504382133, -0.008227168582379818, 0.0023235275875777006, 0.01861288957297802, -0.05338408425450325, 0.021585524082183838, 0.032809995114803314, 0.0043664430268108845, 0.05175592005252838, -0.01282796822488308, -0.01846487447619438, 0.010040352120995522, -0.01032404787838459, -0.01255044061690569, 0.03727511689066887, -0.03327871114015579, -0.01349403616040945, -0.02069743350148201, 0.026494689285755157, -0.05005374550819397, 0.042801011353731155, -0.04746348410844803, 0.04080280661582947, -0.02429913356900215, 0.025137884542346, 0.0548148974776268, -0.005189776886254549, -0.0036479535046964884, 0.0019658245146274567, -0.0015927032800391316, 0.022905325517058372, 0.0035554443020373583, -0.0008025189745239913, -0.028912268579006195, -0.020956460386514664, 0.0659160241484642, -0.03490687534213066, 0.02893693745136261, 0.017613787204027176, -0.03764515370130539, 0.0155539121478796, 0.02130182832479477, -0.019982028752565384, 0.010428891517221928, -0.023115012794733047, 0.019328294321894646, 0.03290867432951927, 0.015183874405920506, 0.03014572709798813, 0.020487746223807335, -0.007838629186153412, 0.031132493168115616, 0.014184772968292236, -0.013975084759294987, 0.022300930693745613, 0.025729944929480553, -0.04578598216176033, -0.02073443867266178, 0.015072863548994064, -0.011446495540440083, 0.03140385448932648, 0.021425174549221992, -0.014949517324566841, -0.030022380873560905, -0.030269071459770203, 0.05358143895864487, -0.0035708623472601175, -0.022892991080880165, -0.005436468403786421, -0.03009638749063015, -0.003157653845846653, 0.055308278650045395, -0.029603004455566406, -0.02204190380871296, -0.009065920487046242, 0.04381244629621506, 0.009756657294929028, -0.06902433931827545, 0.0014246446080505848, 9.824304288486019e-05, -0.005482723005115986, -0.08269105851650238, 0.0053686280734837055, 0.005710912868380547, -0.0006240529473870993, 0.004471287131309509, -0.03041708655655384, 0.003200824838131666, -0.04183891415596008, 0.03251396492123604, -0.0017438019858673215, -0.0027629469987004995, ...]\n0.644881\n\n\n4\nExamined patient for frequent headaches. No alarming neurological signs detected. Recommended reducing caffeine intake and keeping hydrated.\n[-0.010823288932442665, 0.03450469672679901, -0.0003536372387316078, 0.01706509292125702, -0.033481039106845856, 0.030859481543302536, -0.00017135703819803894, 0.05228135362267494, -0.010186624713242054, 0.017614372074604034, 0.003529740497469902, 0.021334487944841385, -0.02444290742278099, 0.054828010499477386, -0.012496092356741428, -0.005037136375904083, 0.008351534605026245, -0.02938641607761383, -0.005436611827462912, 0.02893700636923313, 0.03158353269100189, 0.06366640329360962, 0.05123273283243179, 0.015329872258007526, 0.0488358773291111, 0.01000561285763979, 0.010242801159620285, -0.041695255786180496, 0.03263215348124504, -0.022308209910988808, 0.02629048004746437, -0.00510579627007246, -0.018038814887404442, 0.03467946499586105, -0.005889142397791147, 0.022645266726613045, 0.025504013523459435, -0.004253789782524109, -0.013145240023732185, -0.0078896414488554, 0.016728036105632782, -0.020210962742567062, 0.016815422102808952, 0.01926220953464508, -0.015641963109374046, -0.046139419078826904, 0.02104736492037773, -0.03822481259703636, 0.051582273095846176, 0.006853501312434673, -0.039173565804958344, 0.012858116999268532, 0.02134697139263153, 0.013245109468698502, 0.014643273316323757, -0.010473747737705708, 0.003486047964543104, -0.024005981162190437, 0.06111975014209747, 0.02057298831641674, 0.052980437874794006, -0.003170836716890335, 0.02688969485461712, 0.030260268598794937, -0.03732599318027496, -0.037800367921590805, 0.03233254700899124, -0.04114597663283348, 0.06072027608752251, -0.013956675305962563, -0.019536849111318588, 0.005062103737145662, 0.04409210756421089, 0.004054052289575338, 0.008794702589511871, -0.013170207850635052, 0.05472814291715622, -0.06970847398042679, -0.025416627526283264, 0.05397912487387657, -0.0226327832788229, 0.025591399520635605, 0.008863362483680248, 0.04014728590846062, -0.012508576735854149, -0.05412892997264862, -0.029186679050326347, -0.03198300674557686, -0.015254970639944077, -0.03430495783686638, -0.024230685085058212, 0.012764490209519863, 0.010411330498754978, 0.012957986444234848, -0.016128823161125183, -0.012021715752780437, 0.015854183584451675, 0.00357655412517488, -0.015666930004954338, 0.02527930773794651, ...]\n0.688291\n\n\n\n\n\n\n\n\nAnd that’s it! Let’s take a look at the notes in order of smallest to largest cosine_dist.\nIdeally, I will see that the notes at the top of the rearranged DataFrame correspond to patients with chronic conditions, and the notes at the bottom correspond to patients with acute conditions.\nThe top 5 notes are:\n\npd.set_option('display.max_colwidth', None)\nnotes_df.sort_values('cosine_dist')[['notes', 'cosine_dist']].head(5)\n\n\n\n\n\n\n\n\n\nnotes\ncosine_dist\n\n\n\n\n43\nPatient with chronic bronchitis showed increasingly frequent flare-ups. Reinforced importance of smoking cessation and upgraded prescribed inhalers.\n0.483188\n\n\n19\nPatient visited for follow-up on hypertension medication. Blood pressure readings stable. Emphasized continued adherence to treatment.\n0.547638\n\n\n27\nPatient reported persistent cough and difficulty breathing. Referred for radiological imaging to rule out pneumonia or tuberculosis.\n0.550634\n\n\n22\nPatient with a history of heart disease complained of palpitations. Recommended further cardiology work-up and emphasized continued adherence to treatment.\n0.558663\n\n\n31\nChronic back pain patient showed improvement after physiotherapy. Recommended continuing sessions and daily exercises at home.\n0.565519\n\n\n\n\n\n\n\n\nThose all seem like patients with chronic conditions to me!\nAnd the bottom 5 notes are:\n\nnotes_df.sort_values('cosine_dist')[['notes', 'cosine_dist']].tail(5)\n\n\n\n\n\n\n\n\n\nnotes\ncosine_dist\n\n\n\n\n39\nChild patient presented with red, itchy eyes. Diagnosed as viral conjunctivitis. Instructed parents about proper eye care and hand hygiene.\n0.743466\n\n\n16\nPatient suffered a minor burn in the kitchen. Prescribed topical ointment and advised on wound care.\n0.745391\n\n\n26\nPatient presented with severe knee pain. Suspected ligament tear. Recommended MRI scan.\n0.746433\n\n\n2\nPatient came in with complaints of stomachache. Upon examination, no obvious signs of appendicitis or gallstones were found. Recommended patient to undergo an ultrasound to assess internal organs.\n0.748090\n\n\n49\nPatient presented with dislocated shoulder. After successful relocation, advised sling use and gentle mobility exercises for rehabilitation.\n0.748190\n\n\n\n\n\n\n\n\nThese definitely seem like new/acute conditions!\nIf you’re interested in looking at all of the distances, see the following output (remember that the smaller the distance, the closer the note to the chronic condition target):\n\nnotes_df.sort_values('cosine_dist')[['notes', 'cosine_dist']]\n\n\n\n\n\n\n\n\n\nnotes\ncosine_dist\n\n\n\n\n43\nPatient with chronic bronchitis showed increasingly frequent flare-ups. Reinforced importance of smoking cessation and upgraded prescribed inhalers.\n0.483188\n\n\n19\nPatient visited for follow-up on hypertension medication. Blood pressure readings stable. Emphasized continued adherence to treatment.\n0.547638\n\n\n27\nPatient reported persistent cough and difficulty breathing. Referred for radiological imaging to rule out pneumonia or tuberculosis.\n0.550634\n\n\n22\nPatient with a history of heart disease complained of palpitations. Recommended further cardiology work-up and emphasized continued adherence to treatment.\n0.558663\n\n\n31\nChronic back pain patient showed improvement after physiotherapy. Recommended continuing sessions and daily exercises at home.\n0.565519\n\n\n15\nAsthmatic patient presented with increased shortness of breath. Stepped-up asthma control medication.\n0.578419\n\n\n11\nPatient admitted for chronic fatigue. Bloodwork ordered to rule out anemia, thyroid disorders, and diabetes. Recommended nutritional counseling.\n0.580687\n\n\n1\nPatient reports difficulty sleeping. Exhibiting signs of insomnia characterized by frequent night time awakenings. Recommended a sleep study to find underlying cause.\n0.583531\n\n\n5\nPatient came in for annual physical. All markers within normal limits. Encouraged to continue healthy lifestyle for prevention of non-communicable diseases.\n0.592007\n\n\n13\nPatient managing COPD came in for routine check-up. Oxygen levels stable. Expressed importance of avoiding respiratory irritants.\n0.597322\n\n\n35\nPatient presented with swollen (oedemic) ankles. Adjusted diuretic medication dosage and recommended follow-up visit next week.\n0.598728\n\n\n30\nElderly patient presented with signs of Parkinson's disease. Referred to a neurologist for specialized care and management.\n0.601647\n\n\n51\nPatient complained of periodic blurry vision. Referred to ophthalmology for proper ocular examination and management.\n0.607228\n\n\n24\nPatient managed for rheumatoid arthritis reported increased inflammation in joints. Adjusted medication and recommended physiotherapy for managing symptoms.\n0.610967\n\n\n28\nPatient presented with severe migraine episodes. Prescribed stronger painkillers and referred to a neurologist for a complete neurological work-up.\n0.613839\n\n\n41\nPatient presented with painful and swollen joints. Diagnosed as gout and recommended dietary adjustment along with pain management.\n0.618245\n\n\n25\nElderly patient with a history of diabetes complained of numbness in feet. Diagnosed peripheral neuropathy, referred to podiatrist for further management.\n0.619704\n\n\n37\nElderly patient exhibited signs of Alzheimer's. Informed family and referred to neurology specialist for comprehensive management.\n0.619889\n\n\n10\nPatient presented with chest pain. Preliminary assessment ruled out heart conditions. Recommended gastroenterology consult to check for possible reflux disease.\n0.625152\n\n\n46\nPatient with anxiety showed improvement with recent medication schedule. Encouraged continued therapy and stress management techniques.\n0.626963\n\n\n18\nPatient reports persistent vertigo. Referred to an Ear, Nose, and Throat (ENT) specialist for detailed evaluation to rule out Meniere's disease.\n0.628280\n\n\n42\nPatient reported sleep apnea symptoms. Advised to undergo a sleep study, prescribed initial treatment using CPAP device at night during sleep.\n0.636255\n\n\n48\nChild patient presented with symptoms of ADHD. Referred to pediatric psychiatrist for cognitive and behavioral evaluation plus treatment.\n0.636682\n\n\n44\nPatient with history of stroke presented with dizziness. Follow-up brain scan scheduled to rule out recurrence.\n0.637395\n\n\n32\nPatient came in for weight management consultation. Diet plan adjusted and encouraged increased physical activity.\n0.639904\n\n\n3\nHypertensive patient presented with higher blood pressure at 160/100. Adjusted medications and emphasized importance of diet and exercise.\n0.644881\n\n\n20\nHistorically anxious patient reported increased panic attacks recently. Referred to psychiatrist for therapeutic support, prescribed low-dose anxiolytic.\n0.645849\n\n\n45\nPatient reported feeling low, indicating depression. Counseled and referred to mental health services for further treatment and support.\n0.647704\n\n\n12\nElderly patient with poor vision due to cataracts. Scheduled for ophthalmology consultation.\n0.648220\n\n\n38\nPatient with history of irritable bowel syndrome showed signs of relief with diet modification. Reinforced importance of hydration and fiber intake.\n0.657894\n\n\n36\nTeenage patient came in with signs of acne. Prescribed topical retinoid and advised consistent skin care routine.\n0.658439\n\n\n9\nPatient returned for post-operative wound check. Incision healing well with no signs of infection. Advised continued wound care protocol and scheduled follow-up in a week.\n0.660693\n\n\n8\nArthritic patient reports increased joint pain. Advised gentle physical therapy and adjustment of pain medication.\n0.661807\n\n\n14\nPatient presented with signs of depression. Recommended seeking a psychologist for cognitive behavioral therapy and prescribed low dose antidepressant.\n0.666589\n\n\n52\nElderly patient with hearing difficulty. Scheduled an appointment for audiology consultation and potential hearing aids.\n0.667571\n\n\n6\nRoutine consultation with patient managing type II diabetes. Blood glucose levels fairly controlled. Advised to keep monitoring sugar levels and follow diet regimen.\n0.669839\n\n\n21\nPatient has concerning mole on the back. Scheduled for dermatology consult to rule out potential skin cancer.\n0.675400\n\n\n29\nPatient suffering from high fevers and weight loss. Ordered comprehensive bloodwork to assess for possible infectious disease.\n0.676904\n\n\n53\nPatient presented with signs of Bell's Palsy. Referred to neurologist and prescribed corticosteroid medication to reduce inflammation.\n0.682327\n\n\n4\nExamined patient for frequent headaches. No alarming neurological signs detected. Recommended reducing caffeine intake and keeping hydrated.\n0.688291\n\n\n50\nPregnant patient at full term demonstrated normal signs. Advised to prepare for labor and familiarize with signs of onset.\n0.696364\n\n\n0\nPatient presented signs of common cold; symptoms include a runny nose and mild fever. Advised rest, hydration, and over-the-counter medication to manage symptoms.\n0.697187\n\n\n33\nPregnant patient showed normal progression in second trimester. Advised rest, proper nutrition and regular exercise.\n0.702353\n\n\n47\nType 1 diabetic patient showed increased blood sugar levels. Insulin doses were adjusted, advised dietary check.\n0.707895\n\n\n40\nPatient showed symptoms of urinary tract infection. Collected sample for testing and started preliminary antibiotic treatment.\n0.710449\n\n\n7\nPatient presented with symptoms of influenza including fever, cough, and body aches. Prescribed antiviral medication and recommended rest and plenty of fluids.\n0.715153\n\n\n23\nPost-surgical patient reported redness and discomfort around stitches. Diagnosed with a mild post-operative infection. Prescribed antibiotics and wound care guidance.\n0.726964\n\n\n17\nRash observed on patient's arm. Allergenic contact dermatitis suspected. Prescribed topical steroid and recommended patch test.\n0.730986\n\n\n34\nPostnatal patient showed healthy recovery from C-section. Wound healing well without signs of infection.\n0.742733\n\n\n39\nChild patient presented with red, itchy eyes. Diagnosed as viral conjunctivitis. Instructed parents about proper eye care and hand hygiene.\n0.743466\n\n\n16\nPatient suffered a minor burn in the kitchen. Prescribed topical ointment and advised on wound care.\n0.745391\n\n\n26\nPatient presented with severe knee pain. Suspected ligament tear. Recommended MRI scan.\n0.746433\n\n\n2\nPatient came in with complaints of stomachache. Upon examination, no obvious signs of appendicitis or gallstones were found. Recommended patient to undergo an ultrasound to assess internal organs.\n0.748090\n\n\n49\nPatient presented with dislocated shoulder. After successful relocation, advised sling use and gentle mobility exercises for rehabilitation.\n0.748190\n\n\n\n\n\n\n\n\nIt definitely seems like it did a pretty good job here (although to be fair, OpenAI did all the hard lifting!)"
  },
  {
    "objectID": "blog/2024-04-11-labeling_doctors_notes.html#zero-shot-binary-classification",
    "href": "blog/2024-04-11-labeling_doctors_notes.html#zero-shot-binary-classification",
    "title": "AI Tutorial: Using Text Embeddings to Label Synthetic Doctor’s Notes Generated with ChatGPT",
    "section": "Zero-shot Binary classification",
    "text": "Zero-shot Binary classification\nThe above approach provides each doctor’s note with a score (cosine distance) relative to a target (“Patient presenting with ongoing chronic condition defined as ongoing for more than one month”). But if I wanted to use this to create a binary label, I would need to come up with a threshold so that everyone whose cosine distance to the target is below, say 0.6, is classified as “presenting with a chronic condition”, in which case, everyone whose cosine distance to the target is above 0.6 is classified as “presenting with an acute condition”.\nThat approach sounds okay, but it turns out that there’s a (hypothetically) better way.\nInstead of just computing the distance to a single target, I can compute two distances for each doctor’s note:\n\nthe distance to a positive label (corresponding to the original target: “Patient presenting with ongoing chronic condition defined as ongoing for more than one month”)\nthe distance to a negative label (“Patient presenting with acute or new condition”)\n\nThen, each note is classified based on which label text its embedding is closest to.\nThe code for doing this is very similar to the code above.\nFortunately, I don’t need to recompute the embeddings for the notes, but I do need to compute the embedding for the new negative label.\n\nlabels = [\n  'Patient presenting with ongoing chronic condition defined as ongoing for more than one month',\n  'Patient presenting with acute or new condition'\n]\n# Compute embedding of the labels\nlabel_embeddings = [get_embedding(label) for label in labels]\n\nThen I need to compute the difference between the cosine distance to the first label and the cosine distance to the second label (to determine which label the note is closest to).\n\n# compute the difference in the cosine distances to the two labels for each note\nnotes_df['cosine_score_binary'] = [cosine(x, label_embeddings[1]) - cosine(x, label_embeddings[0]) for x in notes_df['embedding']]\n\nI have saved this cosine_score_binary in a new column in the DataFrame:\n\nnotes_df[['notes', 'cosine_score_binary']].head()\n\n\n\n\n\n\n\n\n\nnotes\ncosine_score_binary\n\n\n\n\n0\nPatient presented signs of common cold; symptoms include a runny nose and mild fever. Advised rest, hydration, and over-the-counter medication to manage symptoms.\n-0.097242\n\n\n1\nPatient reports difficulty sleeping. Exhibiting signs of insomnia characterized by frequent night time awakenings. Recommended a sleep study to find underlying cause.\n0.061629\n\n\n2\nPatient came in with complaints of stomachache. Upon examination, no obvious signs of appendicitis or gallstones were found. Recommended patient to undergo an ultrasound to assess internal organs.\n-0.074738\n\n\n3\nHypertensive patient presented with higher blood pressure at 160/100. Adjusted medications and emphasized importance of diet and exercise.\n-0.009268\n\n\n4\nExamined patient for frequent headaches. No alarming neurological signs detected. Recommended reducing caffeine intake and keeping hydrated.\n-0.013830\n\n\n\n\n\n\n\n\nA positive cosine_score_binary value means that the note is closer to the first label, “Patient presenting with ongoing chronic condition defined as ongoing for more than one month” (i.e., the distance to the second label is greater than the distance to the first label), and a negative value means that the note is closer to the second label, “Patient presenting with acute or new condition”.\nI can use this to create a binary classification for each note.\n\nnotes_df['chronic_assessment'] = ['chronic' if score &gt; 0 else 'acute' for score in notes_df['cosine_score_binary']]\n\nThen I can view the results in descending order of the cosine_score_binary so that the first notes are those that are deemed most chronic and least acute:\n\nnotes_df[['notes', 'cosine_score_binary', 'chronic_assessment']].sort_values('cosine_score_binary', ascending=False)\n\n\n\n\n\n\n\n\n\nnotes\ncosine_score_binary\nchronic_assessment\n\n\n\n\n31\nChronic back pain patient showed improvement after physiotherapy. Recommended continuing sessions and daily exercises at home.\n0.110414\nchronic\n\n\n43\nPatient with chronic bronchitis showed increasingly frequent flare-ups. Reinforced importance of smoking cessation and upgraded prescribed inhalers.\n0.091766\nchronic\n\n\n19\nPatient visited for follow-up on hypertension medication. Blood pressure readings stable. Emphasized continued adherence to treatment.\n0.081715\nchronic\n\n\n1\nPatient reports difficulty sleeping. Exhibiting signs of insomnia characterized by frequent night time awakenings. Recommended a sleep study to find underlying cause.\n0.061629\nchronic\n\n\n6\nRoutine consultation with patient managing type II diabetes. Blood glucose levels fairly controlled. Advised to keep monitoring sugar levels and follow diet regimen.\n0.055009\nchronic\n\n\n11\nPatient admitted for chronic fatigue. Bloodwork ordered to rule out anemia, thyroid disorders, and diabetes. Recommended nutritional counseling.\n0.053559\nchronic\n\n\n51\nPatient complained of periodic blurry vision. Referred to ophthalmology for proper ocular examination and management.\n0.049252\nchronic\n\n\n5\nPatient came in for annual physical. All markers within normal limits. Encouraged to continue healthy lifestyle for prevention of non-communicable diseases.\n0.045713\nchronic\n\n\n25\nElderly patient with a history of diabetes complained of numbness in feet. Diagnosed peripheral neuropathy, referred to podiatrist for further management.\n0.037681\nchronic\n\n\n13\nPatient managing COPD came in for routine check-up. Oxygen levels stable. Expressed importance of avoiding respiratory irritants.\n0.035781\nchronic\n\n\n27\nPatient reported persistent cough and difficulty breathing. Referred for radiological imaging to rule out pneumonia or tuberculosis.\n0.030508\nchronic\n\n\n18\nPatient reports persistent vertigo. Referred to an Ear, Nose, and Throat (ENT) specialist for detailed evaluation to rule out Meniere's disease.\n0.028819\nchronic\n\n\n24\nPatient managed for rheumatoid arthritis reported increased inflammation in joints. Adjusted medication and recommended physiotherapy for managing symptoms.\n0.026801\nchronic\n\n\n30\nElderly patient presented with signs of Parkinson's disease. Referred to a neurologist for specialized care and management.\n0.019938\nchronic\n\n\n9\nPatient returned for post-operative wound check. Incision healing well with no signs of infection. Advised continued wound care protocol and scheduled follow-up in a week.\n0.018720\nchronic\n\n\n42\nPatient reported sleep apnea symptoms. Advised to undergo a sleep study, prescribed initial treatment using CPAP device at night during sleep.\n0.014099\nchronic\n\n\n22\nPatient with a history of heart disease complained of palpitations. Recommended further cardiology work-up and emphasized continued adherence to treatment.\n0.009364\nchronic\n\n\n37\nElderly patient exhibited signs of Alzheimer's. Informed family and referred to neurology specialist for comprehensive management.\n0.007949\nchronic\n\n\n15\nAsthmatic patient presented with increased shortness of breath. Stepped-up asthma control medication.\n0.002814\nchronic\n\n\n32\nPatient came in for weight management consultation. Diet plan adjusted and encouraged increased physical activity.\n0.001463\nchronic\n\n\n33\nPregnant patient showed normal progression in second trimester. Advised rest, proper nutrition and regular exercise.\n0.000649\nchronic\n\n\n12\nElderly patient with poor vision due to cataracts. Scheduled for ophthalmology consultation.\n-0.000179\nacute\n\n\n36\nTeenage patient came in with signs of acne. Prescribed topical retinoid and advised consistent skin care routine.\n-0.000709\nacute\n\n\n41\nPatient presented with painful and swollen joints. Diagnosed as gout and recommended dietary adjustment along with pain management.\n-0.004671\nacute\n\n\n21\nPatient has concerning mole on the back. Scheduled for dermatology consult to rule out potential skin cancer.\n-0.007328\nacute\n\n\n3\nHypertensive patient presented with higher blood pressure at 160/100. Adjusted medications and emphasized importance of diet and exercise.\n-0.009268\nacute\n\n\n38\nPatient with history of irritable bowel syndrome showed signs of relief with diet modification. Reinforced importance of hydration and fiber intake.\n-0.011980\nacute\n\n\n4\nExamined patient for frequent headaches. No alarming neurological signs detected. Recommended reducing caffeine intake and keeping hydrated.\n-0.013830\nacute\n\n\n28\nPatient presented with severe migraine episodes. Prescribed stronger painkillers and referred to a neurologist for a complete neurological work-up.\n-0.015010\nacute\n\n\n52\nElderly patient with hearing difficulty. Scheduled an appointment for audiology consultation and potential hearing aids.\n-0.015633\nacute\n\n\n45\nPatient reported feeling low, indicating depression. Counseled and referred to mental health services for further treatment and support.\n-0.018623\nacute\n\n\n35\nPatient presented with swollen (oedemic) ankles. Adjusted diuretic medication dosage and recommended follow-up visit next week.\n-0.020730\nacute\n\n\n47\nType 1 diabetic patient showed increased blood sugar levels. Insulin doses were adjusted, advised dietary check.\n-0.020986\nacute\n\n\n17\nRash observed on patient's arm. Allergenic contact dermatitis suspected. Prescribed topical steroid and recommended patch test.\n-0.030286\nacute\n\n\n34\nPostnatal patient showed healthy recovery from C-section. Wound healing well without signs of infection.\n-0.032995\nacute\n\n\n8\nArthritic patient reports increased joint pain. Advised gentle physical therapy and adjustment of pain medication.\n-0.033021\nacute\n\n\n48\nChild patient presented with symptoms of ADHD. Referred to pediatric psychiatrist for cognitive and behavioral evaluation plus treatment.\n-0.038751\nacute\n\n\n53\nPatient presented with signs of Bell's Palsy. Referred to neurologist and prescribed corticosteroid medication to reduce inflammation.\n-0.042732\nacute\n\n\n46\nPatient with anxiety showed improvement with recent medication schedule. Encouraged continued therapy and stress management techniques.\n-0.048093\nacute\n\n\n44\nPatient with history of stroke presented with dizziness. Follow-up brain scan scheduled to rule out recurrence.\n-0.053002\nacute\n\n\n14\nPatient presented with signs of depression. Recommended seeking a psychologist for cognitive behavioral therapy and prescribed low dose antidepressant.\n-0.057786\nacute\n\n\n29\nPatient suffering from high fevers and weight loss. Ordered comprehensive bloodwork to assess for possible infectious disease.\n-0.059315\nacute\n\n\n16\nPatient suffered a minor burn in the kitchen. Prescribed topical ointment and advised on wound care.\n-0.062896\nacute\n\n\n23\nPost-surgical patient reported redness and discomfort around stitches. Diagnosed with a mild post-operative infection. Prescribed antibiotics and wound care guidance.\n-0.068201\nacute\n\n\n2\nPatient came in with complaints of stomachache. Upon examination, no obvious signs of appendicitis or gallstones were found. Recommended patient to undergo an ultrasound to assess internal organs.\n-0.074738\nacute\n\n\n39\nChild patient presented with red, itchy eyes. Diagnosed as viral conjunctivitis. Instructed parents about proper eye care and hand hygiene.\n-0.077558\nacute\n\n\n10\nPatient presented with chest pain. Preliminary assessment ruled out heart conditions. Recommended gastroenterology consult to check for possible reflux disease.\n-0.080826\nacute\n\n\n26\nPatient presented with severe knee pain. Suspected ligament tear. Recommended MRI scan.\n-0.081825\nacute\n\n\n20\nHistorically anxious patient reported increased panic attacks recently. Referred to psychiatrist for therapeutic support, prescribed low-dose anxiolytic.\n-0.083702\nacute\n\n\n49\nPatient presented with dislocated shoulder. After successful relocation, advised sling use and gentle mobility exercises for rehabilitation.\n-0.091579\nacute\n\n\n0\nPatient presented signs of common cold; symptoms include a runny nose and mild fever. Advised rest, hydration, and over-the-counter medication to manage symptoms.\n-0.097242\nacute\n\n\n7\nPatient presented with symptoms of influenza including fever, cough, and body aches. Prescribed antiviral medication and recommended rest and plenty of fluids.\n-0.100791\nacute\n\n\n40\nPatient showed symptoms of urinary tract infection. Collected sample for testing and started preliminary antibiotic treatment.\n-0.101196\nacute\n\n\n50\nPregnant patient at full term demonstrated normal signs. Advised to prepare for labor and familiarize with signs of onset.\n-0.101702\nacute\n\n\n\n\n\n\n\n\nWhile the results aren’t perfect, they’re certainly pretty impressive! Especially given how little code we had to write to achieve it!\nHopefully, this tutorial has given you some ideas to try in your own work. But keep in mind that this is a rapidly evolving field, and the specific pieces of code used here may or may not work in the future as things continue to shift."
  },
  {
    "objectID": "blog/2023-03-21-quarto-website.html#footnotes",
    "href": "blog/2023-03-21-quarto-website.html#footnotes",
    "title": "Thanks, Quarto, for saving my blog!",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nwhich mostly involves writing the same paragraph in five different ways before realizing that the point I’m trying to make is irrelevant and subsequently deleting the whole thing↩︎\nI know that this implies that I only blog in my spare time, but if I’m being honest, I don’t think I’ve ever actually blogged in my spare time. I only ever blog during work hours, like the responsible employee that I am. I’m actually at work right now. But shhhh.↩︎"
  },
  {
    "objectID": "blog/2017-06-29-website.html#goodbye-github-pages-i-hope-youre-not-too-upset",
    "href": "blog/2017-06-29-website.html#goodbye-github-pages-i-hope-youre-not-too-upset",
    "title": "Migrating from GitHub Pages to Netlify: how and why?",
    "section": "",
    "text": "After finding myself increasingly frustrated with GitHub Pages inability to cooperate with my website engine of choice, Hugo, I’ve decided to make a move. Here are the reasons why:\n\nHaving all of my project pages being subpages of my personal page felt weird.\nDealing with git subtrees, merge conflicts on the master branch, and having to do all kinds of work-arounds to get Hugo to play nice with GitHub Pages was driving me crazy.\nI found a much easier way!"
  },
  {
    "objectID": "blog/2017-07-05-confounding.html#footnotes",
    "href": "blog/2017-07-05-confounding.html#footnotes",
    "title": "Confounding in causal inference: what is it, and what to do about it?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI know the \\(\\perp\\) symbol means perpendicular not independent, but I couldn’t be bothered figuring out how to add LaTeX preamble to markdown to define the double vertical line independent symbol. Don’t judge me!↩︎"
  }
]