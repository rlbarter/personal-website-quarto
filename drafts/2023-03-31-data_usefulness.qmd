---
title: 'Data usefulness as a function of "correctness and relevance"'
author: "Rebecca Barter"
format:
  html:
    toc: true
    toc-location: left
categories: [data science, data, data usefulness]
date: 2023-04-03
image: img/data_usefulness/four_c.jpg
description: ""
draft: true
editor_options: 
  chunk_output_type: console
---

![](img/data_usefulness/four_c.jpg){fig-alt='A logo for the data usefulness'}

# What is "data usefulness"?

Every data science project starts with a real-world question and a dataset that can (hopefully!) be used to find some insights that answer the question. But all datasets are not made equal, and the ability of your particular dataset to lead us to a reliable and trustworthy answer depends not just on your data analysis skills, but also on the *usefulness* of the data that you're working with. 

"*Data usefulness*" is a proxy for how well the data reflects the underlying reality it is intended to represent as well as how easy it is to work with. Data usefulness should always be evaluated in the context of the particular problem that you're trying to solve or question you're trying to answer. After all, if your data is not reflecting reality, or isn't relevant to the problem you're trying to solve, then it's not going to be of much use now, is it?

Examples of data that has low usefulness include data whose values are not *correct*, i.e., don't accurately reflect the real-world quantities they are supposed to capture (e.g., measuring the temperature with a broken thermometer), as well as data that is not *current*, e.g., because it is missing relevant data points (e.g., a "public opinion" survey that was only taken by one your own followers on social media) or is *outdated* (e.g., using polls from 2023 to predict the outcome of an election in 2024). While the inconsistent formatting is often easy to fix, the other issues aren't.

In this blog post, I will introduce a technique for assessing whether your data are useful based on whether your data is:

1. **Correct**: Are the values in the data *correct*, i.e., are they accurately reflecting the real-world quantities they were designed to capture? 

<!-- 1. **Complete**ness: Does the data contain all of the relevant information needed to answer your question? -->

<!-- 1. **Consist**ency: Was the data collected and formatted in a consistent way? -->

1. **Relevant**: Is the data up-to-date and relevant to the situation that you plan to apply it to? And are the observations in your data reflective of the population of interest?

These measures are more *qualitative* than they are *quantitative*, but they can be really helpful for giving you a sense of your data's potential to provide insights for your real-world problem.


# The four "C"s for assessing data quality

By way of an example, let's assess the usefulness of the CDC's [NHANES survey data](https://www.cdc.gov/nchs/nhanes/index.htm) for answering the question of "*is a relationship between inactivity and depression?*". If you're interested, the specific version of the data that I'm using can be downloaded from [Kaggle](https://www.kaggle.com/datasets/cdc/national-health-and-nutrition-examination-survey). 


## Correctness

*Correctness is a measure of whether the values reported in the data are an accurate reflection of the real-world quantity they are designed to measure.*

Let's focus on the `PAD680` variable from the `questionnaire.csv` file in the NHANES data, which contains the responses to the question of "*How much time do you usually spend sitting on a typical day?*" The answers to this question are reported in minutes, and range from 0 to 1200. While there are also encoded options for "don't know" (`9999`) and "refused" (`7777`), around 30\% of the values are missing (`NA`). 

The table below shows the distribution of the types of responses for this variable, where we categorize a non-missing value that is less than 1200 as "reported". Note that fewer than 1% have a "don't know" or "refused" response.


```{r}
#| echo: false
#| results: hide
#| warning: false
#| message: false
library(tidyverse)
library(gt)
# load in the questionnaire data from my github repo
question <- read_csv("https://raw.githubusercontent.com/rlbarter/personal-website-quarto/main/blog/data/nhanes_data/questionnaire.csv")
# create a table for each response type
question |> 
  summarise(dont_know = sum(PAD680 == 9999, na.rm = TRUE),
            refused = sum(PAD680 == 7777, na.rm = TRUE),
            missing = sum(is.na(PAD680)),
            reported = sum(sum(PAD680 <= 1200, na.rm = TRUE))) |>
  gt()
```

The histogram in Figure \@ref(fig:inactivity-hist) shows the distribution of the participants answers to the question "*How much time do you usually spend sitting on a typical day?*" (in minutes), for the participants whose answer was reported in the data. A 

```{r}
#| label: inactivity-hist
#| caption: "A histogram showing the distribution of responses to the NHANES question 'How much time do you usually spend sitting on a typical day?', with a ghost column added to convey the number of missing values."
#| message: false
#| warning: false
question |>
  mutate(missing = is.na(PAD680)) |>
  mutate(PAD680 = if_else(is.na(PAD680), -200, PAD680)) |>
  filter(PAD680 < 1200) |>
  ggplot() +
  geom_histogram(aes(x = PAD680, fill = missing), col = "white", bins = 20) +
  geom_vline(xintercept = -100, linetype = "dashed") +
  geom_text(aes(x = -200, y = 1500, label = lab), 
            tribble(~x, ~y, ~lab,
                    -200, 1500, "Missing values (NA)"),
            angle = 90, hjust = "center", vjust = "center", size = 5, nudge_x = -12.5) +
  theme_classic() +
  scale_y_continuous(expand = c(0, 0)) +
  scale_fill_manual(values = c("grey20", "grey70"), guide = "none") +
  labs(y = "Number of responses", x = "Answer to the question 'How much time do you usually spend sitting on a typical day?'") +
  theme(panel.background = element_rect(fill = "transparent", colour = NA),  
        plot.background = element_rect(fill = "transparent", colour = NA))
```


**How well do you think this variable (`PAD680`) is reflecting reality?** 

To answer this question, we need to first ask what real-world quantity is this variable trying to capture? Fortunately, this is fairly clear from the question ("How much time do you usually spend sitting on a typical day?"): this variable is trying to capture is the amount of time each person spends sitting in a typical day (note that the question specifically says that this should not include time sleeping).

Let's start with the 30% of values that are missing. Do you think that these values are reflecting reality? Since a missing value does not capture the amount of time each person spent sitting in a typical day, these missing values are *not* reflecting reality. Presumably these values are missing because the individual refused to or did not want to provide an estimate of how much time they spent sitting on any given day, but this begs the question of why are these values are recorded as *missing*, rather than "refused" or "don't know"? (Fewer than 1% of answers were "refused" or "don't know", which implies a potential inconsistency in the way that the data are being collected by the survey takers. We'll get to consistency below.)

What about the non-missing values? Do you think that these values are "correct" in the sense that they are accurately reflecting the actual amount of time each person in the study spends sitting on a typical day? Let me ask you a question: **_how much time do you usually spend sitting on a typical day?_** Do you know an exact answer to this question, or did you provide a general approximation? Since this data was not collected by actually *observing* how much each person sits on a typical day, and it's really hard to actually guess exactly how much time you spend sitting on a typical day, these values are, at best, vague ball-park-figures that are deeply influenced by flawed self-perception and a fear of being judged on their answer.

In this example, even the *non-missing* values are unlikely to be "correct". Don't lose all hope yet, however! This doesn't mean that you can't analyze your data, it just means that you need to be careful to remember and communicate that the results are based on potentially inaccurate *self-reported measures* of inactivity that are not perfect reflections of reality. 

In terms of your downstream results, this means that you should not to take any *specific numeric* conclusions that you arrive at (e.g., people who spend more than half of their waking hours sitting are more likely to be depressed) too seriously. These results can certainly be used to indicate a general trend (although note that there is nothing *causal* about this study), but will necessarily not be proof of a real-world quantity. 

Note that if you want to *test* the stability of your conclusions to your data, check out Bin Yu's [*Veridical Data Science*](https://www.pnas.org/doi/10.1073/pnas.1901326117) framework (and keep an eye out for the "Veridical Data Science" book written by Bin Yu and myself!) for some advice on how to do that.




## Completeness

**Completeness is a measure of whether the data contains all of the relevant information needed to answer your question.**




## Consistency

**Completeness is a measure of whether the values in your data have been collected and reported in a consistent way.**


## Currency

**Currency is a measure of whether the values in your data are current, i.e., relevant to the real-world scenario in which you plan to apply your results.**




# Can data cleaning help?

While data cleaning can address consistency (e.g., by reformatting inconsistently recorded data values) correctness to some extent (e.g., by replacing invalid or incorrect values with more correct values when they are known), data cleaning cannot fix a lack of completeness or currency.

