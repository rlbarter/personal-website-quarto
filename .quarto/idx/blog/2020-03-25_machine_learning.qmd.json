{"title":"Tidymodels: tidy machine learning in R","markdown":{"yaml":{"title":"Tidymodels: tidy machine learning in R","author":"Rebecca Barter","format":{"html":{"toc":true,"toc-location":"left"}},"categories":["R","tidyverse","machine learning","tidymodels","caret","recipes","parsnip","tune","rsample"],"date":"2020-04-14","description":"The tidyverse's take on machine learning is finally here. Tidymodels forms the basis of tidy machine learning, and this post provides a whirlwind tour to get you started."},"headingText":"What is tidymodels","containsRefs":false,"markdown":"\n\nThere's a new modeling pipeline in town: tidymodels. Over the past few years, tidymodels has been gradually emerging as the tidyverse's machine learning toolkit. \n\nWhy tidymodels? Well, it turns out that R has a consistency problem. Since everything was made by different people and using different principles, everything has a slightly different interface, and trying to keep everything in line can be frustrating. Several years ago, [Max Kuhn](https://twitter.com/topepos) (formerly at Pfeizer, now at RStudio) developed the caret R package (see my [caret tutorial](http://www.rebeccabarter.com/blog/2017-11-17-caret_tutorial/)) aimed at creating a uniform interface for the massive variety of machine learning models that exist in R. Caret was great in a lot of ways, but also limited in others. In my own use, I found it to be quite slow whenever I tried to use on problems of any kind of modest size. \n\n\nThat said, caret was a great starting point, so RStudio hired Max Kuhn to work on a tidy version of caret, and he and many other people have developed what has become tidymodels. Tidymodels has been in development for a few years, with snippets of it being released as they were developed (see my [post on the recipes package](http://www.rebeccabarter.com/blog/2019-06-06_pre_processing/)). I've been holding off writing a post about tidymodels until it seemed as though the different pieces fit together sufficiently for it to all feel cohesive. I feel like they're finally there - which means it is time for me to learn it! While caret isn't going anywhere (you can continue to use caret, and your existing caret code isn't going to stop working), tidymodels will eventually make it redundant. \n\n\nThe main resources I used to learn tidymodels were Alison Hill's slides from [Introduction to Machine Learning with the Tidyverse](https://education.rstudio.com/blog/2020/02/conf20-intro-ml/), which contains all the slides for the course she prepared with Garrett Grolemund for RStudio::conf(2020), and Edgar Ruiz's [Gentle introduction to tidymodels](https://rviews.rstudio.com/2019/06/19/a-gentle-intro-to-tidymodels/) on the RStudio website.\n\n\nNote that throughout this post I'll be assuming basic tidyverse knowledge, primarily of dplyr (e.g. piping `%>%` and function such as `mutate()`). Fortunately, for all you purrr-phobes out there, purrr is *not* required. If you'd like to brush up on your tidyverse skills, check out my [Introduction to the Tidyverse](http://www.rebeccabarter.com/blog/2019-08-05_base_r_to_tidyverse/) posts. If you'd like to learn purrr (purrr is very handy for working with tidymodels but is no longer a requirement), check out my [purrr post](http://www.rebeccabarter.com/blog/2019-08-19_purrr/).\n\n\n\n\n\n\nMuch like the tidyverse consists of many core packages, such as ggplot2 and dplyr, tidymodels also consists of several core packages, including \n\n- `rsample`: for sample splitting (e.g. train/test or cross-validation)\n\n- `recipes`: for pre-processing \n\n- `parsnip`: for specifying the model\n\n- `yardstick`: for evaluating the model\n\n\n\nSimilarly to how you can load the entire tidyverse suite of packages by typing `library(tidyverse)`, you can load the entire tidymodels suite of packages by typing `library(tidymodels)`.\n\nWe will also be using the `tune` package (for parameter tuning procedure) and the `workflows` package (for putting everything together) that I had thought were a part of CRAN's tidymodels package bundle, but apparently they aren't. These will need to be loaded separately for now.\n\n\n\nUnlike in my [tidyverse post](http://www.rebeccabarter.com/blog/2019-08-05_base_r_to_tidyverse/), I *won't* base this post around the packages themselves, but I will mention the packages in passing.\n\n# Getting set up\n\n\nFirst we need to load some libraries: `tidymodels` and `tidyverse`. \n\n```{r, message=FALSE}\n# load the relevant tidymodels libraries\nlibrary(tidymodels)\nlibrary(tidyverse)\nlibrary(workflows)\nlibrary(tune)\n```\n\n\nIf you don't already have the tidymodels library (or any of the other libraries) installed, then you'll need to install it (once only) using `install.packages(\"tidymodels\")`.\n\n\nWe will use the Pima Indian Women's diabetes dataset which contains information on 768 Pima Indian women's diabetes status, as well as many predictive features such as the number of pregnancies (pregnant), plasma glucose concentration (glucose), diastolic blood pressure (pressure), triceps skin fold thickness (triceps), 2-hour serum insulin (insulin), BMI (mass), diabetes pedigree function (pedigree), and their age (age). In case you were wondering, the [Pima Indians](https://en.wikipedia.org/wiki/Pima_people) are a group of Native Americans living in an area consisting of what is now central and southern Arizona. The short name, \"Pima\" is believed to have come from a phrase meaning \"I don't know,\" which they used repeatedly in their initial meetings with Spanish colonists. Thanks Wikipedia!\n\n```{r}\n# load the Pima Indians dataset from the mlbench dataset\nlibrary(mlbench)\ndata(PimaIndiansDiabetes)\n# rename dataset to have shorter name because lazy\ndiabetes_orig <- PimaIndiansDiabetes\n```\n\n```{r}\ndiabetes_orig\n```\n\n\nA quick exploration reveals that there are more zeros in the data than expected (especially since a BMI or tricep skin fold thickness of 0 is impossible), implying that missing values are recorded as zeros. See for instance the histogram of the tricep skin fold thickness, which has a number of 0 entries that are set apart from the other entries. \n\n```{r, fig.align=\"center\"}\nggplot(diabetes_orig) +\n  geom_histogram(aes(x = triceps))\n```\n\nThis phenomena can also seen in the glucose, pressure, insulin and mass variables. Thus, we convert the 0 entries in all variables (other than \"pregnant\") to `NA`. To do that, we use the `mutate_at()` function (which will soon be superseded by `mutate()` with `across()`) to specify which variables we want to apply our mutating function to, and we use the `if_else()` function to specify what to replace the value with if the condition is true or false.\n\n```{r}\ndiabetes_clean <- diabetes_orig %>%\n  mutate_at(vars(triceps, glucose, pressure, insulin, mass), \n            function(.var) { \n              if_else(condition = (.var == 0), # if true (i.e. the entry is 0)\n                      true = as.numeric(NA),  # replace the value with NA\n                      false = .var # otherwise leave it as it is\n                      )\n            })\n```\n\nOur data is ready. Hopefully you've replenished your cup of tea (or coffee if you're into that for some reason). Let's start making some tidy models!\n\n# Split into train/test\n\nFirst, let's split our dataset into training and testing data. The training data will be used to fit our model and tune its parameters, where the testing data will be used to evaluate our final model's performance.\n\nThis split can be done automatically using the `inital_split()` function (from `rsample`) which creates a special \"split\" object. \n\n```{r}\nset.seed(234589)\n# split the data into trainng (75%) and testing (25%)\ndiabetes_split <- initial_split(diabetes_clean, \n                                prop = 3/4)\ndiabetes_split\n```\n\nThe printed output of `diabetes_split`, our split object, tells us how many observations we have in the training set, the testing set, and overall: `<train/test/total>`.\n\nThe training and testing sets can be extracted from the \"split\" object using the `training()` and `testing()` functions. Although, we won't actually use these objects in the pipeline (we will be using the `diabetes_split` object itself).\n\n```{r}\n# extract training and testing sets\ndiabetes_train <- training(diabetes_split)\ndiabetes_test <- testing(diabetes_split)\n```\n\nAt some point we're going to want to do some parameter tuning, and to do that we're going to want to use cross-validation. So we can create a cross-validated version of the training set in preparation for that moment using `vfold_cv()`.\n\n\n```{r}\n# create CV object from training data\ndiabetes_cv <- vfold_cv(diabetes_train)\n```\n\n\n\n# Define a recipe\n\nRecipes allow you to specify the role of each variable as an outcome or predictor variable (using a \"formula\"), and any pre-processing steps you want to conduct (such as normalization, imputation, PCA, etc).\n\nCreating a recipe has two parts (layered on top of one another using pipes `%>%`):\n\n1. **Specify the formula** (`recipe()`): specify the outcome variable and predictor variables\n\n1. **Specify pre-processing steps** (`step_zzz()`): define the pre-processing steps, such as imputation, creating dummy variables, scaling, and more\n\n\nFor instance, we can define the following recipe \n\n```{r}\n# define the recipe\ndiabetes_recipe <- \n  # which consists of the formula (outcome ~ predictors)\n  recipe(diabetes ~ pregnant + glucose + pressure + triceps + \n           insulin + mass + pedigree + age, \n         data = diabetes_clean) %>%\n  # and some pre-processing steps\n  step_normalize(all_numeric()) %>%\n  step_impute_knn(all_predictors())\n```\n\nIf you've ever seen formulas before (e.g. using the `lm()` function in R), you might have noticed that we could have written our formula much more efficiently using the formula short-hand where `.` represents all of the variables in the data: `outcome ~ .` will fit a model that predicts the outcome using *all other columns*.\n\nThe full list of pre-processing steps available can be found [here](https://tidymodels.github.io/recipes/articles/Custom_Steps.html). In the recipe steps above we used the functions `all_numeric()` and `all_predictors()` as arguments to the pre-processing steps. These are called \"role selections\", and they specify that we want to apply the step to \"all numeric\" variables or \"all predictor variables\". The list of all potential role selectors can be found by typing `?selections` into your console.\n\n\nNote that we used the original `diabetes_clean` data object (we set `recipe(..., data = diabetes_clean)`), rather than the `diabetes_train` object or the `diabetes_split` object. It turns out we could have used any of these. All recipes takes from the data object at this point is the *names and roles* of the outcome and predictor variables. We will apply this recipe to specific datasets later. This means that for large data sets, the head of the data could be used to pass the recipe a smaller data set to save time and memory.\n\n\nIndeed, if we print a summary of the `diabetes_recipe` object, it just shows us how many predictor variables we've specified and the steps we've specified (but it doesn't actually implement them yet!). \n\n```{r}\ndiabetes_recipe\n```\n\n\nIf you want to extract the pre-processed dataset itself, you can first `prep()` the recipe for a specific dataset and `juice()` the prepped recipe to extract the pre-processed data. It turns out that extracting the pre-processed data isn't actually necessary for the pipeline, since this will be done under the hood when the model is fit, but sometimes it's useful anyway.\n\n```{r}\ndiabetes_train_preprocessed <- diabetes_recipe %>%\n  # apply the recipe to the training data\n  prep(diabetes_train) %>%\n  # extract the pre-processed training dataset\n  juice()\ndiabetes_train_preprocessed\n```\n\n\nI wrote a much longer [post on recipes](http://www.rebeccabarter.com/blog/2019-06-06_pre_processing/) if you'd like to check out more details. However, note that the preparation and bake steps described in that post are no longer necessary in the tidymodels pipeline, since they're now implemented under the hood by the later model fitting functions in this pipeline.\n\n\n\n\n# Specify the model\n\n\nSo far we've split our data into training/testing, and we've specified our pre-processing steps using a recipe. The next thing we want to specify is our model (using the `parsnip` package).\n\nParsnip offers a unified interface for the massive variety of models that exist in R. This means that you only have to learn one way of specifying a model, and you can use this specification and have it generate a linear model, a random forest model, a support vector machine model, and more with a single line of code.\n\nThere are a few primary components that you need to provide for the model specification\n\n\n1. The **model type**: what kind of model you want to fit, set using a different function depending on the model, such as `rand_forest()` for random forest, `logistic_reg()` for logistic regression, `svm_poly()` for a polynomial SVM model etc. The full list of models available via parsnip can be found [here](https://tidymodels.github.io/parsnip/articles/articles/Models.html).\n\n1. The **arguments**: the model parameter values (now consistently named across different models), set using `set_args()`.\n\n1. The **engine**: the underlying package the model should come from (e.g. \"ranger\" for the ranger implementation of Random Forest), set using `set_engine()`.\n\n1. The **mode**: the type of prediction - since several packages can do both classification (binary/categorical prediction) and regression (continuous prediction), set using `set_mode()`.\n\n\nFor instance, if we want to fit a random forest model as implemented by the `ranger` package for the purpose of classification and we want to tune the `mtry` parameter (the number of randomly selected variables to be considered at each split in the trees), then we would define the following model specification:\n\n\n```{r}\nrf_model <- \n  # specify that the model is a random forest\n  rand_forest() %>%\n  # specify that the `mtry` parameter needs to be tuned\n  set_args(mtry = tune()) %>%\n  # select the engine/package that underlies the model\n  set_engine(\"ranger\", importance = \"impurity\") %>%\n  # choose either the continuous regression or binary classification mode\n  set_mode(\"classification\") \n```\n\nIf you want to be able to examine the variable importance of your final model later, you will need to set `importance` argument when setting the engine. For ranger, the importance options are `\"impurity\"` or `\"permutation\"`.\n\nAs another example, the following code would instead specify a logistic regression model from the `glm` package.\n\n```{r}\nlr_model <- \n  # specify that the model is a random forest\n  logistic_reg() %>%\n  # select the engine/package that underlies the model\n  set_engine(\"glm\") %>%\n  # choose either the continuous regression or binary classification mode\n  set_mode(\"classification\") \n```\n\n\nNote that this code doesn't actually fit the model. Like the recipe, it just outlines a description of the model. Moreover, setting a parameter to `tune()` means that it will be tuned later in the tune stage of the pipeline (i.e. the value of the parameter that yields the best performance will be chosen). You could also just specify a particular value of the parameter if you don't want to tune it e.g. using `set_args(mtry = 4)`.\n\nAnother thing to note is that nothing about this model specification is specific to the diabetes dataset.\n\n# Put it all together in a workflow\n\nWe're now ready to put the model and recipes together into a workflow. You initiate a workflow using `workflow()` (from the `workflows` package) and then you can add a recipe and add a model to it.\n\n\n```{r}\n# set the workflow\nrf_workflow <- workflow() %>%\n  # add the recipe\n  add_recipe(diabetes_recipe) %>%\n  # add the model\n  add_model(rf_model)\n```\n\n\nNote that we still haven't yet implemented the pre-processing steps in the recipe nor have we fit the model. We've just written the framework. It is only when we tune the parameters or fit the model that the recipe and model frameworks are actually implemented.\n\n# Tune the parameters\n\nSince we had a parameter that we designated to be tuned (`mtry`), we need to tune it (i.e. choose the value that leads to the best performance) before fitting our model. If you don't have any parameters to tune, you can skip this step.\n\nNote that we will do our tuning using the cross-validation object (`diabetes_cv`). To do this, we specify the range of `mtry` values we want to try, and then we add a tuning layer to our workflow using `tune_grid()` (from the `tune` package). Note that we focus on two metrics: `accuracy` and `roc_auc` (from the `yardstick` package).\n\n```{r}\n# specify which values eant to try\nrf_grid <- expand.grid(mtry = c(3, 4, 5))\n# extract results\nrf_tune_results <- rf_workflow %>%\n  tune_grid(resamples = diabetes_cv, #CV object\n            grid = rf_grid, # grid of values to try\n            metrics = metric_set(accuracy, roc_auc) # metrics we care about\n            )\n```\n\nYou can tune multiple parameters at once by providing multiple parameters to the `expand.grid()` function, e.g. `expand.grid(mtry = c(3, 4, 5), trees = c(100, 500))`.\n\nIt's always a good idea to explore the results of the cross-validation. `collect_metrics()` is a really handy function that can be used in a variety of circumstances to extract any metrics that have been calculated within the object it's being used on. In this case, the metrics come from the cross-validation performance across the different values of the parameters.\n\n```{r}\n# print results\nrf_tune_results %>%\n  collect_metrics()\n```\n\nAcross both accuracy and AUC, `mtry = 4` yields the best performance (*just*).\n\n\n# Finalize the workflow\n\nWe want to add a layer to our workflow that corresponds to the tuned parameter, i.e. sets `mtry` to be the value that yielded the best results. If you didn't tune any parameters, you can skip this step.\n\nWe can extract the best value for the accuracy metric by applying the `select_best()` function to the tune object.\n\n```{r}\nparam_final <- rf_tune_results %>%\n  select_best(metric = \"accuracy\")\nparam_final\n```\n\nThen we can add this parameter to the workflow using the `finalize_workflow()` function.\n\n```{r}\nrf_workflow <- rf_workflow %>%\n  finalize_workflow(param_final)\n```\n\n# Evaluate the model on the test set\n\nNow we've defined our recipe, our model, and tuned the model's parameters, we're ready to actually fit the final model. Since all of this information is contained within the workflow object, we will apply the `last_fit()` function to our workflow and our train/test split object. This will automatically train the model specified by the workflow using the training data, and produce evaluations based on the test set.\n\n```{r}\nrf_fit <- rf_workflow %>%\n  # fit on the training set and evaluate on test set\n  last_fit(diabetes_split)\n```\n\n\nNote that the fit object that is created is a data-frame-like object; specifically, it is a tibble with list columns.\n\n```{r}\nrf_fit\n```\n\nThis is a really nice feature of tidymodels (and is what makes it work so nicely with the tidyverse) since you can do all of your tidyverse operations to the model object. While truly taking advantage of this flexibility requires proficiency with purrr, if you don't want to deal with purrr and list-columns, there are functions that can extract the relevant information from the fit object that remove the need for purrr as we will see below.\n\n\nSince we supplied the train/test object when we fit the workflow, the metrics are evaluated on the *test* set. Now when we use the `collect_metrics()` function (recall we used this when tuning our parameters), it extracts the performance of the final model (since `rf_fit` now consists of a single final model) applied to the *test* set.\n\n\n```{r}\ntest_performance <- rf_fit %>% collect_metrics()\ntest_performance\n```\n\nOverall the performance is very good, with an accuracy of 0.74 and an AUC of 0.82.\n\nYou can also extract the test set predictions themselves using the `collect_predictions()` function. Note that there are 192 rows in the predictions object below which matches the number of *test set* observations (just to give you some evidence that these are based on the test set rather than the training set).\n\n\n```{r}\n# generate predictions from the test set\ntest_predictions <- rf_fit %>% collect_predictions()\ntest_predictions\n```\n\n\nSince this is just a normal data frame/tibble object, we can generate summaries and plots such as a confusion matrix.\n\n```{r}\n# generate a confusion matrix\ntest_predictions %>% \n  conf_mat(truth = diabetes, estimate = .pred_class)\n```\n\nWe could also plot distributions of the predicted probability distributions for each class.\n\n```{r}\ntest_predictions %>%\n  ggplot() +\n  geom_density(aes(x = .pred_pos, fill = diabetes), \n               alpha = 0.5)\n```\n\n\n\n\nIf you're familiar with purrr, you could use purrr functions to extract the predictions column using `pull()`. The following code does almost the same thing as `collect_predictions()`. You could similarly have done this with the `.metrics` column.\n\n```{r}\ntest_predictions <- rf_fit %>% pull(.predictions)\ntest_predictions\n```\n\n\n# Fitting and using your final model\n\nThe previous section evaluated the model trained on the training data using the testing data. But once you've determined your final model, you often want to train it on your full dataset and then use it to predict the response for *new* data.\n\n\nIf you want to use your model to predict the response for new observations, you need to use the `fit()` function on your workflow and the dataset that you want to fit the final model on (e.g. the complete training + testing dataset).\n\n```{r}\nfinal_model <- fit(rf_workflow, diabetes_clean)\n```\n\nThe `final_model` object contains a few things including the ranger object trained with the parameters established through the workflow contained in `rf_workflow` based on the data in `diabetes_clean` (the combined training and testing data).\n\n\n```{r}\nfinal_model\n```\n\nIf we wanted to predict the diabetes status of a new woman, we could use the normal `predict()` function.\n\nFor instance, below we define the data for a new woman.\n\n```{r}\nnew_woman <- tribble(~pregnant, ~glucose, ~pressure, ~triceps, ~insulin, ~mass, ~pedigree, ~age,\n                     2, 95, 70, 31, 102, 28.2, 0.67, 47)\nnew_woman\n```\n\nThe predicted diabetes status of this new woman is \"negative\".\n\n```{r}\npredict(final_model, new_data = new_woman)\n```\n\n\n# Variable importance \n\nIf you want to extract the variable importance scores from your model, as far as I can tell, for now you need to extract the model object from the `fit()` object (which for us is called `final_model`). The function that extracts the model is `pull_workflow_fit()` and then you need to grab the `fit` object that the output contains.\n\n```{r}\nranger_obj <- pull_workflow_fit(final_model)$fit\nranger_obj\n```\n\n\nThen you can extract the variable importance from the ranger object itself (`variable.importance` is a specific object contained within ranger output - this will need to be adapted for the specific object type of other models).\n\n```{r}\nranger_obj$variable.importance\n```\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../styles.css"],"toc":true,"output-file":"2020-03-25_machine_learning.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.280","theme":"simplex","linkcolor":"#6633c4","code-copy":true,"footnotes-hover":true,"title-block-banner":true,"comments":{"utterances":{"repo":"rlbarter/blog_comments"}},"title":"Tidymodels: tidy machine learning in R","author":"Rebecca Barter","categories":["R","tidyverse","machine learning","tidymodels","caret","recipes","parsnip","tune","rsample"],"date":"2020-04-14","description":"The tidyverse's take on machine learning is finally here. Tidymodels forms the basis of tidy machine learning, and this post provides a whirlwind tour to get you started.","toc-location":"left"},"extensions":{"book":{"multiFile":true}}}}}