{"title":"A basic tutorial of caret: the machine learning package in R","markdown":{"yaml":{"title":"A basic tutorial of caret: the machine learning package in R","author":"Rebecca Barter","categories":["R","machine learning"],"format":{"html":{"toc":true,"toc-location":"left"}},"date":"2017-11-17","freeze":true,"description":"R has a wide number of packages for machine learning (ML), which is great, but also quite frustrating since each package was designed independently and has very different syntax, inputs and outputs. Caret unifies these packages into a single package with constant syntax, saving everyone a lot of frustration and time!"},"headingText":"A simple view of caret: the default `train` function","containsRefs":false,"markdown":"\n\nNote: If you're new to caret, I suggest learning tidymodels instead [http://www.rebeccabarter.com/blog/2020-03-25_machine_learning/](http://www.rebeccabarter.com/blog/2020-03-25_machine_learning/). Tidymodels is essentially caret's successor. Don't worry though, your caret code will still work!\n\nOlder note: This tutorial was based on an older version of the abalone data that had a binary `old` varibale rather than a numeric `age` variable. It has been modified lightly so that it uses a manual `old` variable (is the abalone older than 10 or not) and ignores the numeric `age` variable.\n\nMaterials prepared by Rebecca Barter. Package developed by Max Kuhn.\n\nAn interactive Jupyter Notebook version of this tutorial can be found at https://github.com/rlbarter/STAT-215A-Fall-2017/tree/master/week11. Feel free to download it and use for your own learning or teaching adventures!\n\nR has a wide number of packages for machine learning (ML), which is great, but also quite frustrating since each package was designed independently and has very different syntax, inputs and outputs.\n\nThis means that if you want to do machine learning in R, you have to learn a large number of separate methods.\n\nRecognizing this, Max Kuhn (at the time working in drug discovery at Pfizer, now at RStudio) put together a single package for performing any machine learning method you like. This package is called `caret`. Caret stands for **C**lassification **A**nd **Re**gression **T**raining. Apparently caret has little to do with our orange friend, the carrot.\n\n<!-- ```{r echo = FALSE} -->\n<!-- knitr::include_graphics(\"img/caret/caret.png\") -->\n<!-- ``` -->\n\nNot only does caret allow you to run a plethora of ML methods, it also provides tools for auxiliary techniques such as:\n\n* Data preparation (imputation, centering/scaling data, removing correlated predictors, reducing skewness)\n\n* Data splitting\n\n* Variable selection\n\n* Model evaluation\n\nAn extensive vignette for caret can be found here: https://topepo.github.io/caret/index.html\n\n\n\n\nTo implement your machine learning model of choice using caret you will use the `train` function. The types of modeling options available are many and are listed here: https://topepo.github.io/caret/available-models.html. In the example below, we will use the ranger implementation of random forest to predict whether abalone are \"old\" or not based on a bunch of physical properties of the abalone (sex, height, weight, diameter, etc). The abalone data came from the [UCI Machine Learning repository](https://archive.ics.uci.edu/ml/datasets/abalone) (we split the data into a training and test set).\n\nFirst we load the data into R: \n\n```{r, message=FALSE, warning=FALSE}\n# load in packages\nlibrary(caret)\nlibrary(ranger)\nlibrary(tidyverse)\nlibrary(e1071)\n# load in abalone dataset\nabalone_data <- read.table(\"data/abalone.data\", sep = \",\")\n# load in column names\ncolnames(abalone_data) <- c(\"sex\", \"length\", \"diameter\", \"height\", \n                            \"whole.weight\", \"shucked.weight\", \n                            \"viscera.weight\", \"shell.weight\", \"age\")\n# add a logical variable for \"old\" (age > 10)\nabalone_data <- abalone_data %>%\n  mutate(old = age > 10) %>%\n  # remove the \"age\" variable\n  select(-age)\n# split into training and testing\nset.seed(23489)\ntrain_index <- sample(1:nrow(abalone_data), 0.9 * nrow(abalone_data))\nabalone_train <- abalone_data[train_index, ]\nabalone_test <- abalone_data[-train_index, ]\n# remove the original dataset\nrm(abalone_data)\n# view the first 6 rows of the training data\nhead(abalone_train)\n```\n\nIt looks like we have 3,759 abalone:\n\n\n```{r}\ndim(abalone_train)\n```\n\nTime to fit a random forest model using caret. Anytime we want to fit a model using `train` we tell it which model to fit by providing a formula for the first argument (`as.factor(old) ~ .` means that we want to model `old` as a function of all of the other variables). Then we need to provide a method (we specify `\"ranger\"` to implement randomForest).\n\n```{r, cache = TRUE}\n# fit a random forest model (using ranger)\nrf_fit <- train(as.factor(old) ~ ., \n                data = abalone_train, \n                method = \"ranger\")\n```\n\nBy default, the `train` function without any arguments re-runs the model over 25 bootstrap samples and across 3 options of the tuning parameter (the tuning parameter for `ranger` is `mtry`; the number of randomly selected predictors at each cut in the tree).\n\n\n```{r}\nrf_fit\n```\n\n\nTo test the data on an independent test set is equally as simple using the inbuilt `predict` function.\n\n\n```{r}\n# predict the outcome on a test set\nabalone_rf_pred <- predict(rf_fit, abalone_test)\n# compare predicted outcome and true outcome\nconfusionMatrix(abalone_rf_pred, as.factor(abalone_test$old))\n```\n\n\n# Getting a little fancier with caret\n\nWe have now seen how to fit a model along with the default resampling implementation (bootstrapping) and parameter selection. While this is great, there are many more things we could do with caret. \n\n\n## Pre-processing (`preProcess`)\n\n\nThere are a number of pre-processing steps that are easily implemented by caret. Several stand-alone functions from caret target specific issues that might arise when setting up the model. These include\n\n* `dummyVars`: creating dummy variables from categorical variables with multiple categories\n\n* `nearZeroVar`: identifying zero- and near zero-variance predictors (these may cause issues when subsampling)\n\n* `findCorrelation`: identifying correlated predictors\n\n* `findLinearCombos`: identify linear dependencies between predictors\n\nIn addition to these individual functions, there also exists the **`preProcess`** function which can be used to perform more common tasks such as centering and scaling, imputation and transformation. `preProcess` takes in a data frame to be processed and a method which can be any of \"BoxCox\", \"YeoJohnson\", \"expoTrans\", \"center\", \"scale\", \"range\", \"knnImpute\", \"bagImpute\", \"medianImpute\", \"pca\", \"ica\", \"spatialSign\", \"corr\", \"zv\", \"nzv\", and \"conditionalX\".\n\n\n```{r}\n# center, scale and perform a YeoJohnson transformation\n# identify and remove variables with near zero variance\n# perform pca\nabalone_no_nzv_pca <- preProcess(select(abalone_train, - old), \n                        method = c(\"center\", \"scale\", \"nzv\", \"pca\"))\nabalone_no_nzv_pca\n```\n\n\n```{r}\n# identify which variables were ignored, centered, scaled, etc\nabalone_no_nzv_pca$method\n```\n\n\n```{r}\n# identify the principal components\nabalone_no_nzv_pca$rotation\n```\n\n\n## Data splitting (`createDataPartition` and `groupKFold`)\n\nGenerating subsets of the data is easy with the **`createDataPartition`** function. While this function can be used to simply generate training and testing sets, it can also be used to subset the data while respecting important groupings that exist within the data.\n\nFirst, we show an example of performing general sample splitting to generate 10 different 80% subsamples.\n\n\n```{r}\n# identify the indices of 10 80% subsamples of the iris data\ntrain_index <- createDataPartition(iris$Species,\n                                   p = 0.8,\n                                   list = FALSE,\n                                   times = 10)\n```\n\n```{r}\n# look at the first 6 indices of each subsample\nhead(train_index)\n```\n\nWhile the above is incredibly useful, it is also very easy to do using a for loop. Not so exciting.\n\n\nSomething that IS more exciting is the ability to do K-fold cross validation which respects groupings in the data. The **`groupKFold`** function does just that! \n\n\nAs an example, let's consider the following made-up abalone groups so that each sequential set of 5 abalone that appear in the dataset together are in the same group. For simplicity we will only consider the first 50 abalone.\n\n```{r}\n# add a madeup grouping variable that groupes each subsequent 5 abalone together\n# filter to the first 50 abalone for simplicity\nabalone_grouped <- cbind(abalone_train[1:50, ], group = rep(1:10, each = 5))\nhead(abalone_grouped, 10)\n```\n\n\nThe following code performs 10-fold cross-validation while respecting the groups in the abalone data. That is, each group of abalone must always appear in the same group together.\n\n```{r}\n# perform grouped K means\ngroup_folds <- groupKFold(abalone_grouped$group, k = 10)\ngroup_folds\n```\n\n\n## Resampling options (`trainControl`)\n\nOne of the most important part of training ML models is tuning parameters. You can use the **`trainControl`** function to specify a number of parameters (including sampling parameters) in your model. The object that is outputted from `trainControl` will be provided as an argument for `train`.\n\n```{r}\nset.seed(998)\n# create a testing and training set\nin_training <- createDataPartition(abalone_train$old, p = .75, list = FALSE)\ntraining <- abalone_train[ in_training,]\ntesting  <- abalone_train[-in_training,]\n```\n\n\n```{r}\n# specify that the resampling method is \nfit_control <- trainControl(## 10-fold CV\n                           method = \"cv\",\n                           number = 10)\n```\n\n\n```{r}\n# run a random forest model\nset.seed(825)\nrf_fit <- train(as.factor(old) ~ ., \n                data = abalone_train, \n                method = \"ranger\",\n                trControl = fit_control)\nrf_fit\n```\n\nWe could instead use our **grouped folds** (rather than random CV folds) by assigning the `index` argument of `trainControl` to be `grouped_folds`.\n\n\n```{r warning=FALSE}\n# specify that the resampling method is \ngroup_fit_control <- trainControl(## use grouped CV folds\n                                  index = group_folds,\n                                  method = \"cv\")\nset.seed(825)\nrf_fit <- train(as.factor(old) ~ ., \n                data = select(abalone_grouped, - group), \n                method = \"ranger\",\n                trControl = group_fit_control)\n\n```\n```{r}\nrf_fit\n```\n\n\nYou can also pass functions to `trainControl` that would have otherwise been passed to `preProcess`.\n\n## Model parameter tuning options (`tuneGrid = `)\n\nYou could specify your own tuning grid for model parameters using the `tuneGrid` argument of the `train` function. For example, you can define a grid of parameter combinations.\n\n```{r}\n# define a grid of parameter options to try\nrf_grid <- expand.grid(mtry = c(2, 3, 4, 5),\n                      splitrule = c(\"gini\", \"extratrees\"),\n                      min.node.size = c(1, 3, 5))\nrf_grid\n```\n\n\n```{r}\n# re-fit the model with the parameter grid\nrf_fit <- train(as.factor(old) ~ ., \n                data = select(abalone_grouped, -group), \n                method = \"ranger\",\n                trControl = group_fit_control,\n                # provide a grid of parameters\n                tuneGrid = rf_grid)\nrf_fit\n```\n\n# Advanced topics\n\nThis tutorial has only scratched the surface of all of the options in the caret package. To find out more, see the extensive vignette https://topepo.github.io/caret/index.html.","srcMarkdownNoYaml":"\n\nNote: If you're new to caret, I suggest learning tidymodels instead [http://www.rebeccabarter.com/blog/2020-03-25_machine_learning/](http://www.rebeccabarter.com/blog/2020-03-25_machine_learning/). Tidymodels is essentially caret's successor. Don't worry though, your caret code will still work!\n\nOlder note: This tutorial was based on an older version of the abalone data that had a binary `old` varibale rather than a numeric `age` variable. It has been modified lightly so that it uses a manual `old` variable (is the abalone older than 10 or not) and ignores the numeric `age` variable.\n\nMaterials prepared by Rebecca Barter. Package developed by Max Kuhn.\n\nAn interactive Jupyter Notebook version of this tutorial can be found at https://github.com/rlbarter/STAT-215A-Fall-2017/tree/master/week11. Feel free to download it and use for your own learning or teaching adventures!\n\nR has a wide number of packages for machine learning (ML), which is great, but also quite frustrating since each package was designed independently and has very different syntax, inputs and outputs.\n\nThis means that if you want to do machine learning in R, you have to learn a large number of separate methods.\n\nRecognizing this, Max Kuhn (at the time working in drug discovery at Pfizer, now at RStudio) put together a single package for performing any machine learning method you like. This package is called `caret`. Caret stands for **C**lassification **A**nd **Re**gression **T**raining. Apparently caret has little to do with our orange friend, the carrot.\n\n<!-- ```{r echo = FALSE} -->\n<!-- knitr::include_graphics(\"img/caret/caret.png\") -->\n<!-- ``` -->\n\nNot only does caret allow you to run a plethora of ML methods, it also provides tools for auxiliary techniques such as:\n\n* Data preparation (imputation, centering/scaling data, removing correlated predictors, reducing skewness)\n\n* Data splitting\n\n* Variable selection\n\n* Model evaluation\n\nAn extensive vignette for caret can be found here: https://topepo.github.io/caret/index.html\n\n\n\n## A simple view of caret: the default `train` function\n\nTo implement your machine learning model of choice using caret you will use the `train` function. The types of modeling options available are many and are listed here: https://topepo.github.io/caret/available-models.html. In the example below, we will use the ranger implementation of random forest to predict whether abalone are \"old\" or not based on a bunch of physical properties of the abalone (sex, height, weight, diameter, etc). The abalone data came from the [UCI Machine Learning repository](https://archive.ics.uci.edu/ml/datasets/abalone) (we split the data into a training and test set).\n\nFirst we load the data into R: \n\n```{r, message=FALSE, warning=FALSE}\n# load in packages\nlibrary(caret)\nlibrary(ranger)\nlibrary(tidyverse)\nlibrary(e1071)\n# load in abalone dataset\nabalone_data <- read.table(\"data/abalone.data\", sep = \",\")\n# load in column names\ncolnames(abalone_data) <- c(\"sex\", \"length\", \"diameter\", \"height\", \n                            \"whole.weight\", \"shucked.weight\", \n                            \"viscera.weight\", \"shell.weight\", \"age\")\n# add a logical variable for \"old\" (age > 10)\nabalone_data <- abalone_data %>%\n  mutate(old = age > 10) %>%\n  # remove the \"age\" variable\n  select(-age)\n# split into training and testing\nset.seed(23489)\ntrain_index <- sample(1:nrow(abalone_data), 0.9 * nrow(abalone_data))\nabalone_train <- abalone_data[train_index, ]\nabalone_test <- abalone_data[-train_index, ]\n# remove the original dataset\nrm(abalone_data)\n# view the first 6 rows of the training data\nhead(abalone_train)\n```\n\nIt looks like we have 3,759 abalone:\n\n\n```{r}\ndim(abalone_train)\n```\n\nTime to fit a random forest model using caret. Anytime we want to fit a model using `train` we tell it which model to fit by providing a formula for the first argument (`as.factor(old) ~ .` means that we want to model `old` as a function of all of the other variables). Then we need to provide a method (we specify `\"ranger\"` to implement randomForest).\n\n```{r, cache = TRUE}\n# fit a random forest model (using ranger)\nrf_fit <- train(as.factor(old) ~ ., \n                data = abalone_train, \n                method = \"ranger\")\n```\n\nBy default, the `train` function without any arguments re-runs the model over 25 bootstrap samples and across 3 options of the tuning parameter (the tuning parameter for `ranger` is `mtry`; the number of randomly selected predictors at each cut in the tree).\n\n\n```{r}\nrf_fit\n```\n\n\nTo test the data on an independent test set is equally as simple using the inbuilt `predict` function.\n\n\n```{r}\n# predict the outcome on a test set\nabalone_rf_pred <- predict(rf_fit, abalone_test)\n# compare predicted outcome and true outcome\nconfusionMatrix(abalone_rf_pred, as.factor(abalone_test$old))\n```\n\n\n# Getting a little fancier with caret\n\nWe have now seen how to fit a model along with the default resampling implementation (bootstrapping) and parameter selection. While this is great, there are many more things we could do with caret. \n\n\n## Pre-processing (`preProcess`)\n\n\nThere are a number of pre-processing steps that are easily implemented by caret. Several stand-alone functions from caret target specific issues that might arise when setting up the model. These include\n\n* `dummyVars`: creating dummy variables from categorical variables with multiple categories\n\n* `nearZeroVar`: identifying zero- and near zero-variance predictors (these may cause issues when subsampling)\n\n* `findCorrelation`: identifying correlated predictors\n\n* `findLinearCombos`: identify linear dependencies between predictors\n\nIn addition to these individual functions, there also exists the **`preProcess`** function which can be used to perform more common tasks such as centering and scaling, imputation and transformation. `preProcess` takes in a data frame to be processed and a method which can be any of \"BoxCox\", \"YeoJohnson\", \"expoTrans\", \"center\", \"scale\", \"range\", \"knnImpute\", \"bagImpute\", \"medianImpute\", \"pca\", \"ica\", \"spatialSign\", \"corr\", \"zv\", \"nzv\", and \"conditionalX\".\n\n\n```{r}\n# center, scale and perform a YeoJohnson transformation\n# identify and remove variables with near zero variance\n# perform pca\nabalone_no_nzv_pca <- preProcess(select(abalone_train, - old), \n                        method = c(\"center\", \"scale\", \"nzv\", \"pca\"))\nabalone_no_nzv_pca\n```\n\n\n```{r}\n# identify which variables were ignored, centered, scaled, etc\nabalone_no_nzv_pca$method\n```\n\n\n```{r}\n# identify the principal components\nabalone_no_nzv_pca$rotation\n```\n\n\n## Data splitting (`createDataPartition` and `groupKFold`)\n\nGenerating subsets of the data is easy with the **`createDataPartition`** function. While this function can be used to simply generate training and testing sets, it can also be used to subset the data while respecting important groupings that exist within the data.\n\nFirst, we show an example of performing general sample splitting to generate 10 different 80% subsamples.\n\n\n```{r}\n# identify the indices of 10 80% subsamples of the iris data\ntrain_index <- createDataPartition(iris$Species,\n                                   p = 0.8,\n                                   list = FALSE,\n                                   times = 10)\n```\n\n```{r}\n# look at the first 6 indices of each subsample\nhead(train_index)\n```\n\nWhile the above is incredibly useful, it is also very easy to do using a for loop. Not so exciting.\n\n\nSomething that IS more exciting is the ability to do K-fold cross validation which respects groupings in the data. The **`groupKFold`** function does just that! \n\n\nAs an example, let's consider the following made-up abalone groups so that each sequential set of 5 abalone that appear in the dataset together are in the same group. For simplicity we will only consider the first 50 abalone.\n\n```{r}\n# add a madeup grouping variable that groupes each subsequent 5 abalone together\n# filter to the first 50 abalone for simplicity\nabalone_grouped <- cbind(abalone_train[1:50, ], group = rep(1:10, each = 5))\nhead(abalone_grouped, 10)\n```\n\n\nThe following code performs 10-fold cross-validation while respecting the groups in the abalone data. That is, each group of abalone must always appear in the same group together.\n\n```{r}\n# perform grouped K means\ngroup_folds <- groupKFold(abalone_grouped$group, k = 10)\ngroup_folds\n```\n\n\n## Resampling options (`trainControl`)\n\nOne of the most important part of training ML models is tuning parameters. You can use the **`trainControl`** function to specify a number of parameters (including sampling parameters) in your model. The object that is outputted from `trainControl` will be provided as an argument for `train`.\n\n```{r}\nset.seed(998)\n# create a testing and training set\nin_training <- createDataPartition(abalone_train$old, p = .75, list = FALSE)\ntraining <- abalone_train[ in_training,]\ntesting  <- abalone_train[-in_training,]\n```\n\n\n```{r}\n# specify that the resampling method is \nfit_control <- trainControl(## 10-fold CV\n                           method = \"cv\",\n                           number = 10)\n```\n\n\n```{r}\n# run a random forest model\nset.seed(825)\nrf_fit <- train(as.factor(old) ~ ., \n                data = abalone_train, \n                method = \"ranger\",\n                trControl = fit_control)\nrf_fit\n```\n\nWe could instead use our **grouped folds** (rather than random CV folds) by assigning the `index` argument of `trainControl` to be `grouped_folds`.\n\n\n```{r warning=FALSE}\n# specify that the resampling method is \ngroup_fit_control <- trainControl(## use grouped CV folds\n                                  index = group_folds,\n                                  method = \"cv\")\nset.seed(825)\nrf_fit <- train(as.factor(old) ~ ., \n                data = select(abalone_grouped, - group), \n                method = \"ranger\",\n                trControl = group_fit_control)\n\n```\n```{r}\nrf_fit\n```\n\n\nYou can also pass functions to `trainControl` that would have otherwise been passed to `preProcess`.\n\n## Model parameter tuning options (`tuneGrid = `)\n\nYou could specify your own tuning grid for model parameters using the `tuneGrid` argument of the `train` function. For example, you can define a grid of parameter combinations.\n\n```{r}\n# define a grid of parameter options to try\nrf_grid <- expand.grid(mtry = c(2, 3, 4, 5),\n                      splitrule = c(\"gini\", \"extratrees\"),\n                      min.node.size = c(1, 3, 5))\nrf_grid\n```\n\n\n```{r}\n# re-fit the model with the parameter grid\nrf_fit <- train(as.factor(old) ~ ., \n                data = select(abalone_grouped, -group), \n                method = \"ranger\",\n                trControl = group_fit_control,\n                # provide a grid of parameters\n                tuneGrid = rf_grid)\nrf_fit\n```\n\n# Advanced topics\n\nThis tutorial has only scratched the surface of all of the options in the caret package. To find out more, see the extensive vignette https://topepo.github.io/caret/index.html."},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../styles.css"],"toc":true,"output-file":"2017-11-17-caret_tutorial.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.5.56","theme":"simplex","linkcolor":"#6633c4","code-copy":true,"footnotes-hover":true,"title-block-banner":true,"comments":{"utterances":{"repo":"rlbarter/blog_comments"}},"title":"A basic tutorial of caret: the machine learning package in R","author":"Rebecca Barter","categories":["R","machine learning"],"date":"2017-11-17","description":"R has a wide number of packages for machine learning (ML), which is great, but also quite frustrating since each package was designed independently and has very different syntax, inputs and outputs. Caret unifies these packages into a single package with constant syntax, saving everyone a lot of frustration and time!","toc-location":"left"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}